```markdown
Q: Why does the total number of steps in an epoch change when using `num_workers > 0` in DDP training with WebDataset?

A: When using `num_workers > 0` in a DataLoader for DDP training, the discrepancy in the total number of steps per epoch can occur due to how data is distributed and processed across multiple workers. Each worker may independently handle a portion of the dataset, leading to potential duplication or misalignment in data processing. To address this, ensure that the `with_epoch` method is applied to the `WebLoader` rather than the `WebDataset`. Additionally, consider implementing cross-worker shuffling to maintain data consistency across workers. Here's an example:

```python
data = wds.WebDataset(self.url, resampled=True).shuffle(1000).map(preprocess_train)
loader = wds.WebLoader(data, pin_memory=True, shuffle=False, batch_size=20, num_workers=2).with_epoch(...)
```

For cross-worker shuffling:

```python
loader = wds.WebLoader(data, pin_memory=True, shuffle=False, batch_size=20, num_workers=2)
loader = loader.unbatched().shuffle(2000).batched(20).with_epoch(200)
```
```
