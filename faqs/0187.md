Q: Why is it recommended to avoid batching in PyTorch's `DataLoader` and instead batch in the dataset for streaming datasets?

A: When using PyTorch's `DataLoader` with `num_workers` greater than zero, data needs to be transferred between different processes. This transfer is more efficient when larger amounts of data are moved at once. Therefore, it is generally more efficient to batch data in the dataset itself and then rebatch after the loader. This approach minimizes the overhead associated with inter-process communication. For example, using the WebDataset Dataloader, you can unbatch, shuffle, and then rebatch as shown below:

```python
loader = wds.WebLoader(dataset, num_workers=4, batch_size=8)
loader = loader.unbatched().shuffle(1000).batched(12)
```
