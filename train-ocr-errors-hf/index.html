<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Fine Tuning LLM with Huggingface and WebDataset - webdataset</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Fine Tuning LLM with Huggingface and WebDataset";
        var mkdocs_page_input_path = "train-ocr-errors-hf.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> webdataset
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="" href="../README.md">README</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../webdataset/">WebDataset</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../wids/">WIDS</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../generate-text-dataset/">Dataset Generation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tesseract-wds/">Tesseract wds</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-wds/">Resnet 50 Training on (Fake)Imagenet with WebDataset</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-wids/">Train resnet50 wids</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-multiray-wds/">WebDataset + Distributed PyTorch Training</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-multiray-wids/">WebIndexedDataset + Distributed PyTorch Training</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Fine Tuning LLM with Huggingface and WebDataset</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../mi-images/">Mini Imagenet Generation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../mi-prompts/">Mini Imagenet Generation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../wds-notes/">Wds notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../column-store/">Using WebDataset as a Column Store</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">webdataset</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Examples</li>
      <li class="breadcrumb-item active">Fine Tuning LLM with Huggingface and WebDataset</li>
    <li class="wy-breadcrumbs-aside">
          <a href="http://github.com/webdataset/webdataset/edit/master/docs/train-ocr-errors-hf.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="fine-tuning-llm-with-huggingface-and-webdataset">Fine Tuning LLM with Huggingface and WebDataset</h1>
<p>This notebook illustrates the use of WebDataset together with Huggingface for fine-tuning large language models.</p>
<p>Some features of note:</p>
<ul>
<li>training data is loaded directly from Huggingface</li>
<li>data is downloaded and stored locally incrementally as needed</li>
<li>a custom sampler is used in order to make remote data access more efficient</li>
</ul>
<pre><code class="language-python"># parameters
base_model = &quot;google/flan-t5-base&quot;
dataset_url = (
    &quot;https://huggingface.co/tmbdev/d-tokens/resolve/main/d-tokens.json?download=true&quot;
)
cache_dir = &quot;./_cache&quot;
batch_size = 1
max_steps = 10000
epochs = 1
learning_rate = 3e-4
</code></pre>
<pre><code class="language-python"># imports
import string
import random
import numpy as np
import regex
import unicodedata
import logging

import torch.utils.data
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import AutoModelForSeq2SeqLM
from transformers.adapters import LoRAConfig
from transformers import TrainingArguments, AdapterTrainer, TrainerCallback

# workaround for running this in the source tree, you usually don't need this
try:
    import wids
except:
    sys.path += [&quot;..&quot;]
    import wids
</code></pre>
<pre><code class="language-python">def normalize_string(s):
    &quot;&quot;&quot;Take a string and normalize it.

    Normalization removes common typographic variants of punctuation characters
    that would otherwise have to be learned explicitly by the model. It also
    simplifies whitespace and removes long strings of punctuations used for
    graphical effect.
    &quot;&quot;&quot;
    # start with Unicode normalization
    s = unicodedata.normalize(&quot;NFKC&quot;, s)
    s = regex.sub(r&quot;[*@`]&quot;, &quot;&quot;, s)
    s = regex.sub(r&quot;[\u0027\u2019\u2018\u201A\u201B]&quot;, &quot;'&quot;, s)
    # replace all single quotes with '
    s = regex.sub(r&quot;[\u0022\u201C\u201D\u201E\u201F]&quot;, '&quot;', s)
    # replace all double quotes with &quot;
    s = regex.sub(r&quot;[\u2013\u2014\u2012\u2015-]&quot;, &quot;-&quot;, s)  # normalize dashes
    s = regex.sub(r&quot;(\p{P})\p{P}+&quot;, r&quot;\1&quot;, s)  # remove duplicate punctuation
    s = regex.sub(r&quot;[^\p{L}\p{N}\p{Z}().,?!:;'\&quot;\n-]+&quot;, &quot; &quot;, s)
    s = regex.sub(r&quot;[ \t]+&quot;, &quot; &quot;, s)
    s = s.strip()
    return s
</code></pre>
<pre><code class="language-python"># Data augmentation. Actually, in this case, we generate a synthetic training sample from
# a clean input string.

replacements = list(
    set(string.ascii_letters + string.digits + &quot; &quot; + &quot;&quot; + string.punctuation)
    - set([&quot;*&quot;])
)


def degrade(s, prange=(0.05, 0.1), seed=None, special=&quot;*&quot;):
    &quot;&quot;&quot;Generate training samples by degrading a string.

    Our model is a sequence-to-sequence model that identifies the location of OCR errors in
    a text string. It is trained on a synthetic dataset that contains pairs of strings, one
    of which is a degraded string, and the other is the degraded string with errors marked
    by asterisks. The model is trained to predict the location of the asterisks.
    &quot;&quot;&quot;
    seed = random.randint(0, 1000000) if seed is None else seed
    rng = random.Random(seed)
    s = normalize_string(s)
    if len(s) &lt; 2:
        return s, s
    for _ in range(100):
        if rng.random() &lt; 0.5:
            # use regex to delete the first k words, where k is random between 1 and 2
            # we do this because otherwise the model will flag lower case letters at the beginning
            # of a string as errors
            k = rng.randint(1, 4)
            expr = r&quot;^([^\p{Z}]+?\p{Z}+){%d}&quot; % k
            s = regex.sub(expr, &quot;&quot;, s, count=1)
        if len(s) &gt; 1:
            break
    result = &quot;&quot;
    target = &quot;&quot;
    p = rng.uniform(*prange)
    for c in s:
        if c == special:
            continue
        if c != &quot;\n&quot; and rng.random() &lt; p:
            r = rng.choice(replacements)
            result += r
            target += special
        else:
            result += c
            target += c
    result = normalize_string(result)
    return result, target


degrade(&quot;Hello, world's biggest ball-of-yarn!&quot;)
</code></pre>
<pre><code class="language-python"># We use Flan T5 as the base model. Other models might work better.

tokenizer = AutoTokenizer.from_pretrained(base_model)
</code></pre>
<pre><code class="language-python"># This is a helper function that takes a sample, unpacks it, applies the degradation,
# and then returns a dictionary with the input_ids and the labels as required by Huggingface.


def make_sample(sample, *, prange=(0.05, 0.1), seed=None, prefix=&quot;ocr-errors: &quot;):
    &quot;&quot;&quot;Given a sample consisting of a clean text string, generate a training sample.

    Args:
        sample: a sample from the webdataset
        prange: range of error probability
        seed: random seed or None for random seed
        prefix: prefix (prompt) to add to the input stringf
    &quot;&quot;&quot;
    clean = sample[&quot;.txt.gz&quot;]
    clean = normalize_string(clean)
    text, target = degrade(clean, prange=prange, seed=seed)
    text_ids = torch.tensor(
        tokenizer.encode(prefix + text, max_length=512, truncation=True, padding=&quot;max_length&quot;)
    )
    target_ids = torch.tensor(tokenizer.encode(target, max_length=512, truncation=True, padding=&quot;max_length&quot;))
    return dict(input_ids=text_ids, labels=target_ids)
</code></pre>
<pre><code class="language-python"># This is really all that is WebDataset specific:
# - we specify a URL for the JSON index file
# - we specify a local cache directory
# - we instantiate a ShardListDataset with keep=True
# - we add the make_sample transform to the dataset
# - we create a custom sampler that respects shard boundaries



dataset = wids.ShardListDataset(
    dataset_url, cache_dir=cache_dir, cache_size=int(1e10), keep=True
)
dataset.add_transform(make_sample)
dataset[999]

sampler = wids.ShardedSampler(dataset)
</code></pre>
<pre><code class="language-python"># This plot illustrates the behavior of the shard sampler: it generates a sequence
# of samples from each shard in turn, and then moves on to the next shard.

import matplotlib as mpl
import matplotlib.pyplot as plt

%matplotlib inline
plt.subplot(121)
plt.plot(list(sampler)[:10000])
plt.subplot(122)
plt.plot(list(sampler)[:500]);
</code></pre>
<pre><code class="language-python"># Standard Hugginface LoRA setup.

# start with the pretrained base model
model = AutoModelForSeq2SeqLM.from_pretrained(base_model)

# set the parameters for LoRA
config = LoRAConfig(
    r=8,
    alpha=16,
    # use it on all of the layers
    intermediate_lora=True,
    output_lora=True,
)

# make a new adapter for the xerr dataset
model.add_adapter(&quot;xerr&quot;, config=config)
# enable the adapter for training
model.train_adapter(&quot;xerr&quot;)
model.set_active_adapters([&quot;xerr&quot;])
</code></pre>
<pre><code class="language-python"># Standard Huggingface adapter training, except for the custom sampler.

training_args = TrainingArguments(
    learning_rate=learning_rate,
    num_train_epochs=epochs,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    logging_steps=2000,
    save_steps=5000,
    output_dir=&quot;./training_output&quot;,
    overwrite_output_dir=True,
    remove_unused_columns=False,
    max_steps=max_steps,
)

# create the trainer
trainer = AdapterTrainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=dataset,
    # eval_dataset=OCRDataset(&quot;test&quot;, maxsize=100),
)

# to set the sampler, we override the get_train_sampler method
# Huggingface doesn't provide a better way to do this

trainer._get_train_sampler = lambda: sampler
</code></pre>
<pre><code class="language-python"># Run the bulk of the training.

trainer.train()
</code></pre>
<pre><code class="language-python"># Show some examples (this isn't really &quot;validation&quot;).

num_validation = 10
validation_dataset = dataset

logging.getLogger(&quot;transformers&quot;).setLevel(logging.ERROR)

for i in range(num_validation):
    # load the input and label (note: we get a different degradation each time)
    sample = validation_dataset[i]
    # convert the input and label to tensors
    input_ids = sample[&quot;input_ids&quot;].unsqueeze(0).to(0)
    label_ids = sample[&quot;labels&quot;].unsqueeze(0).to(0)
    # use the model to generate the output
    output = model.generate(input_ids, max_length=1024)
    # convert the tokens to text
    input_text = (
        tokenizer.decode(input_ids[0], skip_special_tokens=True)
        .replace(&quot;ocr-errors:&quot;, &quot;&quot;)
        .strip()
    )
    output_text = tokenizer.decode(output[0], skip_special_tokens=True).strip()
    label_text = tokenizer.decode(label_ids[0], skip_special_tokens=True).strip()

    print(f&quot;[{i}]&quot;)
    print(&quot;Input: &quot;, input_text)
    print(&quot;Output:&quot;, output_text)
    print(&quot;Label: &quot;, label_text)
    print(&quot;---&quot;)
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../train-resnet50-multiray-wids/" class="btn btn-neutral float-left" title="WebIndexedDataset + Distributed PyTorch Training"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../mi-images/" class="btn btn-neutral float-right" title="Mini Imagenet Generation">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="http://github.com/webdataset/webdataset" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../train-resnet50-multiray-wids/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../mi-images/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
