<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Mini Imagenet Generation - webdataset</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Mini Imagenet Generation";
        var mkdocs_page_input_path = "mi-prompts.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> webdataset
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="" href="../README.md">README</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../webdataset/">WebDataset</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../wids/">WIDS</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../generate-text-dataset/">Dataset Generation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tesseract-wds/">Tesseract wds</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-wds/">Resnet 50 Training on (Fake)Imagenet with WebDataset</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-wids/">Train resnet50 wids</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-multiray-wds/">WebDataset + Distributed PyTorch Training</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-multiray-wids/">WebIndexedDataset + Distributed PyTorch Training</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-ocr-errors-hf/">Fine Tuning LLM with Huggingface and WebDataset</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../mi-images/">Mini Imagenet Generation</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Mini Imagenet Generation</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../wds-notes/">Wds notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../column-store/">Using WebDataset as a Column Store</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">webdataset</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Examples</li>
      <li class="breadcrumb-item active">Mini Imagenet Generation</li>
    <li class="wy-breadcrumbs-aside">
          <a href="http://github.com/webdataset/webdataset/edit/master/docs/mi-prompts.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="mini-imagenet-generation">Mini Imagenet Generation</h1>
<p>This is one of a pair of notebooks used for generating an ImageNet-like dataset of training
data using stable diffusion models. The difficulty of such artificial datasets can be
easily tuned, and they are useful for debugging and testing deep learning applications.</p>
<p>The first notebook uses Mistral-7B for taking class labels and generating descriptive prompts
for image generation. The prompts are written out as shards to disk and shuffled. The process
is parallelized using Ray.</p>
<p>The second notebook uses Stable Diffustion to take descriptive prompts/image captions
and renders them as image. This is a straightfowrard shard-to-shard transformation.</p>
<p>Note that we are using explicit parallelization over shard files in the initial generation
and the image generation, while we are using ray.data for the actual shuffling. That is
because using explicit parallelization over shards makes it easier to restart jobs that have
failed halfway through for some reason.</p>
<pre><code class="language-python">import itertools, random, uuid
from pprint import pprint
import os

import torch
import webdataset as wds
from transformers import AutoModelForCausalLM, AutoTokenizer
from webdataset import filters
import textwrap
import time

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from typing import List

def take(n, iterable):
    &quot;&quot;&quot;Return first n items of the iterable as a list&quot;&quot;&quot;
    return list(itertools.islice(iterable, n))


def get_gpu_memories():
    memory = []
    if torch.cuda.is_available(): 
        for i in range(torch.cuda.device_count()):
            memory.append(torch.cuda.get_device_properties(i).total_memory - 
                                torch.cuda.memory_allocated(i))
    return memory

def get_num_gpus():
    cluster_resources = ray.cluster_resources()
    return cluster_resources[&quot;GPU&quot;]

def ray_get(future, timeout=0.1):
    ready, not_ready = ray.wait([future], timeout=timeout)
    if not_ready:
        raise TimeoutError()
    return ray.get(future)

def is_ready(actor, timeout=0.1):
    ready, not_ready = ray.wait([actor], timeout=timeout)
    if not_ready:
        return False
    return True

def select_or_delete(actors, predicate):
    result = []
    for actor in actors:
        if predicate(actor):
            result.append(actor)
        else:
            del actor
    return result
</code></pre>
<pre><code class="language-python"># parameters

# number of classes, must be 10, 100, or 1000
nclasses = 10 

# number of images per shard 
nimages = 100 

# number of prompts generated at once per class
ngenerated = 20 

# number of training shards 
nshards = 1281  

# number of validation shards
nvalshards = 50  

# output directory
odir = f&quot;./mini-imagenet-{nclasses}&quot; 

# output file prefix
oprefix = f&quot;mi{nclasses}&quot; 

# number of actors to use, -1 for =number of GPUs
nactors = -1  

# check that each actor has sufficient memory
check_sufficient = True 

# seconds to wait for actors to start up
actor_startup_wait = 10 
</code></pre>
<pre><code class="language-python">!echo &quot;odir=$odir&quot;
!mkdir -p $odir
</code></pre>
<pre><code class="language-python">if nclasses == 10:
    imagenet_classes = &quot;dog cat car plane bird fish frog horse sheep truck&quot;.split()
elif nclasses == 100:
    imagenet_classes = sorted(list(set(&quot;&quot;&quot;
    3d_printer aircraft_carrier airplane apple backpack banana baseball_bat
    baseball_glove bat bear bed bench bird book bottle bowl broccoli cake camel car
    carrot cat cell_phone chair clock cloud couch cup dining_table dog donut
    elephant fire fish fork fox frisbee frog giraffe hair_drier handbag horse
    hot_dog hydrant kangaroo keyboard kite knife lamp laptop lion meteor microwave
    monitor monkey mouse mushroom octopus orange oven palm_tree panda parking_meter
    pear pizza plane potted_plant refrigerator remote rocket sandwich scissors sheep
    sink skateboard skis snowboard spoon sports_ball stop_sign street_sign suitcase
    surfboard sweet_pepper table teddy_bear telephone tennis_racket tie tiger
    toaster toilet toothbrush tree truck tv umbrella vase wine_glass zebra
    &quot;&quot;&quot;.split())))
elif nclasses == 1000:
    imagenet_classes = open(&quot;imagenet1000.txt&quot;).read().split()
else:
    raise ValueError(f&quot;invalid number of classes: {nclasses}, must be 10, 100, or 1000&quot;)

assert len(imagenet_classes) == nclasses
</code></pre>
<h1 id="generation-classes">Generation Classes</h1>
<p>We encapsulate the model and the generation in a low level and high level class. We can then instantiate those classes once per GPU and call them to generate the shards.</p>
<pre><code class="language-python">class TextGenerationModel:
    def __init__(self, model_name: str = &quot;mistralai/Mistral-7B-Instruct-v0.1&quot;, temperature: float = 2.0, top_p: float = 0.9, top_k: int = 10, max_length: int = 96, num_return_sequences: int = 10):
        &quot;&quot;&quot;
        Initialize the text generation model.

        Args:
            model_name: The name of the pretrained model.
            temperature: The temperature for the generation process.
            top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling.
            top_k: The number of highest probability vocabulary tokens to keep for top-k filtering.
            max_length: The maximum length of the sequence to be generated.
            num_return_sequences: The number of independently computed returned sequences for each element in the batch.
        &quot;&quot;&quot;
        # Load the tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            padding_side=&quot;left&quot;,
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

        # Ensure the model is on GPU
        self.model.to(&quot;cuda&quot;).half()

        # Set generation parameters
        self.temperature = temperature
        self.top_p = top_p
        self.top_k = top_k
        self.max_length = max_length
        self.num_return_sequences = num_return_sequences

    def generate_responses(self, texts: List[str]) -&gt; List[str]:
        &quot;&quot;&quot;
        Generate responses for the given texts.

        Args:
            texts: A list of texts to generate responses for.

        Returns:
            A list of generated responses.
        &quot;&quot;&quot;
        # Prepare the inputs
        inputs = self.tokenizer(texts, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=self.max_length).to(&quot;cuda&quot;)

        # Generate responses
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                do_sample=True,
                temperature=self.temperature,
                top_p=self.top_p,
                top_k=self.top_k,
                max_length=self.max_length,
                num_return_sequences=self.num_return_sequences,
            )

        # Decode the responses
        responses = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]

        return responses    
</code></pre>
<pre><code class="language-python">from typing import Dict, List, Iterator

class CaptionGenerator:
    def __init__(self):
        self.template = &quot;[INST] Generate a random, detailed visual caption/description of a photo showing: {object}. [/INST]&quot;

    def load_model(self):
        self.model = TextGenerationModel()

    def gpu_is_sufficient(self):
        gpu_memories = get_gpu_memories()
        assert len(gpu_memories) == 1, &quot;more than one GPU allocated to actor???&quot;
        return gpu_memories[0] / 1e9 &gt; 32.0

    def process_batch(self, batch: List[Dict], trim: bool = True) -&gt; List[Dict]:
        &quot;&quot;&quot;Process a batch of samples, generating responses for each.&quot;&quot;&quot;
        n = len(batch)
        texts = [batch[i][&quot;text&quot;] for i in range(n)]
        responses = self.model.generate_responses(texts)
        if trim:
            responses = [response.split(&quot;[/INST]&quot;)[-1].strip() for response in responses]
        responses = [responses[i : i + self.model.num_return_sequences] for i in range(0, len(responses), self.model.num_return_sequences)]
        for i in range(n):
            batch[i][&quot;responses&quot;] = responses[i]
        return batch

    def process_list_by_batches(self, samples: List[Dict], batch_size: int = 1) -&gt; Iterator[Dict]:
        &quot;&quot;&quot;Process a list of samples by batches.&quot;&quot;&quot;
        samples_iter = iter(samples)
        while True:
            batch = take(batch_size, samples_iter)
            if not batch:
                break
            responses = self.process_batch(batch)
            yield from responses

    def make_samples(self, n: int) -&gt; Iterator[Dict]:
        &quot;&quot;&quot;Generate a list of samples.&quot;&quot;&quot;
        for i in range(n):
            cls = random.randrange(len(imagenet_classes))
            object = imagenet_classes[cls]
            yield dict(cls=cls, object=object, text=self.template.format(object=object))

    def make_captions(self, samples: List[Dict]) -&gt; Iterator[Dict]:
        &quot;&quot;&quot;Generate captions for a list of samples.&quot;&quot;&quot;
        for sample in self.process_list_by_batches(samples):
            for response in sample[&quot;responses&quot;]:
                yield dict(
                    cls=sample[&quot;cls&quot;],
                    object=sample[&quot;object&quot;],
                    text=sample[&quot;text&quot;],
                    response=response,
                )

    def make_shard(self, output: str, n: int, k: int = 5):
        &quot;&quot;&quot;
        Generate a shard of samples with generated captions.

        Args:
            output: The output file to write the shard to.
            n: The number of samples to generate in the shard.
            k: The number of return sequences for each sample.
        &quot;&quot;&quot;
        if os.path.exists(output):
            return
        self.model.num_return_sequences = k
        writer = wds.TarWriter(output+&quot;.temp&quot;)
        captions = self.make_captions(self.make_samples(n // k + k))
        for caption in itertools.islice(captions, n):
            sample = dict(
                __key__=uuid.uuid4().hex,
                json=caption,
            )
            writer.write(sample)
        os.rename(output+&quot;.temp&quot;, output)

</code></pre>
<h1 id="parallelization-with-ra">Parallelization with Ra</h1>
<p>For parallel generation, we use a Ray cluster. This will also do the right thing with a single machine/single GPU setup. It automatically scales up.</p>
<pre><code class="language-python">
import ray

if not ray.is_initialized():
    ray.init()

</code></pre>
<pre><code class="language-python">@ray.remote(num_gpus=1)
class RayCaptionGenerator(CaptionGenerator):
    def __init__(self):
        super().__init__()
</code></pre>
<pre><code class="language-python"># Start up and create the actor pool.
# This tries to adapt to the number of GPUs available.
# It also checks that each actor has sufficient memory.
# If not, set up your cluster differently by excluding GPUs that are too small.
# (Ray's facilities for heterogenous clusters are somewhat limited)

ngpus = get_num_gpus() if nactors == -1 else nactors

print(f&quot;using {ngpus} actors&quot;)
actors = [RayCaptionGenerator.remote() for i in range(int(ngpus))]

print(&quot;loading the models&quot;)
for actor in actors:
    assert ray.get(actor.gpu_is_sufficient.remote()), &quot;GPU memory insufficient&quot;
    ray.get(actor.load_model.remote())

print(&quot;creating the pool&quot;)
pool = ray.util.ActorPool(actors)
</code></pre>
<pre><code class="language-python"># It would be nice if there were a .map_with_actors method in pool,
# but there isn't, so we use this workaround.

def apply_actor(actor, dest):
    return actor.make_shard.remote(dest, nimages, ngenerated)

!mkdir -p $odir/prompts
</code></pre>
<pre><code class="language-python"># Perform the actual shard generation.

dests = [f&quot;{odir}/prompts/prompts-{i:06d}.tar&quot; for i in range(nshards + nvalshards)]
result = list(pool.map(apply_actor, dests))
</code></pre>
<pre><code class="language-python">del actors
del pool
</code></pre>
<h1 id="shuffle">Shuffle</h1>
<p>For shuffling the dataset, we use the ray.data <code>read_webdataset</code> and <code>write_webdataset</code> functions.</p>
<pre><code class="language-python">import ray
from ray.data import read_webdataset
import glob
</code></pre>
<pre><code class="language-python">!mkdir -p $odir/shuffled
!rm -f $odir/shuffled/*
shards = glob.glob(f&quot;{odir}/prompts/prompts-*.tar&quot;)
dataset = read_webdataset(shards)
shuffled_dataset = dataset.random_shuffle()
shuffled_dataset.repartition(len(shards)).write_webdataset(f&quot;{odir}/shuffled/&quot;)
</code></pre>
<pre><code class="language-python"># The output of write_webdataset is a directory of shards, but not following
# the usual naming conventions. We rename the shards to follow typical
# webdataset conventions.

import glob
import os

shuffled = sorted(glob.glob(f&quot;{odir}/shuffled/*.tar&quot;))
for i in range(nshards):
    os.rename(shuffled[i], f&quot;{odir}/shuffled/{oprefix}-{i:06d}.tar&quot;)
for i in range(nvalshards):
    os.rename(shuffled[nshards+i], f&quot;{odir}/shuffled/{oprefix}-val-{i:06d}.tar&quot;)

</code></pre>
<pre><code class="language-python">
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../mi-images/" class="btn btn-neutral float-left" title="Mini Imagenet Generation"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../wds-notes/" class="btn btn-neutral float-right" title="Wds notes">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="http://github.com/webdataset/webdataset" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../mi-images/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../wds-notes/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
