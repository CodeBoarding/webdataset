{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"%matplotlib inline import matplotlib.pyplot as plt import torch.utils.data import torch.nn from random import randrange import os os.environ[\"WDS_VERBOSE_CACHE\"] = \"1\" os.environ[\"GOPEN_VERBOSE\"] = \"0\" The WebDataset Format WebDataset format files are tar files, with two conventions: within each tar file, files that belong together and make up a training sample share the same basename when stripped of all filename extensions the shards of a tar file are numbered like something-000000.tar to something-012345.tar , usually specified using brace notation something-{000000..012345}.tar You can find a longer, more detailed specification of the WebDataset format in the WebDataset Format Specification WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. WebDataset can also read concatenated MsgPack and CBORs sources. The WebDataset representation allows writing purely sequential I/O pipelines for large scale deep learning. This is important for achieving high I/O rates from local storage (3x-10x for local drives compared to random access) and for using object stores and cloud storage for training. The WebDataset format represents images, movies, audio, etc. in their native file formats, making the creation of WebDataset format data as easy as just creating a tar archive. Because of the way data is aligned, WebDataset works well with block deduplication as well and aligns data on predictable boundaries. Standard tools can be used for accessing and processing WebDataset-format files. bucket = \"https://storage.googleapis.com/webdataset/testdata/\" dataset = \"publaynet-train-{000000..000009}.tar\" url = bucket + dataset !curl -s {url} | tar tf - | sed 10q PMC4991227_00003.json PMC4991227_00003.png PMC4537884_00002.json PMC4537884_00002.png PMC4323233_00003.json PMC4323233_00003.png PMC5429906_00004.json PMC5429906_00004.png PMC5592712_00002.json PMC5592712_00002.png tar: stdout: write error Note that in these .tar files, we have pairs of .json and .png files; each such pair makes up a training sample. WebDataset Libraries There are several libraries supporting the WebDataset format: webdataset for Python3 (includes the wids library), this repository Webdataset.jl a Julia implementation tarp , a Golang implementation and command line tool Ray Data sources and sinks The webdataset library can be used with PyTorch, Tensorflow, and Jax. The webdataset Library The webdataset library is an implementation of PyTorch IterableDataset (or a mock implementation thereof if you aren't using PyTorch). It implements as form of stream processing. Some of its features are: large scale parallel data access through sharding high performance disk I/O due to purely sequential reads latency insensitive due to big fat pipes no local storage required instant startup for training jobs only requires reading from file descriptors/network streams, no special APIs its API encourages high performance I/O pipelines scalable from tiny desktop datasets to petascale datasets provides local caching if desired requires no dataset metadata; any collection of shards can be read and used instantly The main limitations people run into are related to the fact that IterableDataset is less commonly used in PyTorch and some existing code may not support it as well, and that achieving an exactly balanced number of training samples across many compute nodes for a fixed epoch size is tricky; for multinode training, webdataset is usually used with shard resampling. There are two interfaces, the concise \"fluid\" interface and a longer \"pipeline\" interface. We'll show examples using the fluid interface, which is usually what you want. import webdataset as wds pil_dataset = wds.WebDataset(url).shuffle(1000).decode(\"pil\").to_tuple(\"png\", \"json\") The resulting datasets are standard PyTorch IterableDataset instances. isinstance(pil_dataset, torch.utils.data.IterableDataset) True for image, json in pil_dataset: break plt.imshow(image) <matplotlib.image.AxesImage at 0x7f73806db970> We can add onto the existing pipeline for augmentation and data preparation. import torchvision.transforms as transforms from PIL import Image preproc = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), lambda x: 1-x, ]) def preprocess(sample): image, json = sample try: label = json[\"annotations\"][0][\"category_id\"] except: label = 0 return preproc(image), label dataset = pil_dataset.map(preprocess) for image, label in dataset: break plt.imshow(image.numpy().transpose(1, 2, 0)) <matplotlib.image.AxesImage at 0x7f7375fc2230> WebDataset is just an instance of a standard IterableDataset . It's a single-threaded way of iterating over a dataset. Since image decompression and data augmentation can be compute intensive, PyTorch usually uses the DataLoader class to parallelize data loading and preprocessing. WebDataset is fully compatible with the standard DataLoader . Here are a number of notebooks showing how to use WebDataset for image classification and LLM training: train-resnet50-wds -- simple, single GPU training from Imagenet train-resnet50-multiray-wds -- multinode training using webdataset generate-text-dataset -- initial dataset generation tesseract-wds -- shard-to-shard transformations, here for OCR running over large datasets train-ocr-errors-hf -- an example of LLM fine tuning using a dataset in webdataset format The wds-notes notebook contains some additional documentation and information about the library. The webdataset Pipeline API The wds.WebDataset fluid interface is just a convenient shorthand for writing down pipelines. The underlying pipeline is an instance of the wds.DataPipeline class, and you can construct data pipelines explicitly, similar to the way you use nn.Sequential inside models. dataset = wds.DataPipeline( wds.SimpleShardList(url), # at this point we have an iterator over all the shards wds.shuffle(100), # add wds.split_by_node here if you are using multiple nodes wds.split_by_worker, # at this point, we have an iterator over the shards assigned to each worker wds.tarfile_to_samples(), # this shuffles the samples in memory wds.shuffle(1000), # this decodes the images and json wds.decode(\"pil\"), wds.to_tuple(\"png\", \"json\"), wds.map(preprocess), wds.shuffle(1000), wds.batched(16) ) batch = next(iter(dataset)) batch[0].shape, batch[1].shape (torch.Size([16, 3, 224, 224]), (16,)) The wids Library for Indexed WebDatasets Installing the webdataset library installs a second library called wids . This library provides fully indexed/random access to the same datasets that webdataset accesses using iterators/streaming. Like the webdataset library, wids is high scalable and provides efficient access to very large datasets. Being indexed, it is easily backwards compatible with existing data pipelines based on indexed dataset, including precise epochs for multinode training. The library comes with its own ChunkedSampler and DistributedChunkedSampler classes, which provided shuffling accross nodes while still preserving enough locality of reference for efficient training. Internally, the library uses a mmap -based tar file reader implementation; this allows very fast access without precomputed indexes, and it also means that shard and the equivalet of \"shuffle buffers\" are shared in memory between workers on the same machine. This additional power comes at some cost: the library requires a small metadata file that lists all the shards in a dataset and the number of samples contained in each, the library requires local storage for as many shards as there are I/O workers on a node, it uses shared memory and mmap , and the availability of indexing makes it easy to accidentally use inefficient access patterns. Generally, the recommendation is to use webdataset for all data generation, data transformation, and training code, and to use wids only if you need fully random access to datasets (e.g., for browing or sparse sampling), need an indexed-based sampler, or are converting tricky legacy code. import wids train_url = \"https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train.json\" dataset = wids.ShardListDataset(train_url) sample = dataset[1900] print(sample.keys()) print(sample[\".txt\"]) plt.imshow(sample[\".jpg\"]) dict_keys(['.cls', '.jpg', '.txt', '__key__', '__dataset__', '__index__', '__shard__', '__shardindex__']) a high quality color photograph of a dog https://storage.googleapis.com/webdataset/fake-ima base: https://storage.googleapis.com/webdataset/fake-imagenet name: imagenet-train nfiles: 1282 nbytes: 31242280960 samples: 128200 cache: /tmp/_wids_cache <matplotlib.image.AxesImage at 0x7f7373669e70> There are several examples of how to use wids in the examples directory. train-resnet50-wids shows how to train a ResNet-50 model on ImageNet using wids train-resnet50-multiray-wids shows how to train a ResNet-50 model on ImageNet using multiple nodes Note that the APIs between webdataset and wids are not fully consistent: wids keeps the extension's \".\" in the keys, while webdataset removes it (\".txt\" vs \"txt\") wids doesn't have a fully fluid interface, and add_transformation just adds to a list of transformations webdataset currently can't read the wids JSON specifications Installation and Documentation $ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets Dependencies The WebDataset library only requires PyTorch, NumPy, and a small library called braceexpand . WebDataset loads a few additional libraries dynamically only when they are actually needed and only in the decoder: PIL/Pillow for image decoding torchvision , torchvideo , torchaudio for image/video/audio decoding msgpack for MessagePack decoding the curl command line tool for accessing HTTP servers the Google/Amazon/Azure command line tools for accessing cloud storage buckets Loading of one of these libraries is triggered by configuring a decoder that attempts to decode content in the given format and encountering a file in that format during decoding. (Eventually, the torch... dependencies will be refactored into those libraries.)","title":"Home"},{"location":"#the-webdataset-format","text":"WebDataset format files are tar files, with two conventions: within each tar file, files that belong together and make up a training sample share the same basename when stripped of all filename extensions the shards of a tar file are numbered like something-000000.tar to something-012345.tar , usually specified using brace notation something-{000000..012345}.tar You can find a longer, more detailed specification of the WebDataset format in the WebDataset Format Specification WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. WebDataset can also read concatenated MsgPack and CBORs sources. The WebDataset representation allows writing purely sequential I/O pipelines for large scale deep learning. This is important for achieving high I/O rates from local storage (3x-10x for local drives compared to random access) and for using object stores and cloud storage for training. The WebDataset format represents images, movies, audio, etc. in their native file formats, making the creation of WebDataset format data as easy as just creating a tar archive. Because of the way data is aligned, WebDataset works well with block deduplication as well and aligns data on predictable boundaries. Standard tools can be used for accessing and processing WebDataset-format files. bucket = \"https://storage.googleapis.com/webdataset/testdata/\" dataset = \"publaynet-train-{000000..000009}.tar\" url = bucket + dataset !curl -s {url} | tar tf - | sed 10q PMC4991227_00003.json PMC4991227_00003.png PMC4537884_00002.json PMC4537884_00002.png PMC4323233_00003.json PMC4323233_00003.png PMC5429906_00004.json PMC5429906_00004.png PMC5592712_00002.json PMC5592712_00002.png tar: stdout: write error Note that in these .tar files, we have pairs of .json and .png files; each such pair makes up a training sample.","title":"The WebDataset Format"},{"location":"#webdataset-libraries","text":"There are several libraries supporting the WebDataset format: webdataset for Python3 (includes the wids library), this repository Webdataset.jl a Julia implementation tarp , a Golang implementation and command line tool Ray Data sources and sinks The webdataset library can be used with PyTorch, Tensorflow, and Jax.","title":"WebDataset Libraries"},{"location":"#the-webdataset-library","text":"The webdataset library is an implementation of PyTorch IterableDataset (or a mock implementation thereof if you aren't using PyTorch). It implements as form of stream processing. Some of its features are: large scale parallel data access through sharding high performance disk I/O due to purely sequential reads latency insensitive due to big fat pipes no local storage required instant startup for training jobs only requires reading from file descriptors/network streams, no special APIs its API encourages high performance I/O pipelines scalable from tiny desktop datasets to petascale datasets provides local caching if desired requires no dataset metadata; any collection of shards can be read and used instantly The main limitations people run into are related to the fact that IterableDataset is less commonly used in PyTorch and some existing code may not support it as well, and that achieving an exactly balanced number of training samples across many compute nodes for a fixed epoch size is tricky; for multinode training, webdataset is usually used with shard resampling. There are two interfaces, the concise \"fluid\" interface and a longer \"pipeline\" interface. We'll show examples using the fluid interface, which is usually what you want. import webdataset as wds pil_dataset = wds.WebDataset(url).shuffle(1000).decode(\"pil\").to_tuple(\"png\", \"json\") The resulting datasets are standard PyTorch IterableDataset instances. isinstance(pil_dataset, torch.utils.data.IterableDataset) True for image, json in pil_dataset: break plt.imshow(image) <matplotlib.image.AxesImage at 0x7f73806db970> We can add onto the existing pipeline for augmentation and data preparation. import torchvision.transforms as transforms from PIL import Image preproc = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), lambda x: 1-x, ]) def preprocess(sample): image, json = sample try: label = json[\"annotations\"][0][\"category_id\"] except: label = 0 return preproc(image), label dataset = pil_dataset.map(preprocess) for image, label in dataset: break plt.imshow(image.numpy().transpose(1, 2, 0)) <matplotlib.image.AxesImage at 0x7f7375fc2230> WebDataset is just an instance of a standard IterableDataset . It's a single-threaded way of iterating over a dataset. Since image decompression and data augmentation can be compute intensive, PyTorch usually uses the DataLoader class to parallelize data loading and preprocessing. WebDataset is fully compatible with the standard DataLoader . Here are a number of notebooks showing how to use WebDataset for image classification and LLM training: train-resnet50-wds -- simple, single GPU training from Imagenet train-resnet50-multiray-wds -- multinode training using webdataset generate-text-dataset -- initial dataset generation tesseract-wds -- shard-to-shard transformations, here for OCR running over large datasets train-ocr-errors-hf -- an example of LLM fine tuning using a dataset in webdataset format The wds-notes notebook contains some additional documentation and information about the library.","title":"The webdataset Library"},{"location":"#the-webdataset-pipeline-api","text":"The wds.WebDataset fluid interface is just a convenient shorthand for writing down pipelines. The underlying pipeline is an instance of the wds.DataPipeline class, and you can construct data pipelines explicitly, similar to the way you use nn.Sequential inside models. dataset = wds.DataPipeline( wds.SimpleShardList(url), # at this point we have an iterator over all the shards wds.shuffle(100), # add wds.split_by_node here if you are using multiple nodes wds.split_by_worker, # at this point, we have an iterator over the shards assigned to each worker wds.tarfile_to_samples(), # this shuffles the samples in memory wds.shuffle(1000), # this decodes the images and json wds.decode(\"pil\"), wds.to_tuple(\"png\", \"json\"), wds.map(preprocess), wds.shuffle(1000), wds.batched(16) ) batch = next(iter(dataset)) batch[0].shape, batch[1].shape (torch.Size([16, 3, 224, 224]), (16,))","title":"The webdataset Pipeline API"},{"location":"#the-wids-library-for-indexed-webdatasets","text":"Installing the webdataset library installs a second library called wids . This library provides fully indexed/random access to the same datasets that webdataset accesses using iterators/streaming. Like the webdataset library, wids is high scalable and provides efficient access to very large datasets. Being indexed, it is easily backwards compatible with existing data pipelines based on indexed dataset, including precise epochs for multinode training. The library comes with its own ChunkedSampler and DistributedChunkedSampler classes, which provided shuffling accross nodes while still preserving enough locality of reference for efficient training. Internally, the library uses a mmap -based tar file reader implementation; this allows very fast access without precomputed indexes, and it also means that shard and the equivalet of \"shuffle buffers\" are shared in memory between workers on the same machine. This additional power comes at some cost: the library requires a small metadata file that lists all the shards in a dataset and the number of samples contained in each, the library requires local storage for as many shards as there are I/O workers on a node, it uses shared memory and mmap , and the availability of indexing makes it easy to accidentally use inefficient access patterns. Generally, the recommendation is to use webdataset for all data generation, data transformation, and training code, and to use wids only if you need fully random access to datasets (e.g., for browing or sparse sampling), need an indexed-based sampler, or are converting tricky legacy code. import wids train_url = \"https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train.json\" dataset = wids.ShardListDataset(train_url) sample = dataset[1900] print(sample.keys()) print(sample[\".txt\"]) plt.imshow(sample[\".jpg\"]) dict_keys(['.cls', '.jpg', '.txt', '__key__', '__dataset__', '__index__', '__shard__', '__shardindex__']) a high quality color photograph of a dog https://storage.googleapis.com/webdataset/fake-ima base: https://storage.googleapis.com/webdataset/fake-imagenet name: imagenet-train nfiles: 1282 nbytes: 31242280960 samples: 128200 cache: /tmp/_wids_cache <matplotlib.image.AxesImage at 0x7f7373669e70> There are several examples of how to use wids in the examples directory. train-resnet50-wids shows how to train a ResNet-50 model on ImageNet using wids train-resnet50-multiray-wids shows how to train a ResNet-50 model on ImageNet using multiple nodes Note that the APIs between webdataset and wids are not fully consistent: wids keeps the extension's \".\" in the keys, while webdataset removes it (\".txt\" vs \"txt\") wids doesn't have a fully fluid interface, and add_transformation just adds to a list of transformations webdataset currently can't read the wids JSON specifications","title":"The wids Library for Indexed WebDatasets"},{"location":"#installation-and-documentation","text":"$ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets","title":"Installation and Documentation"},{"location":"#dependencies","text":"The WebDataset library only requires PyTorch, NumPy, and a small library called braceexpand . WebDataset loads a few additional libraries dynamically only when they are actually needed and only in the decoder: PIL/Pillow for image decoding torchvision , torchvideo , torchaudio for image/video/audio decoding msgpack for MessagePack decoding the curl command line tool for accessing HTTP servers the Google/Amazon/Azure command line tools for accessing cloud storage buckets Loading of one of these libraries is triggered by configuring a decoder that attempts to decode content in the given format and encountering a file in that format during decoding. (Eventually, the torch... dependencies will be refactored into those libraries.)","title":"Dependencies"},{"location":"column-store/","text":"Using WebDataset as a Column Store Sometimes it is desirable to break up a dataset not just by rows but also by columns. This is quite easy in WebDataset, although there is no explicit API for it (one will likely be added). The idea is to just use the __url__ field in a sample to load additional columns as necessary. # We usually abbreviate webdataset as wds import webdataset as wds batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\" # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset(training_urls, resampled=True, shardshuffle=True) This function computes the URL for an additional column from a base URL. This is then used by the add_column function to add data from that additional URL to the data already loaded from the base URL. def find_column_url(url): # In this function, given the main URL for a shard, find the corresponding # extra column URL. # For the demo, we just return the same URL, which means that we simply # add the same values to the samples twice. return url # .replace(\"-train\", \"-train-more\") def add_column(src, find_column_url=find_column_url): \"\"\"Given an iterator over a dataset, add an extra column from a separate dataset.\"\"\" last_url = None column_src = None for sample in src: # We use the __url__ field to keep track of which shard we are working on. # We then open the corresponding URL for the extra column data if necessary. if last_url != sample[\"__url__\"]: column_url = find_column_url(sample[\"__url__\"]) print(\"*** opening column_url\", column_url) column_src = iter(wds.WebDataset(column_url, shardshuffle=False)) last_url = sample[\"__url__\"] # Read the next sample from the extra column data. extra = next(column_src) # Check that the keys match. assert extra[\"__key__\"] == sample[\"__key__\"] # Update the sample with the extra data. for k, v in extra.items(): if k[0] != \"_\": sample[k] = v yield sample trainset = trainset.compose(add_column) # NB: any shuffling, decoding, etc. needs to happen after the `add_column` call Let's see all of it in action. Actually, nothing particularly interesting happens here because we are just loading the same data for the base URL and the additional column. Really, the only feedback you get from this code is the message about opening the column_url. for k, v in next(iter(trainset)).items(): print(k, repr(v)[:60]) *** opening column_url https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-001010.tar __key__ '001010-000002' __url__ 'https://storage.googleapis.com/webdataset/fake-imagenet/ima cls b'9' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x txt b'a high quality color photograph of a frog' Some comments: The code above assumes an exact correspondence between the samples in the different columnn shards; this is really what you ought to aim for. But you can add code to skip data. For small amounts of data (like class labels), you probably just want to store the data in a dbm-style database and use .associate(data) . You could also use wids to retrieve additional samples in add_column . If you want to do the same thing in wids , the code becomes even simpler: class CombinedDataset: def __init__(self, ds1, ds2): self.ds1 = wids.ShardListDataset(ds1) self.ds2 = wids.ShardListDataset(ds2) assert len(self.ds1) == len(self.ds2) def getitem(self, index): return self.ds1[index].update(self.ds2[index]) def __len__(self): return len(self.ds1)","title":"Using WebDataset as a Column Store"},{"location":"column-store/#using-webdataset-as-a-column-store","text":"Sometimes it is desirable to break up a dataset not just by rows but also by columns. This is quite easy in WebDataset, although there is no explicit API for it (one will likely be added). The idea is to just use the __url__ field in a sample to load additional columns as necessary. # We usually abbreviate webdataset as wds import webdataset as wds batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\" # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset(training_urls, resampled=True, shardshuffle=True) This function computes the URL for an additional column from a base URL. This is then used by the add_column function to add data from that additional URL to the data already loaded from the base URL. def find_column_url(url): # In this function, given the main URL for a shard, find the corresponding # extra column URL. # For the demo, we just return the same URL, which means that we simply # add the same values to the samples twice. return url # .replace(\"-train\", \"-train-more\") def add_column(src, find_column_url=find_column_url): \"\"\"Given an iterator over a dataset, add an extra column from a separate dataset.\"\"\" last_url = None column_src = None for sample in src: # We use the __url__ field to keep track of which shard we are working on. # We then open the corresponding URL for the extra column data if necessary. if last_url != sample[\"__url__\"]: column_url = find_column_url(sample[\"__url__\"]) print(\"*** opening column_url\", column_url) column_src = iter(wds.WebDataset(column_url, shardshuffle=False)) last_url = sample[\"__url__\"] # Read the next sample from the extra column data. extra = next(column_src) # Check that the keys match. assert extra[\"__key__\"] == sample[\"__key__\"] # Update the sample with the extra data. for k, v in extra.items(): if k[0] != \"_\": sample[k] = v yield sample trainset = trainset.compose(add_column) # NB: any shuffling, decoding, etc. needs to happen after the `add_column` call Let's see all of it in action. Actually, nothing particularly interesting happens here because we are just loading the same data for the base URL and the additional column. Really, the only feedback you get from this code is the message about opening the column_url. for k, v in next(iter(trainset)).items(): print(k, repr(v)[:60]) *** opening column_url https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-001010.tar __key__ '001010-000002' __url__ 'https://storage.googleapis.com/webdataset/fake-imagenet/ima cls b'9' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x txt b'a high quality color photograph of a frog' Some comments: The code above assumes an exact correspondence between the samples in the different columnn shards; this is really what you ought to aim for. But you can add code to skip data. For small amounts of data (like class labels), you probably just want to store the data in a dbm-style database and use .associate(data) . You could also use wids to retrieve additional samples in add_column . If you want to do the same thing in wids , the code becomes even simpler: class CombinedDataset: def __init__(self, ds1, ds2): self.ds1 = wids.ShardListDataset(ds1) self.ds2 = wids.ShardListDataset(ds2) assert len(self.ds1) == len(self.ds2) def getitem(self, index): return self.ds1[index].update(self.ds2[index]) def __len__(self): return len(self.ds1)","title":"Using WebDataset as a Column Store"},{"location":"generate-text-dataset/","text":"Dataset Generation This is a simple example of dataset generation using WebDataset TarWriter . Shard are uploaded to a server or to the cloud as they are generated. Parallel dataset generation with Ray is illustrated at the very end. This particular notebook generates short text samples using GPT-2. These can be used to generate OCR training data. # package installs for colab import sys if \"google.colab\" in sys.modules: !pip install --quiet webdataset !pip install --quiet adapter-transformers !pip install --quiet sentencepiece !pip install --quiet datasets import uuid import webdataset as wds import os from transformers import GPT2LMHeadModel, GPT2Tokenizer from transformers import pipeline import textwrap # Parameters nsamples = 10 ntokens = 100 nshards = 3 # text generation with Huggingface and GPT2 tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\") tokenizer.pad_token = tokenizer.eos_token model = GPT2LMHeadModel.from_pretrained(\"gpt2\") generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) def generate(n, prompt=\"\"): \"\"\"Generate n words of text, starting with prompt.\"\"\" global tokenizer, model, generator output = generator( prompt, max_length=n + len(tokenizer.encode(prompt)), do_sample=True, temperature=0.99, top_k=50, top_p=0.99, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, )[0] return output[\"generated_text\"] text = generate(100).strip() print() print(textwrap.fill(text, 64)) # function generating an entire shard using TarWriter def generate_shard(oname, nsamples=10000, ntokens=500, prefix=\"\"): \"\"\"Generate a shard of samples with text. Each sample has a \"__key__\" field and a \"txt.gz\" field. That is, the individual text files are compressed automatically on write. They will be automatically decompressed when read. \"\"\" with wds.TarWriter(oname) as output: for i in range(nsamples): text = generate(100).strip() key = uuid.uuid4().hex text = generate(ntokens) sample = {\"__key__\": key, \"txt.gz\": text} output.write(sample) if i % 10 == 0: print(f\"{i:6d} {prefix}:\", repr(text)[:60]) generate_shard(\"temp.tar\", nsamples=10, ntokens=10) !ls -l temp.tar !tar tf temp.tar | head -5 # We need a couple of simple functions to upload to the cloud. def cloud_exists(oname): \"\"\"Check whether a file exists in the cloud.\"\"\" # return os.system(f\"gsutil stat gs://mybucket/500tokens/{oname}\") == 0 return True def cloud_upload(oname): \"\"\"Upload a file to the cloud.\"\"\" # assert os.system(f\"gsutil cp {oname} gs://mybucket/500tokens/{oname}\") == 0 pass # We can now generate a shard and upload it to the cloud. # We skip the generation if the file already exists in the cloud. def generate_and_upload(i): \"\"\"Generate a shard and upload it to the cloud.\"\"\" oname = f\"text-{i:06d}.tar\" if cloud_exists(oname): print(f\"{oname} already exists, skipping\") return False generate_shard(oname, nsamples=nsamples, ntokens=ntokens, prefix=f\"{i:6d} {oname}\") cloud_upload(oname) os.remove(oname) return True # For sequential generation, use this for i in range(nshards): generate_and_upload(i) %%script true # For parallel generation, use this import ray @ray.remote(num_cpus=1, num_gpus=1) def ray_generate_and_upload(i): \"\"\"A Ray remote function that generates a shard and uploads it to the cloud.\"\"\" return generate_and_upload(i) def generate_shards(nshards=10): \"\"\"Generate a number of shards and upload them to the cloud. Runs in parallel on a Ray cluster. \"\"\" ray.init(address='auto') # Connect to the Ray cluster tasks = [ray_generate_and_upload.remote(i) for i in range(nshards)] ray.shutdown() return shard_names","title":"Dataset Generation"},{"location":"generate-text-dataset/#dataset-generation","text":"This is a simple example of dataset generation using WebDataset TarWriter . Shard are uploaded to a server or to the cloud as they are generated. Parallel dataset generation with Ray is illustrated at the very end. This particular notebook generates short text samples using GPT-2. These can be used to generate OCR training data. # package installs for colab import sys if \"google.colab\" in sys.modules: !pip install --quiet webdataset !pip install --quiet adapter-transformers !pip install --quiet sentencepiece !pip install --quiet datasets import uuid import webdataset as wds import os from transformers import GPT2LMHeadModel, GPT2Tokenizer from transformers import pipeline import textwrap # Parameters nsamples = 10 ntokens = 100 nshards = 3 # text generation with Huggingface and GPT2 tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\") tokenizer.pad_token = tokenizer.eos_token model = GPT2LMHeadModel.from_pretrained(\"gpt2\") generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) def generate(n, prompt=\"\"): \"\"\"Generate n words of text, starting with prompt.\"\"\" global tokenizer, model, generator output = generator( prompt, max_length=n + len(tokenizer.encode(prompt)), do_sample=True, temperature=0.99, top_k=50, top_p=0.99, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, )[0] return output[\"generated_text\"] text = generate(100).strip() print() print(textwrap.fill(text, 64)) # function generating an entire shard using TarWriter def generate_shard(oname, nsamples=10000, ntokens=500, prefix=\"\"): \"\"\"Generate a shard of samples with text. Each sample has a \"__key__\" field and a \"txt.gz\" field. That is, the individual text files are compressed automatically on write. They will be automatically decompressed when read. \"\"\" with wds.TarWriter(oname) as output: for i in range(nsamples): text = generate(100).strip() key = uuid.uuid4().hex text = generate(ntokens) sample = {\"__key__\": key, \"txt.gz\": text} output.write(sample) if i % 10 == 0: print(f\"{i:6d} {prefix}:\", repr(text)[:60]) generate_shard(\"temp.tar\", nsamples=10, ntokens=10) !ls -l temp.tar !tar tf temp.tar | head -5 # We need a couple of simple functions to upload to the cloud. def cloud_exists(oname): \"\"\"Check whether a file exists in the cloud.\"\"\" # return os.system(f\"gsutil stat gs://mybucket/500tokens/{oname}\") == 0 return True def cloud_upload(oname): \"\"\"Upload a file to the cloud.\"\"\" # assert os.system(f\"gsutil cp {oname} gs://mybucket/500tokens/{oname}\") == 0 pass # We can now generate a shard and upload it to the cloud. # We skip the generation if the file already exists in the cloud. def generate_and_upload(i): \"\"\"Generate a shard and upload it to the cloud.\"\"\" oname = f\"text-{i:06d}.tar\" if cloud_exists(oname): print(f\"{oname} already exists, skipping\") return False generate_shard(oname, nsamples=nsamples, ntokens=ntokens, prefix=f\"{i:6d} {oname}\") cloud_upload(oname) os.remove(oname) return True # For sequential generation, use this for i in range(nshards): generate_and_upload(i) %%script true # For parallel generation, use this import ray @ray.remote(num_cpus=1, num_gpus=1) def ray_generate_and_upload(i): \"\"\"A Ray remote function that generates a shard and uploads it to the cloud.\"\"\" return generate_and_upload(i) def generate_shards(nshards=10): \"\"\"Generate a number of shards and upload them to the cloud. Runs in parallel on a Ray cluster. \"\"\" ray.init(address='auto') # Connect to the Ray cluster tasks = [ray_generate_and_upload.remote(i) for i in range(nshards)] ray.shutdown() return shard_names","title":"Dataset Generation"},{"location":"mi-images/","text":"Mini Imagenet Generation This is one of a pair of notebooks used for generating an ImageNet-like dataset of training data using stable diffusion models. The difficulty of such artificial datasets can be easily tuned, and they are useful for debugging and testing deep learning applications. The first notebook uses Mistral-7B for taking class labels and generating descriptive prompts for image generation. The prompts are written out as shards to disk and shuffled. The process is parallelized using Ray. The second notebook uses Stable Diffustion to take descriptive prompts/image captions and renders them as image. This is a straightfowrard shard-to-shard transformation. Note that we are using explicit parallelization over shard files in the initial generation and the image generation, while we are using ray.data for the actual shuffling. That is because using explicit parallelization over shards makes it easier to restart jobs that have failed halfway through for some reason. %matplotlib inline import matplotlib.pyplot as plt from pprint import pprint import webdataset as wds from diffusers import AutoPipelineForText2Image import torch import warnings import logging import logging import tqdm from IPython.display import display, clear_output from PIL import Image as PILImage from itertools import islice import glob import os import io from contextlib import contextmanager import sys class SuppressWarning: def __enter__(self): logging.disable(logging.WARNING) def __exit__(self, type, value, traceback): logging.disable(logging.NOTSET) tqdm.tqdm.disable = True def get_num_gpus(): cluster_resources = ray.cluster_resources() return cluster_resources[\"GPU\"] @contextmanager def suppress_outputs(redirect): old_stdout = sys.stdout old_stderr = sys.stderr sys.stdout = redirect sys.stderr = redirect try: yield finally: sys.stdout = old_stdout sys.stderr = old_stderr # parameters odir = \"./mini-imagenet-10\" nactors = -1 check_sufficient = True actor_startup_wait = 10 Transformation Class We encapsulate the rendering into a RenderPrompts class. This class is instantiated once per GPU, loads the model, and then is ready to transform shards. \"\"\" class ShardTransformer: def __init__(self): self.pipe = AutoPipelineForText2Image.from_pretrained( \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\" ).to(\"cuda\") def transform_shard(self, input_shard, output_shard, display_in_notebook=False): ds = wds.WebDataset(input_shard).decode() output = wds.TarWriter(output_shard) for sample in ds: sample = dict(sample) text = sample[\"json\"][\"response\"] with SuppressWarning(): image = self.pipe(prompt=text, num_inference_steps=4, guidance_scale=0.1).images[0] sample[\"jpg\"] = image output.write(sample) if display_in_notebook: clear_output(wait=True) display(image) pprint(text) output.close() \"\"\" def maybe_clear_output(): try: clear_output(wait=True) except: pass class RenderPrompts: def __init__(self, display_in_notebook=False): self.display_in_notebook = display_in_notebook def gpu_is_sufficient(self): return torch.cuda.get_device_properties(0).total_memory > 10**10 def load_model(self): self.pipe = AutoPipelineForText2Image.from_pretrained( \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\" ).to(\"cuda\") def transform_sample(self, sample): text = sample[\"json\"][\"response\"] with SuppressWarning(): image = self.pipe(prompt=text, num_inference_steps=4, guidance_scale=0.1).images[0] sample[\"jpg\"] = image return sample def transform_sample_with_redirect(self, sample): stdout = io.StringIO() with suppress_outputs(stdout): sample = self.transform_sample(sample) sample[\"stdout\"] = stdout.getvalue() return sample def transform_shard(self, input_shard, output_shard, maxcount=999999999): ds = wds.WebDataset(input_shard).decode() output = wds.TarWriter(output_shard+\".temp\") for sample in islice(ds, maxcount): transformed_sample = self.transform_sample_with_redirect(dict(sample)) del transformed_sample[\"stdout\"] maybe_clear_output() output.write(transformed_sample) if self.display_in_notebook: clear_output(wait=True) display(transformed_sample['jpg']) pprint(transformed_sample[\"json\"][\"response\"]) output.close() os.rename(output_shard+\".temp\", output_shard) transformer = RenderPrompts(display_in_notebook=True) transformer.load_model() shards = glob.glob(f\"{odir}/shuffled/*.tar\") transformer.transform_shard(shards[0], \"temp.tar\", maxcount=10) del transformer ('\"A stunning black convertible car cruising down the picturesque countryside ' 'highway. The sleek silhouette of the car blends beautifully into the setting ' 'sun, leaving a trail of wanderers admiring its beauty and sophistication. ' 'The soft top is folded down, letting the sweet sound of the wind and sun-in ' 'take its occupants away') Parallelization with Ray For parallel rendering, we use a Ray cluster. This will also work on a single machine with just one GPU. import ray if not ray.is_initialized(): ray.init(log_to_driver=False) @ray.remote(num_gpus=1) class RayRenderPrompts(RenderPrompts): def __init__(self): super().__init__() 2023-12-30 03:30:54,938 INFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m # Start up and create the actor pool. # This tries to adapt to the number of GPUs available. # It also checks that each actor has sufficient memory. # If not, set up your cluster differently by excluding GPUs that are too small. # (Ray's facilities for heterogenous clusters are somewhat limited) ngpus = get_num_gpus() if nactors == -1 else nactors print(f\"using {ngpus} actors\") actors = [RayRenderPrompts.remote() for i in range(int(ngpus))] print(\"loading the models\") for actor in actors: assert ray.get(actor.gpu_is_sufficient.remote()), \"GPU memory insufficient\" ray.get(actor.load_model.remote()) print(\"creating the pool\") pool = ray.util.ActorPool(actors) using 1.0 actors loading the models creating the pool import glob import os def apply_actor(actor, action): src, dst = action print(f\"START {src} -> {dst}\") result = actor.transform_shard.remote(src, dst) print(f\"DONE {src} -> {dst}\") return result !mkdir -p $odir/images shards = [os.path.basename(p) for p in sorted(glob.glob(f\"{odir}/shuffled/*.tar\"))] actions = [(f\"{odir}/shuffled/{shard}\", f\"{odir}/images/{shard}\") for shard in shards] result = list(pool.map(apply_actor, actions)) START ./mini-imagenet-10/shuffled/mi10-000000.tar -> ./mini-imagenet-10/images/mi10-000000.tar DONE ./mini-imagenet-10/shuffled/mi10-000000.tar -> ./mini-imagenet-10/images/mi10-000000.tar START ./mini-imagenet-10/shuffled/mi10-000001.tar -> ./mini-imagenet-10/images/mi10-000001.tar DONE ./mini-imagenet-10/shuffled/mi10-000001.tar -> ./mini-imagenet-10/images/mi10-000001.tar START ./mini-imagenet-10/shuffled/mi10-000002.tar -> ./mini-imagenet-10/images/mi10-000002.tar DONE ./mini-imagenet-10/shuffled/mi10-000002.tar -> ./mini-imagenet-10/images/mi10-000002.tar START ./mini-imagenet-10/shuffled/mi10-000003.tar -> ./mini-imagenet-10/images/mi10-000003.tar DONE ./mini-imagenet-10/shuffled/mi10-000003.tar -> ./mini-imagenet-10/images/mi10-000003.tar START ./mini-imagenet-10/shuffled/mi10-000004.tar -> ./mini-imagenet-10/images/mi10-000004.tar DONE ./mini-imagenet-10/shuffled/mi10-000004.tar -> ./mini-imagenet-10/images/mi10-000004.tar START ./mini-imagenet-10/shuffled/mi10-000005.tar -> ./mini-imagenet-10/images/mi10-000005.tar DONE ./mini-imagenet-10/shuffled/mi10-000005.tar -> ./mini-imagenet-10/images/mi10-000005.tar START ./mini-imagenet-10/shuffled/mi10-000006.tar -> ./mini-imagenet-10/images/mi10-000006.tar DONE ./mini-imagenet-10/shuffled/mi10-000006.tar -> ./mini-imagenet-10/images/mi10-000006.tar START ./mini-imagenet-10/shuffled/mi10-000007.tar -> ./mini-imagenet-10/images/mi10-000007.tar DONE ./mini-imagenet-10/shuffled/mi10-000007.tar -> ./mini-imagenet-10/images/mi10-000007.tar START ./mini-imagenet-10/shuffled/mi10-000008.tar -> ./mini-imagenet-10/images/mi10-000008.tar DONE ./mini-imagenet-10/shuffled/mi10-000008.tar -> ./mini-imagenet-10/images/mi10-000008.tar START ./mini-imagenet-10/shuffled/mi10-000009.tar -> ./mini-imagenet-10/images/mi10-000009.tar DONE ./mini-imagenet-10/shuffled/mi10-000009.tar -> ./mini-imagenet-10/images/mi10-000009.tar START ./mini-imagenet-10/shuffled/mi10-000010.tar -> ./mini-imagenet-10/images/mi10-000010.tar DONE ./mini-imagenet-10/shuffled/mi10-000010.tar -> ./mini-imagenet-10/images/mi10-000010.tar START ./mini-imagenet-10/shuffled/mi10-000011.tar -> ./mini-imagenet-10/images/mi10-000011.tar DONE ./mini-imagenet-10/shuffled/mi10-000011.tar -> ./mini-imagenet-10/images/mi10-000011.tar START ./mini-imagenet-10/shuffled/mi10-000012.tar -> ./mini-imagenet-10/images/mi10-000012.tar DONE ./mini-imagenet-10/shuffled/mi10-000012.tar -> ./mini-imagenet-10/images/mi10-000012.tar START ./mini-imagenet-10/shuffled/mi10-000013.tar -> ./mini-imagenet-10/images/mi10-000013.tar DONE ./mini-imagenet-10/shuffled/mi10-000013.tar -> ./mini-imagenet-10/images/mi10-000013.tar START ./mini-imagenet-10/shuffled/mi10-000014.tar -> ./mini-imagenet-10/images/mi10-000014.tar DONE ./mini-imagenet-10/shuffled/mi10-000014.tar -> ./mini-imagenet-10/images/mi10-000014.tar START ./mini-imagenet-10/shuffled/mi10-000015.tar -> ./mini-imagenet-10/images/mi10-000015.tar DONE ./mini-imagenet-10/shuffled/mi10-000015.tar -> ./mini-imagenet-10/images/mi10-000015.tar START ./mini-imagenet-10/shuffled/mi10-000016.tar -> ./mini-imagenet-10/images/mi10-000016.tar DONE ./mini-imagenet-10/shuffled/mi10-000016.tar -> ./mini-imagenet-10/images/mi10-000016.tar START ./mini-imagenet-10/shuffled/mi10-000017.tar -> ./mini-imagenet-10/images/mi10-000017.tar DONE ./mini-imagenet-10/shuffled/mi10-000017.tar -> ./mini-imagenet-10/images/mi10-000017.tar START ./mini-imagenet-10/shuffled/mi10-000018.tar -> ./mini-imagenet-10/images/mi10-000018.tar DONE ./mini-imagenet-10/shuffled/mi10-000018.tar -> ./mini-imagenet-10/images/mi10-000018.tar START ./mini-imagenet-10/shuffled/mi10-000019.tar -> ./mini-imagenet-10/images/mi10-000019.tar DONE ./mini-imagenet-10/shuffled/mi10-000019.tar -> ./mini-imagenet-10/images/mi10-000019.tar START ./mini-imagenet-10/shuffled/mi10-000020.tar -> ./mini-imagenet-10/images/mi10-000020.tar DONE ./mini-imagenet-10/shuffled/mi10-000020.tar -> ./mini-imagenet-10/images/mi10-000020.tar START ./mini-imagenet-10/shuffled/mi10-000021.tar -> ./mini-imagenet-10/images/mi10-000021.tar DONE ./mini-imagenet-10/shuffled/mi10-000021.tar -> ./mini-imagenet-10/images/mi10-000021.tar START ./mini-imagenet-10/shuffled/mi10-000022.tar -> ./mini-imagenet-10/images/mi10-000022.tar DONE ./mini-imagenet-10/shuffled/mi10-000022.tar -> ./mini-imagenet-10/images/mi10-000022.tar START ./mini-imagenet-10/shuffled/mi10-000023.tar -> ./mini-imagenet-10/images/mi10-000023.tar DONE ./mini-imagenet-10/shuffled/mi10-000023.tar -> ./mini-imagenet-10/images/mi10-000023.tar START ./mini-imagenet-10/shuffled/mi10-000024.tar -> ./mini-imagenet-10/images/mi10-000024.tar DONE ./mini-imagenet-10/shuffled/mi10-000024.tar -> ./mini-imagenet-10/images/mi10-000024.tar START ./mini-imagenet-10/shuffled/mi10-000025.tar -> ./mini-imagenet-10/images/mi10-000025.tar DONE ./mini-imagenet-10/shuffled/mi10-000025.tar -> ./mini-imagenet-10/images/mi10-000025.tar START ./mini-imagenet-10/shuffled/mi10-000026.tar -> ./mini-imagenet-10/images/mi10-000026.tar DONE ./mini-imagenet-10/shuffled/mi10-000026.tar -> ./mini-imagenet-10/images/mi10-000026.tar START ./mini-imagenet-10/shuffled/mi10-000027.tar -> ./mini-imagenet-10/images/mi10-000027.tar DONE ./mini-imagenet-10/shuffled/mi10-000027.tar -> ./mini-imagenet-10/images/mi10-000027.tar START ./mini-imagenet-10/shuffled/mi10-000028.tar -> ./mini-imagenet-10/images/mi10-000028.tar DONE ./mini-imagenet-10/shuffled/mi10-000028.tar -> ./mini-imagenet-10/images/mi10-000028.tar START ./mini-imagenet-10/shuffled/mi10-000029.tar -> ./mini-imagenet-10/images/mi10-000029.tar DONE ./mini-imagenet-10/shuffled/mi10-000029.tar -> ./mini-imagenet-10/images/mi10-000029.tar START ./mini-imagenet-10/shuffled/mi10-000030.tar -> ./mini-imagenet-10/images/mi10-000030.tar DONE ./mini-imagenet-10/shuffled/mi10-000030.tar -> ./mini-imagenet-10/images/mi10-000030.tar START ./mini-imagenet-10/shuffled/mi10-000031.tar -> ./mini-imagenet-10/images/mi10-000031.tar DONE ./mini-imagenet-10/shuffled/mi10-000031.tar -> ./mini-imagenet-10/images/mi10-000031.tar START ./mini-imagenet-10/shuffled/mi10-000032.tar -> ./mini-imagenet-10/images/mi10-000032.tar DONE ./mini-imagenet-10/shuffled/mi10-000032.tar -> ./mini-imagenet-10/images/mi10-000032.tar START ./mini-imagenet-10/shuffled/mi10-000033.tar -> ./mini-imagenet-10/images/mi10-000033.tar DONE ./mini-imagenet-10/shuffled/mi10-000033.tar -> ./mini-imagenet-10/images/mi10-000033.tar START ./mini-imagenet-10/shuffled/mi10-000034.tar -> ./mini-imagenet-10/images/mi10-000034.tar DONE ./mini-imagenet-10/shuffled/mi10-000034.tar -> ./mini-imagenet-10/images/mi10-000034.tar START ./mini-imagenet-10/shuffled/mi10-000035.tar -> ./mini-imagenet-10/images/mi10-000035.tar DONE ./mini-imagenet-10/shuffled/mi10-000035.tar -> ./mini-imagenet-10/images/mi10-000035.tar START ./mini-imagenet-10/shuffled/mi10-000036.tar -> ./mini-imagenet-10/images/mi10-000036.tar DONE ./mini-imagenet-10/shuffled/mi10-000036.tar -> ./mini-imagenet-10/images/mi10-000036.tar START ./mini-imagenet-10/shuffled/mi10-000037.tar -> ./mini-imagenet-10/images/mi10-000037.tar DONE ./mini-imagenet-10/shuffled/mi10-000037.tar -> ./mini-imagenet-10/images/mi10-000037.tar START ./mini-imagenet-10/shuffled/mi10-000038.tar -> ./mini-imagenet-10/images/mi10-000038.tar DONE ./mini-imagenet-10/shuffled/mi10-000038.tar -> ./mini-imagenet-10/images/mi10-000038.tar START ./mini-imagenet-10/shuffled/mi10-000039.tar -> ./mini-imagenet-10/images/mi10-000039.tar DONE ./mini-imagenet-10/shuffled/mi10-000039.tar -> ./mini-imagenet-10/images/mi10-000039.tar START ./mini-imagenet-10/shuffled/mi10-000040.tar -> ./mini-imagenet-10/images/mi10-000040.tar DONE ./mini-imagenet-10/shuffled/mi10-000040.tar -> ./mini-imagenet-10/images/mi10-000040.tar START ./mini-imagenet-10/shuffled/mi10-000041.tar -> ./mini-imagenet-10/images/mi10-000041.tar DONE ./mini-imagenet-10/shuffled/mi10-000041.tar -> ./mini-imagenet-10/images/mi10-000041.tar START ./mini-imagenet-10/shuffled/mi10-000042.tar -> ./mini-imagenet-10/images/mi10-000042.tar DONE ./mini-imagenet-10/shuffled/mi10-000042.tar -> ./mini-imagenet-10/images/mi10-000042.tar START ./mini-imagenet-10/shuffled/mi10-000043.tar -> ./mini-imagenet-10/images/mi10-000043.tar DONE ./mini-imagenet-10/shuffled/mi10-000043.tar -> ./mini-imagenet-10/images/mi10-000043.tar START ./mini-imagenet-10/shuffled/mi10-000044.tar -> ./mini-imagenet-10/images/mi10-000044.tar DONE ./mini-imagenet-10/shuffled/mi10-000044.tar -> ./mini-imagenet-10/images/mi10-000044.tar START ./mini-imagenet-10/shuffled/mi10-000045.tar -> ./mini-imagenet-10/images/mi10-000045.tar DONE ./mini-imagenet-10/shuffled/mi10-000045.tar -> ./mini-imagenet-10/images/mi10-000045.tar START ./mini-imagenet-10/shuffled/mi10-000046.tar -> ./mini-imagenet-10/images/mi10-000046.tar DONE ./mini-imagenet-10/shuffled/mi10-000046.tar -> ./mini-imagenet-10/images/mi10-000046.tar START ./mini-imagenet-10/shuffled/mi10-000047.tar -> ./mini-imagenet-10/images/mi10-000047.tar DONE ./mini-imagenet-10/shuffled/mi10-000047.tar -> ./mini-imagenet-10/images/mi10-000047.tar START ./mini-imagenet-10/shuffled/mi10-000048.tar -> ./mini-imagenet-10/images/mi10-000048.tar DONE ./mini-imagenet-10/shuffled/mi10-000048.tar -> ./mini-imagenet-10/images/mi10-000048.tar START ./mini-imagenet-10/shuffled/mi10-000049.tar -> ./mini-imagenet-10/images/mi10-000049.tar DONE ./mini-imagenet-10/shuffled/mi10-000049.tar -> ./mini-imagenet-10/images/mi10-000049.tar START ./mini-imagenet-10/shuffled/mi10-000050.tar -> ./mini-imagenet-10/images/mi10-000050.tar DONE ./mini-imagenet-10/shuffled/mi10-000050.tar -> ./mini-imagenet-10/images/mi10-000050.tar START ./mini-imagenet-10/shuffled/mi10-000051.tar -> ./mini-imagenet-10/images/mi10-000051.tar DONE ./mini-imagenet-10/shuffled/mi10-000051.tar -> ./mini-imagenet-10/images/mi10-000051.tar START ./mini-imagenet-10/shuffled/mi10-000052.tar -> ./mini-imagenet-10/images/mi10-000052.tar DONE ./mini-imagenet-10/shuffled/mi10-000052.tar -> ./mini-imagenet-10/images/mi10-000052.tar START ./mini-imagenet-10/shuffled/mi10-000053.tar -> ./mini-imagenet-10/images/mi10-000053.tar DONE ./mini-imagenet-10/shuffled/mi10-000053.tar -> ./mini-imagenet-10/images/mi10-000053.tar START ./mini-imagenet-10/shuffled/mi10-000054.tar -> ./mini-imagenet-10/images/mi10-000054.tar DONE ./mini-imagenet-10/shuffled/mi10-000054.tar -> ./mini-imagenet-10/images/mi10-000054.tar START ./mini-imagenet-10/shuffled/mi10-000055.tar -> ./mini-imagenet-10/images/mi10-000055.tar DONE ./mini-imagenet-10/shuffled/mi10-000055.tar -> ./mini-imagenet-10/images/mi10-000055.tar START ./mini-imagenet-10/shuffled/mi10-000056.tar -> ./mini-imagenet-10/images/mi10-000056.tar DONE ./mini-imagenet-10/shuffled/mi10-000056.tar -> ./mini-imagenet-10/images/mi10-000056.tar START ./mini-imagenet-10/shuffled/mi10-000057.tar -> ./mini-imagenet-10/images/mi10-000057.tar DONE ./mini-imagenet-10/shuffled/mi10-000057.tar -> ./mini-imagenet-10/images/mi10-000057.tar START ./mini-imagenet-10/shuffled/mi10-000058.tar -> ./mini-imagenet-10/images/mi10-000058.tar DONE ./mini-imagenet-10/shuffled/mi10-000058.tar -> ./mini-imagenet-10/images/mi10-000058.tar START ./mini-imagenet-10/shuffled/mi10-000059.tar -> ./mini-imagenet-10/images/mi10-000059.tar DONE ./mini-imagenet-10/shuffled/mi10-000059.tar -> ./mini-imagenet-10/images/mi10-000059.tar START ./mini-imagenet-10/shuffled/mi10-000060.tar -> ./mini-imagenet-10/images/mi10-000060.tar DONE ./mini-imagenet-10/shuffled/mi10-000060.tar -> ./mini-imagenet-10/images/mi10-000060.tar START ./mini-imagenet-10/shuffled/mi10-000061.tar -> ./mini-imagenet-10/images/mi10-000061.tar DONE ./mini-imagenet-10/shuffled/mi10-000061.tar -> ./mini-imagenet-10/images/mi10-000061.tar START ./mini-imagenet-10/shuffled/mi10-000062.tar -> ./mini-imagenet-10/images/mi10-000062.tar DONE ./mini-imagenet-10/shuffled/mi10-000062.tar -> ./mini-imagenet-10/images/mi10-000062.tar START ./mini-imagenet-10/shuffled/mi10-000063.tar -> ./mini-imagenet-10/images/mi10-000063.tar DONE ./mini-imagenet-10/shuffled/mi10-000063.tar -> ./mini-imagenet-10/images/mi10-000063.tar START ./mini-imagenet-10/shuffled/mi10-000064.tar -> ./mini-imagenet-10/images/mi10-000064.tar DONE ./mini-imagenet-10/shuffled/mi10-000064.tar -> ./mini-imagenet-10/images/mi10-000064.tar START ./mini-imagenet-10/shuffled/mi10-000065.tar -> ./mini-imagenet-10/images/mi10-000065.tar DONE ./mini-imagenet-10/shuffled/mi10-000065.tar -> ./mini-imagenet-10/images/mi10-000065.tar START ./mini-imagenet-10/shuffled/mi10-000066.tar -> ./mini-imagenet-10/images/mi10-000066.tar DONE ./mini-imagenet-10/shuffled/mi10-000066.tar -> ./mini-imagenet-10/images/mi10-000066.tar START ./mini-imagenet-10/shuffled/mi10-000067.tar -> ./mini-imagenet-10/images/mi10-000067.tar DONE ./mini-imagenet-10/shuffled/mi10-000067.tar -> ./mini-imagenet-10/images/mi10-000067.tar START ./mini-imagenet-10/shuffled/mi10-000068.tar -> ./mini-imagenet-10/images/mi10-000068.tar DONE ./mini-imagenet-10/shuffled/mi10-000068.tar -> ./mini-imagenet-10/images/mi10-000068.tar START ./mini-imagenet-10/shuffled/mi10-000069.tar -> ./mini-imagenet-10/images/mi10-000069.tar DONE ./mini-imagenet-10/shuffled/mi10-000069.tar -> ./mini-imagenet-10/images/mi10-000069.tar START ./mini-imagenet-10/shuffled/mi10-000070.tar -> ./mini-imagenet-10/images/mi10-000070.tar DONE ./mini-imagenet-10/shuffled/mi10-000070.tar -> ./mini-imagenet-10/images/mi10-000070.tar START ./mini-imagenet-10/shuffled/mi10-000071.tar -> ./mini-imagenet-10/images/mi10-000071.tar DONE ./mini-imagenet-10/shuffled/mi10-000071.tar -> ./mini-imagenet-10/images/mi10-000071.tar START ./mini-imagenet-10/shuffled/mi10-000072.tar -> ./mini-imagenet-10/images/mi10-000072.tar DONE ./mini-imagenet-10/shuffled/mi10-000072.tar -> ./mini-imagenet-10/images/mi10-000072.tar START ./mini-imagenet-10/shuffled/mi10-000073.tar -> ./mini-imagenet-10/images/mi10-000073.tar DONE ./mini-imagenet-10/shuffled/mi10-000073.tar -> ./mini-imagenet-10/images/mi10-000073.tar START ./mini-imagenet-10/shuffled/mi10-000074.tar -> ./mini-imagenet-10/images/mi10-000074.tar DONE ./mini-imagenet-10/shuffled/mi10-000074.tar -> ./mini-imagenet-10/images/mi10-000074.tar START ./mini-imagenet-10/shuffled/mi10-000075.tar -> ./mini-imagenet-10/images/mi10-000075.tar DONE ./mini-imagenet-10/shuffled/mi10-000075.tar -> ./mini-imagenet-10/images/mi10-000075.tar START ./mini-imagenet-10/shuffled/mi10-000076.tar -> ./mini-imagenet-10/images/mi10-000076.tar DONE ./mini-imagenet-10/shuffled/mi10-000076.tar -> ./mini-imagenet-10/images/mi10-000076.tar START ./mini-imagenet-10/shuffled/mi10-000077.tar -> ./mini-imagenet-10/images/mi10-000077.tar DONE ./mini-imagenet-10/shuffled/mi10-000077.tar -> ./mini-imagenet-10/images/mi10-000077.tar START ./mini-imagenet-10/shuffled/mi10-000078.tar -> ./mini-imagenet-10/images/mi10-000078.tar DONE ./mini-imagenet-10/shuffled/mi10-000078.tar -> ./mini-imagenet-10/images/mi10-000078.tar START ./mini-imagenet-10/shuffled/mi10-000079.tar -> ./mini-imagenet-10/images/mi10-000079.tar DONE ./mini-imagenet-10/shuffled/mi10-000079.tar -> ./mini-imagenet-10/images/mi10-000079.tar START ./mini-imagenet-10/shuffled/mi10-000080.tar -> ./mini-imagenet-10/images/mi10-000080.tar DONE ./mini-imagenet-10/shuffled/mi10-000080.tar -> ./mini-imagenet-10/images/mi10-000080.tar START ./mini-imagenet-10/shuffled/mi10-000081.tar -> ./mini-imagenet-10/images/mi10-000081.tar DONE ./mini-imagenet-10/shuffled/mi10-000081.tar -> ./mini-imagenet-10/images/mi10-000081.tar START ./mini-imagenet-10/shuffled/mi10-000082.tar -> ./mini-imagenet-10/images/mi10-000082.tar DONE ./mini-imagenet-10/shuffled/mi10-000082.tar -> ./mini-imagenet-10/images/mi10-000082.tar START ./mini-imagenet-10/shuffled/mi10-000083.tar -> ./mini-imagenet-10/images/mi10-000083.tar DONE ./mini-imagenet-10/shuffled/mi10-000083.tar -> ./mini-imagenet-10/images/mi10-000083.tar START ./mini-imagenet-10/shuffled/mi10-000084.tar -> ./mini-imagenet-10/images/mi10-000084.tar DONE ./mini-imagenet-10/shuffled/mi10-000084.tar -> ./mini-imagenet-10/images/mi10-000084.tar START ./mini-imagenet-10/shuffled/mi10-000085.tar -> ./mini-imagenet-10/images/mi10-000085.tar DONE ./mini-imagenet-10/shuffled/mi10-000085.tar -> ./mini-imagenet-10/images/mi10-000085.tar START ./mini-imagenet-10/shuffled/mi10-000086.tar -> ./mini-imagenet-10/images/mi10-000086.tar DONE ./mini-imagenet-10/shuffled/mi10-000086.tar -> ./mini-imagenet-10/images/mi10-000086.tar START ./mini-imagenet-10/shuffled/mi10-000087.tar -> ./mini-imagenet-10/images/mi10-000087.tar DONE ./mini-imagenet-10/shuffled/mi10-000087.tar -> ./mini-imagenet-10/images/mi10-000087.tar START ./mini-imagenet-10/shuffled/mi10-000088.tar -> ./mini-imagenet-10/images/mi10-000088.tar DONE ./mini-imagenet-10/shuffled/mi10-000088.tar -> ./mini-imagenet-10/images/mi10-000088.tar START ./mini-imagenet-10/shuffled/mi10-000089.tar -> ./mini-imagenet-10/images/mi10-000089.tar DONE ./mini-imagenet-10/shuffled/mi10-000089.tar -> ./mini-imagenet-10/images/mi10-000089.tar START ./mini-imagenet-10/shuffled/mi10-000090.tar -> ./mini-imagenet-10/images/mi10-000090.tar DONE ./mini-imagenet-10/shuffled/mi10-000090.tar -> ./mini-imagenet-10/images/mi10-000090.tar START ./mini-imagenet-10/shuffled/mi10-000091.tar -> ./mini-imagenet-10/images/mi10-000091.tar DONE ./mini-imagenet-10/shuffled/mi10-000091.tar -> ./mini-imagenet-10/images/mi10-000091.tar START ./mini-imagenet-10/shuffled/mi10-000092.tar -> ./mini-imagenet-10/images/mi10-000092.tar DONE ./mini-imagenet-10/shuffled/mi10-000092.tar -> ./mini-imagenet-10/images/mi10-000092.tar START ./mini-imagenet-10/shuffled/mi10-000093.tar -> ./mini-imagenet-10/images/mi10-000093.tar DONE ./mini-imagenet-10/shuffled/mi10-000093.tar -> ./mini-imagenet-10/images/mi10-000093.tar START ./mini-imagenet-10/shuffled/mi10-000094.tar -> ./mini-imagenet-10/images/mi10-000094.tar DONE ./mini-imagenet-10/shuffled/mi10-000094.tar -> ./mini-imagenet-10/images/mi10-000094.tar START ./mini-imagenet-10/shuffled/mi10-000095.tar -> ./mini-imagenet-10/images/mi10-000095.tar DONE ./mini-imagenet-10/shuffled/mi10-000095.tar -> ./mini-imagenet-10/images/mi10-000095.tar START ./mini-imagenet-10/shuffled/mi10-000096.tar -> ./mini-imagenet-10/images/mi10-000096.tar DONE ./mini-imagenet-10/shuffled/mi10-000096.tar -> ./mini-imagenet-10/images/mi10-000096.tar START ./mini-imagenet-10/shuffled/mi10-000097.tar -> ./mini-imagenet-10/images/mi10-000097.tar DONE ./mini-imagenet-10/shuffled/mi10-000097.tar -> ./mini-imagenet-10/images/mi10-000097.tar START ./mini-imagenet-10/shuffled/mi10-000098.tar -> ./mini-imagenet-10/images/mi10-000098.tar DONE ./mini-imagenet-10/shuffled/mi10-000098.tar -> ./mini-imagenet-10/images/mi10-000098.tar START ./mini-imagenet-10/shuffled/mi10-000099.tar -> ./mini-imagenet-10/images/mi10-000099.tar DONE ./mini-imagenet-10/shuffled/mi10-000099.tar -> ./mini-imagenet-10/images/mi10-000099.tar START ./mini-imagenet-10/shuffled/mi10-000100.tar -> ./mini-imagenet-10/images/mi10-000100.tar DONE ./mini-imagenet-10/shuffled/mi10-000100.tar -> ./mini-imagenet-10/images/mi10-000100.tar START ./mini-imagenet-10/shuffled/mi10-000101.tar -> ./mini-imagenet-10/images/mi10-000101.tar DONE ./mini-imagenet-10/shuffled/mi10-000101.tar -> ./mini-imagenet-10/images/mi10-000101.tar START ./mini-imagenet-10/shuffled/mi10-000102.tar -> ./mini-imagenet-10/images/mi10-000102.tar DONE ./mini-imagenet-10/shuffled/mi10-000102.tar -> ./mini-imagenet-10/images/mi10-000102.tar START ./mini-imagenet-10/shuffled/mi10-000103.tar -> ./mini-imagenet-10/images/mi10-000103.tar DONE ./mini-imagenet-10/shuffled/mi10-000103.tar -> ./mini-imagenet-10/images/mi10-000103.tar START ./mini-imagenet-10/shuffled/mi10-000104.tar -> ./mini-imagenet-10/images/mi10-000104.tar DONE ./mini-imagenet-10/shuffled/mi10-000104.tar -> ./mini-imagenet-10/images/mi10-000104.tar START ./mini-imagenet-10/shuffled/mi10-000105.tar -> ./mini-imagenet-10/images/mi10-000105.tar DONE ./mini-imagenet-10/shuffled/mi10-000105.tar -> ./mini-imagenet-10/images/mi10-000105.tar START ./mini-imagenet-10/shuffled/mi10-000106.tar -> ./mini-imagenet-10/images/mi10-000106.tar DONE ./mini-imagenet-10/shuffled/mi10-000106.tar -> ./mini-imagenet-10/images/mi10-000106.tar START ./mini-imagenet-10/shuffled/mi10-000107.tar -> ./mini-imagenet-10/images/mi10-000107.tar DONE ./mini-imagenet-10/shuffled/mi10-000107.tar -> ./mini-imagenet-10/images/mi10-000107.tar START ./mini-imagenet-10/shuffled/mi10-000108.tar -> ./mini-imagenet-10/images/mi10-000108.tar DONE ./mini-imagenet-10/shuffled/mi10-000108.tar -> ./mini-imagenet-10/images/mi10-000108.tar START ./mini-imagenet-10/shuffled/mi10-000109.tar -> ./mini-imagenet-10/images/mi10-000109.tar DONE ./mini-imagenet-10/shuffled/mi10-000109.tar -> ./mini-imagenet-10/images/mi10-000109.tar START ./mini-imagenet-10/shuffled/mi10-000110.tar -> ./mini-imagenet-10/images/mi10-000110.tar DONE ./mini-imagenet-10/shuffled/mi10-000110.tar -> ./mini-imagenet-10/images/mi10-000110.tar START ./mini-imagenet-10/shuffled/mi10-000111.tar -> ./mini-imagenet-10/images/mi10-000111.tar DONE ./mini-imagenet-10/shuffled/mi10-000111.tar -> ./mini-imagenet-10/images/mi10-000111.tar START ./mini-imagenet-10/shuffled/mi10-000112.tar -> ./mini-imagenet-10/images/mi10-000112.tar DONE ./mini-imagenet-10/shuffled/mi10-000112.tar -> ./mini-imagenet-10/images/mi10-000112.tar START ./mini-imagenet-10/shuffled/mi10-000113.tar -> ./mini-imagenet-10/images/mi10-000113.tar DONE ./mini-imagenet-10/shuffled/mi10-000113.tar -> ./mini-imagenet-10/images/mi10-000113.tar START ./mini-imagenet-10/shuffled/mi10-000114.tar -> ./mini-imagenet-10/images/mi10-000114.tar DONE ./mini-imagenet-10/shuffled/mi10-000114.tar -> ./mini-imagenet-10/images/mi10-000114.tar START ./mini-imagenet-10/shuffled/mi10-000115.tar -> ./mini-imagenet-10/images/mi10-000115.tar DONE ./mini-imagenet-10/shuffled/mi10-000115.tar -> ./mini-imagenet-10/images/mi10-000115.tar START ./mini-imagenet-10/shuffled/mi10-000116.tar -> ./mini-imagenet-10/images/mi10-000116.tar DONE ./mini-imagenet-10/shuffled/mi10-000116.tar -> ./mini-imagenet-10/images/mi10-000116.tar START ./mini-imagenet-10/shuffled/mi10-000117.tar -> ./mini-imagenet-10/images/mi10-000117.tar DONE ./mini-imagenet-10/shuffled/mi10-000117.tar -> ./mini-imagenet-10/images/mi10-000117.tar START ./mini-imagenet-10/shuffled/mi10-000118.tar -> ./mini-imagenet-10/images/mi10-000118.tar DONE ./mini-imagenet-10/shuffled/mi10-000118.tar -> ./mini-imagenet-10/images/mi10-000118.tar START ./mini-imagenet-10/shuffled/mi10-000119.tar -> ./mini-imagenet-10/images/mi10-000119.tar DONE ./mini-imagenet-10/shuffled/mi10-000119.tar -> ./mini-imagenet-10/images/mi10-000119.tar START ./mini-imagenet-10/shuffled/mi10-000120.tar -> ./mini-imagenet-10/images/mi10-000120.tar DONE ./mini-imagenet-10/shuffled/mi10-000120.tar -> ./mini-imagenet-10/images/mi10-000120.tar START ./mini-imagenet-10/shuffled/mi10-000121.tar -> ./mini-imagenet-10/images/mi10-000121.tar DONE ./mini-imagenet-10/shuffled/mi10-000121.tar -> ./mini-imagenet-10/images/mi10-000121.tar START ./mini-imagenet-10/shuffled/mi10-000122.tar -> ./mini-imagenet-10/images/mi10-000122.tar DONE ./mini-imagenet-10/shuffled/mi10-000122.tar -> ./mini-imagenet-10/images/mi10-000122.tar START ./mini-imagenet-10/shuffled/mi10-000123.tar -> ./mini-imagenet-10/images/mi10-000123.tar DONE ./mini-imagenet-10/shuffled/mi10-000123.tar -> ./mini-imagenet-10/images/mi10-000123.tar START ./mini-imagenet-10/shuffled/mi10-000124.tar -> ./mini-imagenet-10/images/mi10-000124.tar DONE ./mini-imagenet-10/shuffled/mi10-000124.tar -> ./mini-imagenet-10/images/mi10-000124.tar START ./mini-imagenet-10/shuffled/mi10-000125.tar -> ./mini-imagenet-10/images/mi10-000125.tar DONE ./mini-imagenet-10/shuffled/mi10-000125.tar -> ./mini-imagenet-10/images/mi10-000125.tar START ./mini-imagenet-10/shuffled/mi10-000126.tar -> ./mini-imagenet-10/images/mi10-000126.tar DONE ./mini-imagenet-10/shuffled/mi10-000126.tar -> ./mini-imagenet-10/images/mi10-000126.tar START ./mini-imagenet-10/shuffled/mi10-000127.tar -> ./mini-imagenet-10/images/mi10-000127.tar DONE ./mini-imagenet-10/shuffled/mi10-000127.tar -> ./mini-imagenet-10/images/mi10-000127.tar START ./mini-imagenet-10/shuffled/mi10-000128.tar -> ./mini-imagenet-10/images/mi10-000128.tar DONE ./mini-imagenet-10/shuffled/mi10-000128.tar -> ./mini-imagenet-10/images/mi10-000128.tar START ./mini-imagenet-10/shuffled/mi10-000129.tar -> ./mini-imagenet-10/images/mi10-000129.tar DONE ./mini-imagenet-10/shuffled/mi10-000129.tar -> ./mini-imagenet-10/images/mi10-000129.tar START ./mini-imagenet-10/shuffled/mi10-000130.tar -> ./mini-imagenet-10/images/mi10-000130.tar DONE ./mini-imagenet-10/shuffled/mi10-000130.tar -> ./mini-imagenet-10/images/mi10-000130.tar START ./mini-imagenet-10/shuffled/mi10-000131.tar -> ./mini-imagenet-10/images/mi10-000131.tar DONE ./mini-imagenet-10/shuffled/mi10-000131.tar -> ./mini-imagenet-10/images/mi10-000131.tar START ./mini-imagenet-10/shuffled/mi10-000132.tar -> ./mini-imagenet-10/images/mi10-000132.tar DONE ./mini-imagenet-10/shuffled/mi10-000132.tar -> ./mini-imagenet-10/images/mi10-000132.tar START ./mini-imagenet-10/shuffled/mi10-000133.tar -> ./mini-imagenet-10/images/mi10-000133.tar DONE ./mini-imagenet-10/shuffled/mi10-000133.tar -> ./mini-imagenet-10/images/mi10-000133.tar START ./mini-imagenet-10/shuffled/mi10-000134.tar -> ./mini-imagenet-10/images/mi10-000134.tar DONE ./mini-imagenet-10/shuffled/mi10-000134.tar -> ./mini-imagenet-10/images/mi10-000134.tar START ./mini-imagenet-10/shuffled/mi10-000135.tar -> ./mini-imagenet-10/images/mi10-000135.tar DONE ./mini-imagenet-10/shuffled/mi10-000135.tar -> ./mini-imagenet-10/images/mi10-000135.tar START ./mini-imagenet-10/shuffled/mi10-000136.tar -> ./mini-imagenet-10/images/mi10-000136.tar DONE ./mini-imagenet-10/shuffled/mi10-000136.tar -> ./mini-imagenet-10/images/mi10-000136.tar START ./mini-imagenet-10/shuffled/mi10-000137.tar -> ./mini-imagenet-10/images/mi10-000137.tar DONE ./mini-imagenet-10/shuffled/mi10-000137.tar -> ./mini-imagenet-10/images/mi10-000137.tar START ./mini-imagenet-10/shuffled/mi10-000138.tar -> ./mini-imagenet-10/images/mi10-000138.tar DONE ./mini-imagenet-10/shuffled/mi10-000138.tar -> ./mini-imagenet-10/images/mi10-000138.tar START ./mini-imagenet-10/shuffled/mi10-000139.tar -> ./mini-imagenet-10/images/mi10-000139.tar DONE ./mini-imagenet-10/shuffled/mi10-000139.tar -> ./mini-imagenet-10/images/mi10-000139.tar START ./mini-imagenet-10/shuffled/mi10-000140.tar -> ./mini-imagenet-10/images/mi10-000140.tar DONE ./mini-imagenet-10/shuffled/mi10-000140.tar -> ./mini-imagenet-10/images/mi10-000140.tar START ./mini-imagenet-10/shuffled/mi10-000141.tar -> ./mini-imagenet-10/images/mi10-000141.tar DONE ./mini-imagenet-10/shuffled/mi10-000141.tar -> ./mini-imagenet-10/images/mi10-000141.tar START ./mini-imagenet-10/shuffled/mi10-000142.tar -> ./mini-imagenet-10/images/mi10-000142.tar DONE ./mini-imagenet-10/shuffled/mi10-000142.tar -> ./mini-imagenet-10/images/mi10-000142.tar START ./mini-imagenet-10/shuffled/mi10-000143.tar -> ./mini-imagenet-10/images/mi10-000143.tar DONE ./mini-imagenet-10/shuffled/mi10-000143.tar -> ./mini-imagenet-10/images/mi10-000143.tar START ./mini-imagenet-10/shuffled/mi10-000144.tar -> ./mini-imagenet-10/images/mi10-000144.tar DONE ./mini-imagenet-10/shuffled/mi10-000144.tar -> ./mini-imagenet-10/images/mi10-000144.tar START ./mini-imagenet-10/shuffled/mi10-000145.tar -> ./mini-imagenet-10/images/mi10-000145.tar DONE ./mini-imagenet-10/shuffled/mi10-000145.tar -> ./mini-imagenet-10/images/mi10-000145.tar START ./mini-imagenet-10/shuffled/mi10-000146.tar -> ./mini-imagenet-10/images/mi10-000146.tar DONE ./mini-imagenet-10/shuffled/mi10-000146.tar -> ./mini-imagenet-10/images/mi10-000146.tar START ./mini-imagenet-10/shuffled/mi10-000147.tar -> ./mini-imagenet-10/images/mi10-000147.tar DONE ./mini-imagenet-10/shuffled/mi10-000147.tar -> ./mini-imagenet-10/images/mi10-000147.tar START ./mini-imagenet-10/shuffled/mi10-000148.tar -> ./mini-imagenet-10/images/mi10-000148.tar DONE ./mini-imagenet-10/shuffled/mi10-000148.tar -> ./mini-imagenet-10/images/mi10-000148.tar START ./mini-imagenet-10/shuffled/mi10-000149.tar -> ./mini-imagenet-10/images/mi10-000149.tar DONE ./mini-imagenet-10/shuffled/mi10-000149.tar -> ./mini-imagenet-10/images/mi10-000149.tar START ./mini-imagenet-10/shuffled/mi10-000150.tar -> ./mini-imagenet-10/images/mi10-000150.tar DONE ./mini-imagenet-10/shuffled/mi10-000150.tar -> ./mini-imagenet-10/images/mi10-000150.tar START ./mini-imagenet-10/shuffled/mi10-000151.tar -> ./mini-imagenet-10/images/mi10-000151.tar DONE ./mini-imagenet-10/shuffled/mi10-000151.tar -> ./mini-imagenet-10/images/mi10-000151.tar START ./mini-imagenet-10/shuffled/mi10-000152.tar -> ./mini-imagenet-10/images/mi10-000152.tar DONE ./mini-imagenet-10/shuffled/mi10-000152.tar -> ./mini-imagenet-10/images/mi10-000152.tar START ./mini-imagenet-10/shuffled/mi10-000153.tar -> ./mini-imagenet-10/images/mi10-000153.tar DONE ./mini-imagenet-10/shuffled/mi10-000153.tar -> ./mini-imagenet-10/images/mi10-000153.tar START ./mini-imagenet-10/shuffled/mi10-000154.tar -> ./mini-imagenet-10/images/mi10-000154.tar DONE ./mini-imagenet-10/shuffled/mi10-000154.tar -> ./mini-imagenet-10/images/mi10-000154.tar START ./mini-imagenet-10/shuffled/mi10-000155.tar -> ./mini-imagenet-10/images/mi10-000155.tar DONE ./mini-imagenet-10/shuffled/mi10-000155.tar -> ./mini-imagenet-10/images/mi10-000155.tar START ./mini-imagenet-10/shuffled/mi10-000156.tar -> ./mini-imagenet-10/images/mi10-000156.tar DONE ./mini-imagenet-10/shuffled/mi10-000156.tar -> ./mini-imagenet-10/images/mi10-000156.tar START ./mini-imagenet-10/shuffled/mi10-000157.tar -> ./mini-imagenet-10/images/mi10-000157.tar DONE ./mini-imagenet-10/shuffled/mi10-000157.tar -> ./mini-imagenet-10/images/mi10-000157.tar START ./mini-imagenet-10/shuffled/mi10-000158.tar -> ./mini-imagenet-10/images/mi10-000158.tar DONE ./mini-imagenet-10/shuffled/mi10-000158.tar -> ./mini-imagenet-10/images/mi10-000158.tar START ./mini-imagenet-10/shuffled/mi10-000159.tar -> ./mini-imagenet-10/images/mi10-000159.tar DONE ./mini-imagenet-10/shuffled/mi10-000159.tar -> ./mini-imagenet-10/images/mi10-000159.tar START ./mini-imagenet-10/shuffled/mi10-000160.tar -> ./mini-imagenet-10/images/mi10-000160.tar DONE ./mini-imagenet-10/shuffled/mi10-000160.tar -> ./mini-imagenet-10/images/mi10-000160.tar START ./mini-imagenet-10/shuffled/mi10-000161.tar -> ./mini-imagenet-10/images/mi10-000161.tar DONE ./mini-imagenet-10/shuffled/mi10-000161.tar -> ./mini-imagenet-10/images/mi10-000161.tar START ./mini-imagenet-10/shuffled/mi10-000162.tar -> ./mini-imagenet-10/images/mi10-000162.tar DONE ./mini-imagenet-10/shuffled/mi10-000162.tar -> ./mini-imagenet-10/images/mi10-000162.tar START ./mini-imagenet-10/shuffled/mi10-000163.tar -> ./mini-imagenet-10/images/mi10-000163.tar DONE ./mini-imagenet-10/shuffled/mi10-000163.tar -> ./mini-imagenet-10/images/mi10-000163.tar START ./mini-imagenet-10/shuffled/mi10-000164.tar -> ./mini-imagenet-10/images/mi10-000164.tar DONE ./mini-imagenet-10/shuffled/mi10-000164.tar -> ./mini-imagenet-10/images/mi10-000164.tar START ./mini-imagenet-10/shuffled/mi10-000165.tar -> ./mini-imagenet-10/images/mi10-000165.tar DONE ./mini-imagenet-10/shuffled/mi10-000165.tar -> ./mini-imagenet-10/images/mi10-000165.tar START ./mini-imagenet-10/shuffled/mi10-000166.tar -> ./mini-imagenet-10/images/mi10-000166.tar DONE ./mini-imagenet-10/shuffled/mi10-000166.tar -> ./mini-imagenet-10/images/mi10-000166.tar START ./mini-imagenet-10/shuffled/mi10-000167.tar -> ./mini-imagenet-10/images/mi10-000167.tar DONE ./mini-imagenet-10/shuffled/mi10-000167.tar -> ./mini-imagenet-10/images/mi10-000167.tar START ./mini-imagenet-10/shuffled/mi10-000168.tar -> ./mini-imagenet-10/images/mi10-000168.tar DONE ./mini-imagenet-10/shuffled/mi10-000168.tar -> ./mini-imagenet-10/images/mi10-000168.tar START ./mini-imagenet-10/shuffled/mi10-000169.tar -> ./mini-imagenet-10/images/mi10-000169.tar DONE ./mini-imagenet-10/shuffled/mi10-000169.tar -> ./mini-imagenet-10/images/mi10-000169.tar START ./mini-imagenet-10/shuffled/mi10-000170.tar -> ./mini-imagenet-10/images/mi10-000170.tar DONE ./mini-imagenet-10/shuffled/mi10-000170.tar -> ./mini-imagenet-10/images/mi10-000170.tar START ./mini-imagenet-10/shuffled/mi10-000171.tar -> ./mini-imagenet-10/images/mi10-000171.tar DONE ./mini-imagenet-10/shuffled/mi10-000171.tar -> ./mini-imagenet-10/images/mi10-000171.tar START ./mini-imagenet-10/shuffled/mi10-000172.tar -> ./mini-imagenet-10/images/mi10-000172.tar DONE ./mini-imagenet-10/shuffled/mi10-000172.tar -> ./mini-imagenet-10/images/mi10-000172.tar START ./mini-imagenet-10/shuffled/mi10-000173.tar -> ./mini-imagenet-10/images/mi10-000173.tar DONE ./mini-imagenet-10/shuffled/mi10-000173.tar -> ./mini-imagenet-10/images/mi10-000173.tar START ./mini-imagenet-10/shuffled/mi10-000174.tar -> ./mini-imagenet-10/images/mi10-000174.tar DONE ./mini-imagenet-10/shuffled/mi10-000174.tar -> ./mini-imagenet-10/images/mi10-000174.tar START ./mini-imagenet-10/shuffled/mi10-000175.tar -> ./mini-imagenet-10/images/mi10-000175.tar DONE ./mini-imagenet-10/shuffled/mi10-000175.tar -> ./mini-imagenet-10/images/mi10-000175.tar START ./mini-imagenet-10/shuffled/mi10-000176.tar -> ./mini-imagenet-10/images/mi10-000176.tar DONE ./mini-imagenet-10/shuffled/mi10-000176.tar -> ./mini-imagenet-10/images/mi10-000176.tar START ./mini-imagenet-10/shuffled/mi10-000177.tar -> ./mini-imagenet-10/images/mi10-000177.tar DONE ./mini-imagenet-10/shuffled/mi10-000177.tar -> ./mini-imagenet-10/images/mi10-000177.tar START ./mini-imagenet-10/shuffled/mi10-000178.tar -> ./mini-imagenet-10/images/mi10-000178.tar DONE ./mini-imagenet-10/shuffled/mi10-000178.tar -> ./mini-imagenet-10/images/mi10-000178.tar START ./mini-imagenet-10/shuffled/mi10-000179.tar -> ./mini-imagenet-10/images/mi10-000179.tar DONE ./mini-imagenet-10/shuffled/mi10-000179.tar -> ./mini-imagenet-10/images/mi10-000179.tar START ./mini-imagenet-10/shuffled/mi10-000180.tar -> ./mini-imagenet-10/images/mi10-000180.tar DONE ./mini-imagenet-10/shuffled/mi10-000180.tar -> ./mini-imagenet-10/images/mi10-000180.tar START ./mini-imagenet-10/shuffled/mi10-000181.tar -> ./mini-imagenet-10/images/mi10-000181.tar DONE ./mini-imagenet-10/shuffled/mi10-000181.tar -> ./mini-imagenet-10/images/mi10-000181.tar START ./mini-imagenet-10/shuffled/mi10-000182.tar -> ./mini-imagenet-10/images/mi10-000182.tar DONE ./mini-imagenet-10/shuffled/mi10-000182.tar -> ./mini-imagenet-10/images/mi10-000182.tar START ./mini-imagenet-10/shuffled/mi10-000183.tar -> ./mini-imagenet-10/images/mi10-000183.tar DONE ./mini-imagenet-10/shuffled/mi10-000183.tar -> ./mini-imagenet-10/images/mi10-000183.tar START ./mini-imagenet-10/shuffled/mi10-000184.tar -> ./mini-imagenet-10/images/mi10-000184.tar DONE ./mini-imagenet-10/shuffled/mi10-000184.tar -> ./mini-imagenet-10/images/mi10-000184.tar START ./mini-imagenet-10/shuffled/mi10-000185.tar -> ./mini-imagenet-10/images/mi10-000185.tar DONE ./mini-imagenet-10/shuffled/mi10-000185.tar -> ./mini-imagenet-10/images/mi10-000185.tar START ./mini-imagenet-10/shuffled/mi10-000186.tar -> ./mini-imagenet-10/images/mi10-000186.tar DONE ./mini-imagenet-10/shuffled/mi10-000186.tar -> ./mini-imagenet-10/images/mi10-000186.tar START ./mini-imagenet-10/shuffled/mi10-000187.tar -> ./mini-imagenet-10/images/mi10-000187.tar DONE ./mini-imagenet-10/shuffled/mi10-000187.tar -> ./mini-imagenet-10/images/mi10-000187.tar START ./mini-imagenet-10/shuffled/mi10-000188.tar -> ./mini-imagenet-10/images/mi10-000188.tar DONE ./mini-imagenet-10/shuffled/mi10-000188.tar -> ./mini-imagenet-10/images/mi10-000188.tar START ./mini-imagenet-10/shuffled/mi10-000189.tar -> ./mini-imagenet-10/images/mi10-000189.tar DONE ./mini-imagenet-10/shuffled/mi10-000189.tar -> ./mini-imagenet-10/images/mi10-000189.tar START ./mini-imagenet-10/shuffled/mi10-000190.tar -> ./mini-imagenet-10/images/mi10-000190.tar DONE ./mini-imagenet-10/shuffled/mi10-000190.tar -> ./mini-imagenet-10/images/mi10-000190.tar START ./mini-imagenet-10/shuffled/mi10-000191.tar -> ./mini-imagenet-10/images/mi10-000191.tar DONE ./mini-imagenet-10/shuffled/mi10-000191.tar -> ./mini-imagenet-10/images/mi10-000191.tar START ./mini-imagenet-10/shuffled/mi10-000192.tar -> ./mini-imagenet-10/images/mi10-000192.tar DONE ./mini-imagenet-10/shuffled/mi10-000192.tar -> ./mini-imagenet-10/images/mi10-000192.tar START ./mini-imagenet-10/shuffled/mi10-000193.tar -> ./mini-imagenet-10/images/mi10-000193.tar DONE ./mini-imagenet-10/shuffled/mi10-000193.tar -> ./mini-imagenet-10/images/mi10-000193.tar START ./mini-imagenet-10/shuffled/mi10-000194.tar -> ./mini-imagenet-10/images/mi10-000194.tar DONE ./mini-imagenet-10/shuffled/mi10-000194.tar -> ./mini-imagenet-10/images/mi10-000194.tar START ./mini-imagenet-10/shuffled/mi10-000195.tar -> ./mini-imagenet-10/images/mi10-000195.tar DONE ./mini-imagenet-10/shuffled/mi10-000195.tar -> ./mini-imagenet-10/images/mi10-000195.tar START ./mini-imagenet-10/shuffled/mi10-000196.tar -> ./mini-imagenet-10/images/mi10-000196.tar DONE ./mini-imagenet-10/shuffled/mi10-000196.tar -> ./mini-imagenet-10/images/mi10-000196.tar START ./mini-imagenet-10/shuffled/mi10-000197.tar -> ./mini-imagenet-10/images/mi10-000197.tar DONE ./mini-imagenet-10/shuffled/mi10-000197.tar -> ./mini-imagenet-10/images/mi10-000197.tar START ./mini-imagenet-10/shuffled/mi10-000198.tar -> ./mini-imagenet-10/images/mi10-000198.tar DONE ./mini-imagenet-10/shuffled/mi10-000198.tar -> ./mini-imagenet-10/images/mi10-000198.tar START ./mini-imagenet-10/shuffled/mi10-000199.tar -> ./mini-imagenet-10/images/mi10-000199.tar DONE ./mini-imagenet-10/shuffled/mi10-000199.tar -> ./mini-imagenet-10/images/mi10-000199.tar START ./mini-imagenet-10/shuffled/mi10-000200.tar -> ./mini-imagenet-10/images/mi10-000200.tar DONE ./mini-imagenet-10/shuffled/mi10-000200.tar -> ./mini-imagenet-10/images/mi10-000200.tar START ./mini-imagenet-10/shuffled/mi10-000201.tar -> ./mini-imagenet-10/images/mi10-000201.tar DONE ./mini-imagenet-10/shuffled/mi10-000201.tar -> ./mini-imagenet-10/images/mi10-000201.tar START ./mini-imagenet-10/shuffled/mi10-000202.tar -> ./mini-imagenet-10/images/mi10-000202.tar DONE ./mini-imagenet-10/shuffled/mi10-000202.tar -> ./mini-imagenet-10/images/mi10-000202.tar START ./mini-imagenet-10/shuffled/mi10-000203.tar -> ./mini-imagenet-10/images/mi10-000203.tar DONE ./mini-imagenet-10/shuffled/mi10-000203.tar -> ./mini-imagenet-10/images/mi10-000203.tar START ./mini-imagenet-10/shuffled/mi10-000204.tar -> ./mini-imagenet-10/images/mi10-000204.tar DONE ./mini-imagenet-10/shuffled/mi10-000204.tar -> ./mini-imagenet-10/images/mi10-000204.tar START ./mini-imagenet-10/shuffled/mi10-000205.tar -> ./mini-imagenet-10/images/mi10-000205.tar DONE ./mini-imagenet-10/shuffled/mi10-000205.tar -> ./mini-imagenet-10/images/mi10-000205.tar START ./mini-imagenet-10/shuffled/mi10-000206.tar -> ./mini-imagenet-10/images/mi10-000206.tar DONE ./mini-imagenet-10/shuffled/mi10-000206.tar -> ./mini-imagenet-10/images/mi10-000206.tar START ./mini-imagenet-10/shuffled/mi10-000207.tar -> ./mini-imagenet-10/images/mi10-000207.tar DONE ./mini-imagenet-10/shuffled/mi10-000207.tar -> ./mini-imagenet-10/images/mi10-000207.tar START ./mini-imagenet-10/shuffled/mi10-000208.tar -> ./mini-imagenet-10/images/mi10-000208.tar DONE ./mini-imagenet-10/shuffled/mi10-000208.tar -> ./mini-imagenet-10/images/mi10-000208.tar START ./mini-imagenet-10/shuffled/mi10-000209.tar -> ./mini-imagenet-10/images/mi10-000209.tar DONE ./mini-imagenet-10/shuffled/mi10-000209.tar -> ./mini-imagenet-10/images/mi10-000209.tar START ./mini-imagenet-10/shuffled/mi10-000210.tar -> ./mini-imagenet-10/images/mi10-000210.tar DONE ./mini-imagenet-10/shuffled/mi10-000210.tar -> ./mini-imagenet-10/images/mi10-000210.tar START ./mini-imagenet-10/shuffled/mi10-000211.tar -> ./mini-imagenet-10/images/mi10-000211.tar DONE ./mini-imagenet-10/shuffled/mi10-000211.tar -> ./mini-imagenet-10/images/mi10-000211.tar START ./mini-imagenet-10/shuffled/mi10-000212.tar -> ./mini-imagenet-10/images/mi10-000212.tar DONE ./mini-imagenet-10/shuffled/mi10-000212.tar -> ./mini-imagenet-10/images/mi10-000212.tar START ./mini-imagenet-10/shuffled/mi10-000213.tar -> ./mini-imagenet-10/images/mi10-000213.tar DONE ./mini-imagenet-10/shuffled/mi10-000213.tar -> ./mini-imagenet-10/images/mi10-000213.tar START ./mini-imagenet-10/shuffled/mi10-000214.tar -> ./mini-imagenet-10/images/mi10-000214.tar DONE ./mini-imagenet-10/shuffled/mi10-000214.tar -> ./mini-imagenet-10/images/mi10-000214.tar START ./mini-imagenet-10/shuffled/mi10-000215.tar -> ./mini-imagenet-10/images/mi10-000215.tar DONE ./mini-imagenet-10/shuffled/mi10-000215.tar -> ./mini-imagenet-10/images/mi10-000215.tar START ./mini-imagenet-10/shuffled/mi10-000216.tar -> ./mini-imagenet-10/images/mi10-000216.tar DONE ./mini-imagenet-10/shuffled/mi10-000216.tar -> ./mini-imagenet-10/images/mi10-000216.tar START ./mini-imagenet-10/shuffled/mi10-000217.tar -> ./mini-imagenet-10/images/mi10-000217.tar DONE ./mini-imagenet-10/shuffled/mi10-000217.tar -> ./mini-imagenet-10/images/mi10-000217.tar START ./mini-imagenet-10/shuffled/mi10-000218.tar -> ./mini-imagenet-10/images/mi10-000218.tar DONE ./mini-imagenet-10/shuffled/mi10-000218.tar -> ./mini-imagenet-10/images/mi10-000218.tar START ./mini-imagenet-10/shuffled/mi10-000219.tar -> ./mini-imagenet-10/images/mi10-000219.tar DONE ./mini-imagenet-10/shuffled/mi10-000219.tar -> ./mini-imagenet-10/images/mi10-000219.tar START ./mini-imagenet-10/shuffled/mi10-000220.tar -> ./mini-imagenet-10/images/mi10-000220.tar DONE ./mini-imagenet-10/shuffled/mi10-000220.tar -> ./mini-imagenet-10/images/mi10-000220.tar START ./mini-imagenet-10/shuffled/mi10-000221.tar -> ./mini-imagenet-10/images/mi10-000221.tar DONE ./mini-imagenet-10/shuffled/mi10-000221.tar -> ./mini-imagenet-10/images/mi10-000221.tar START ./mini-imagenet-10/shuffled/mi10-000222.tar -> ./mini-imagenet-10/images/mi10-000222.tar DONE ./mini-imagenet-10/shuffled/mi10-000222.tar -> ./mini-imagenet-10/images/mi10-000222.tar START ./mini-imagenet-10/shuffled/mi10-000223.tar -> ./mini-imagenet-10/images/mi10-000223.tar DONE ./mini-imagenet-10/shuffled/mi10-000223.tar -> ./mini-imagenet-10/images/mi10-000223.tar START ./mini-imagenet-10/shuffled/mi10-000224.tar -> ./mini-imagenet-10/images/mi10-000224.tar DONE ./mini-imagenet-10/shuffled/mi10-000224.tar -> ./mini-imagenet-10/images/mi10-000224.tar START ./mini-imagenet-10/shuffled/mi10-000225.tar -> ./mini-imagenet-10/images/mi10-000225.tar DONE ./mini-imagenet-10/shuffled/mi10-000225.tar -> ./mini-imagenet-10/images/mi10-000225.tar START ./mini-imagenet-10/shuffled/mi10-000226.tar -> ./mini-imagenet-10/images/mi10-000226.tar DONE ./mini-imagenet-10/shuffled/mi10-000226.tar -> ./mini-imagenet-10/images/mi10-000226.tar START ./mini-imagenet-10/shuffled/mi10-000227.tar -> ./mini-imagenet-10/images/mi10-000227.tar DONE ./mini-imagenet-10/shuffled/mi10-000227.tar -> ./mini-imagenet-10/images/mi10-000227.tar START ./mini-imagenet-10/shuffled/mi10-000228.tar -> ./mini-imagenet-10/images/mi10-000228.tar DONE ./mini-imagenet-10/shuffled/mi10-000228.tar -> ./mini-imagenet-10/images/mi10-000228.tar START ./mini-imagenet-10/shuffled/mi10-000229.tar -> ./mini-imagenet-10/images/mi10-000229.tar DONE ./mini-imagenet-10/shuffled/mi10-000229.tar -> ./mini-imagenet-10/images/mi10-000229.tar START ./mini-imagenet-10/shuffled/mi10-000230.tar -> ./mini-imagenet-10/images/mi10-000230.tar DONE ./mini-imagenet-10/shuffled/mi10-000230.tar -> ./mini-imagenet-10/images/mi10-000230.tar START ./mini-imagenet-10/shuffled/mi10-000231.tar -> ./mini-imagenet-10/images/mi10-000231.tar DONE ./mini-imagenet-10/shuffled/mi10-000231.tar -> ./mini-imagenet-10/images/mi10-000231.tar START ./mini-imagenet-10/shuffled/mi10-000232.tar -> ./mini-imagenet-10/images/mi10-000232.tar DONE ./mini-imagenet-10/shuffled/mi10-000232.tar -> ./mini-imagenet-10/images/mi10-000232.tar START ./mini-imagenet-10/shuffled/mi10-000233.tar -> ./mini-imagenet-10/images/mi10-000233.tar DONE ./mini-imagenet-10/shuffled/mi10-000233.tar -> ./mini-imagenet-10/images/mi10-000233.tar START ./mini-imagenet-10/shuffled/mi10-000234.tar -> ./mini-imagenet-10/images/mi10-000234.tar DONE ./mini-imagenet-10/shuffled/mi10-000234.tar -> ./mini-imagenet-10/images/mi10-000234.tar START ./mini-imagenet-10/shuffled/mi10-000235.tar -> ./mini-imagenet-10/images/mi10-000235.tar DONE ./mini-imagenet-10/shuffled/mi10-000235.tar -> ./mini-imagenet-10/images/mi10-000235.tar START ./mini-imagenet-10/shuffled/mi10-000236.tar -> ./mini-imagenet-10/images/mi10-000236.tar DONE ./mini-imagenet-10/shuffled/mi10-000236.tar -> ./mini-imagenet-10/images/mi10-000236.tar START ./mini-imagenet-10/shuffled/mi10-000237.tar -> ./mini-imagenet-10/images/mi10-000237.tar DONE ./mini-imagenet-10/shuffled/mi10-000237.tar -> ./mini-imagenet-10/images/mi10-000237.tar START ./mini-imagenet-10/shuffled/mi10-000238.tar -> ./mini-imagenet-10/images/mi10-000238.tar DONE ./mini-imagenet-10/shuffled/mi10-000238.tar -> ./mini-imagenet-10/images/mi10-000238.tar START ./mini-imagenet-10/shuffled/mi10-000239.tar -> ./mini-imagenet-10/images/mi10-000239.tar DONE ./mini-imagenet-10/shuffled/mi10-000239.tar -> ./mini-imagenet-10/images/mi10-000239.tar START ./mini-imagenet-10/shuffled/mi10-000240.tar -> ./mini-imagenet-10/images/mi10-000240.tar DONE ./mini-imagenet-10/shuffled/mi10-000240.tar -> ./mini-imagenet-10/images/mi10-000240.tar START ./mini-imagenet-10/shuffled/mi10-000241.tar -> ./mini-imagenet-10/images/mi10-000241.tar DONE ./mini-imagenet-10/shuffled/mi10-000241.tar -> ./mini-imagenet-10/images/mi10-000241.tar START ./mini-imagenet-10/shuffled/mi10-000242.tar -> ./mini-imagenet-10/images/mi10-000242.tar DONE ./mini-imagenet-10/shuffled/mi10-000242.tar -> ./mini-imagenet-10/images/mi10-000242.tar START ./mini-imagenet-10/shuffled/mi10-000243.tar -> ./mini-imagenet-10/images/mi10-000243.tar DONE ./mini-imagenet-10/shuffled/mi10-000243.tar -> ./mini-imagenet-10/images/mi10-000243.tar START ./mini-imagenet-10/shuffled/mi10-000244.tar -> ./mini-imagenet-10/images/mi10-000244.tar DONE ./mini-imagenet-10/shuffled/mi10-000244.tar -> ./mini-imagenet-10/images/mi10-000244.tar START ./mini-imagenet-10/shuffled/mi10-000245.tar -> ./mini-imagenet-10/images/mi10-000245.tar DONE ./mini-imagenet-10/shuffled/mi10-000245.tar -> ./mini-imagenet-10/images/mi10-000245.tar START ./mini-imagenet-10/shuffled/mi10-000246.tar -> ./mini-imagenet-10/images/mi10-000246.tar DONE ./mini-imagenet-10/shuffled/mi10-000246.tar -> ./mini-imagenet-10/images/mi10-000246.tar START ./mini-imagenet-10/shuffled/mi10-000247.tar -> ./mini-imagenet-10/images/mi10-000247.tar DONE ./mini-imagenet-10/shuffled/mi10-000247.tar -> ./mini-imagenet-10/images/mi10-000247.tar START ./mini-imagenet-10/shuffled/mi10-000248.tar -> ./mini-imagenet-10/images/mi10-000248.tar DONE ./mini-imagenet-10/shuffled/mi10-000248.tar -> ./mini-imagenet-10/images/mi10-000248.tar START ./mini-imagenet-10/shuffled/mi10-000249.tar -> ./mini-imagenet-10/images/mi10-000249.tar DONE ./mini-imagenet-10/shuffled/mi10-000249.tar -> ./mini-imagenet-10/images/mi10-000249.tar START ./mini-imagenet-10/shuffled/mi10-000250.tar -> ./mini-imagenet-10/images/mi10-000250.tar DONE ./mini-imagenet-10/shuffled/mi10-000250.tar -> ./mini-imagenet-10/images/mi10-000250.tar START ./mini-imagenet-10/shuffled/mi10-000251.tar -> ./mini-imagenet-10/images/mi10-000251.tar DONE ./mini-imagenet-10/shuffled/mi10-000251.tar -> ./mini-imagenet-10/images/mi10-000251.tar START ./mini-imagenet-10/shuffled/mi10-000252.tar -> ./mini-imagenet-10/images/mi10-000252.tar DONE ./mini-imagenet-10/shuffled/mi10-000252.tar -> ./mini-imagenet-10/images/mi10-000252.tar START ./mini-imagenet-10/shuffled/mi10-000253.tar -> ./mini-imagenet-10/images/mi10-000253.tar DONE ./mini-imagenet-10/shuffled/mi10-000253.tar -> ./mini-imagenet-10/images/mi10-000253.tar START ./mini-imagenet-10/shuffled/mi10-000254.tar -> ./mini-imagenet-10/images/mi10-000254.tar DONE ./mini-imagenet-10/shuffled/mi10-000254.tar -> ./mini-imagenet-10/images/mi10-000254.tar START ./mini-imagenet-10/shuffled/mi10-000255.tar -> ./mini-imagenet-10/images/mi10-000255.tar DONE ./mini-imagenet-10/shuffled/mi10-000255.tar -> ./mini-imagenet-10/images/mi10-000255.tar START ./mini-imagenet-10/shuffled/mi10-000256.tar -> ./mini-imagenet-10/images/mi10-000256.tar DONE ./mini-imagenet-10/shuffled/mi10-000256.tar -> ./mini-imagenet-10/images/mi10-000256.tar START ./mini-imagenet-10/shuffled/mi10-000257.tar -> ./mini-imagenet-10/images/mi10-000257.tar DONE ./mini-imagenet-10/shuffled/mi10-000257.tar -> ./mini-imagenet-10/images/mi10-000257.tar START ./mini-imagenet-10/shuffled/mi10-000258.tar -> ./mini-imagenet-10/images/mi10-000258.tar DONE ./mini-imagenet-10/shuffled/mi10-000258.tar -> ./mini-imagenet-10/images/mi10-000258.tar START ./mini-imagenet-10/shuffled/mi10-000259.tar -> ./mini-imagenet-10/images/mi10-000259.tar DONE ./mini-imagenet-10/shuffled/mi10-000259.tar -> ./mini-imagenet-10/images/mi10-000259.tar START ./mini-imagenet-10/shuffled/mi10-000260.tar -> ./mini-imagenet-10/images/mi10-000260.tar DONE ./mini-imagenet-10/shuffled/mi10-000260.tar -> ./mini-imagenet-10/images/mi10-000260.tar START ./mini-imagenet-10/shuffled/mi10-000261.tar -> ./mini-imagenet-10/images/mi10-000261.tar DONE ./mini-imagenet-10/shuffled/mi10-000261.tar -> ./mini-imagenet-10/images/mi10-000261.tar START ./mini-imagenet-10/shuffled/mi10-000262.tar -> ./mini-imagenet-10/images/mi10-000262.tar DONE ./mini-imagenet-10/shuffled/mi10-000262.tar -> ./mini-imagenet-10/images/mi10-000262.tar START ./mini-imagenet-10/shuffled/mi10-000263.tar -> ./mini-imagenet-10/images/mi10-000263.tar DONE ./mini-imagenet-10/shuffled/mi10-000263.tar -> ./mini-imagenet-10/images/mi10-000263.tar START ./mini-imagenet-10/shuffled/mi10-000264.tar -> ./mini-imagenet-10/images/mi10-000264.tar DONE ./mini-imagenet-10/shuffled/mi10-000264.tar -> ./mini-imagenet-10/images/mi10-000264.tar START ./mini-imagenet-10/shuffled/mi10-000265.tar -> ./mini-imagenet-10/images/mi10-000265.tar DONE ./mini-imagenet-10/shuffled/mi10-000265.tar -> ./mini-imagenet-10/images/mi10-000265.tar START ./mini-imagenet-10/shuffled/mi10-000266.tar -> ./mini-imagenet-10/images/mi10-000266.tar DONE ./mini-imagenet-10/shuffled/mi10-000266.tar -> ./mini-imagenet-10/images/mi10-000266.tar START ./mini-imagenet-10/shuffled/mi10-000267.tar -> ./mini-imagenet-10/images/mi10-000267.tar DONE ./mini-imagenet-10/shuffled/mi10-000267.tar -> ./mini-imagenet-10/images/mi10-000267.tar START ./mini-imagenet-10/shuffled/mi10-000268.tar -> ./mini-imagenet-10/images/mi10-000268.tar DONE ./mini-imagenet-10/shuffled/mi10-000268.tar -> ./mini-imagenet-10/images/mi10-000268.tar START ./mini-imagenet-10/shuffled/mi10-000269.tar -> ./mini-imagenet-10/images/mi10-000269.tar DONE ./mini-imagenet-10/shuffled/mi10-000269.tar -> ./mini-imagenet-10/images/mi10-000269.tar START ./mini-imagenet-10/shuffled/mi10-000270.tar -> ./mini-imagenet-10/images/mi10-000270.tar DONE ./mini-imagenet-10/shuffled/mi10-000270.tar -> ./mini-imagenet-10/images/mi10-000270.tar START ./mini-imagenet-10/shuffled/mi10-000271.tar -> ./mini-imagenet-10/images/mi10-000271.tar DONE ./mini-imagenet-10/shuffled/mi10-000271.tar -> ./mini-imagenet-10/images/mi10-000271.tar START ./mini-imagenet-10/shuffled/mi10-000272.tar -> ./mini-imagenet-10/images/mi10-000272.tar DONE ./mini-imagenet-10/shuffled/mi10-000272.tar -> ./mini-imagenet-10/images/mi10-000272.tar START ./mini-imagenet-10/shuffled/mi10-000273.tar -> ./mini-imagenet-10/images/mi10-000273.tar DONE ./mini-imagenet-10/shuffled/mi10-000273.tar -> ./mini-imagenet-10/images/mi10-000273.tar START ./mini-imagenet-10/shuffled/mi10-000274.tar -> ./mini-imagenet-10/images/mi10-000274.tar DONE ./mini-imagenet-10/shuffled/mi10-000274.tar -> ./mini-imagenet-10/images/mi10-000274.tar START ./mini-imagenet-10/shuffled/mi10-000275.tar -> ./mini-imagenet-10/images/mi10-000275.tar DONE ./mini-imagenet-10/shuffled/mi10-000275.tar -> ./mini-imagenet-10/images/mi10-000275.tar START ./mini-imagenet-10/shuffled/mi10-000276.tar -> ./mini-imagenet-10/images/mi10-000276.tar DONE ./mini-imagenet-10/shuffled/mi10-000276.tar -> ./mini-imagenet-10/images/mi10-000276.tar START ./mini-imagenet-10/shuffled/mi10-000277.tar -> ./mini-imagenet-10/images/mi10-000277.tar DONE ./mini-imagenet-10/shuffled/mi10-000277.tar -> ./mini-imagenet-10/images/mi10-000277.tar START ./mini-imagenet-10/shuffled/mi10-000278.tar -> ./mini-imagenet-10/images/mi10-000278.tar DONE ./mini-imagenet-10/shuffled/mi10-000278.tar -> ./mini-imagenet-10/images/mi10-000278.tar START ./mini-imagenet-10/shuffled/mi10-000279.tar -> ./mini-imagenet-10/images/mi10-000279.tar DONE ./mini-imagenet-10/shuffled/mi10-000279.tar -> ./mini-imagenet-10/images/mi10-000279.tar START ./mini-imagenet-10/shuffled/mi10-000280.tar -> ./mini-imagenet-10/images/mi10-000280.tar DONE ./mini-imagenet-10/shuffled/mi10-000280.tar -> ./mini-imagenet-10/images/mi10-000280.tar START ./mini-imagenet-10/shuffled/mi10-000281.tar -> ./mini-imagenet-10/images/mi10-000281.tar DONE ./mini-imagenet-10/shuffled/mi10-000281.tar -> ./mini-imagenet-10/images/mi10-000281.tar START ./mini-imagenet-10/shuffled/mi10-000282.tar -> ./mini-imagenet-10/images/mi10-000282.tar DONE ./mini-imagenet-10/shuffled/mi10-000282.tar -> ./mini-imagenet-10/images/mi10-000282.tar START ./mini-imagenet-10/shuffled/mi10-000283.tar -> ./mini-imagenet-10/images/mi10-000283.tar DONE ./mini-imagenet-10/shuffled/mi10-000283.tar -> ./mini-imagenet-10/images/mi10-000283.tar START ./mini-imagenet-10/shuffled/mi10-000284.tar -> ./mini-imagenet-10/images/mi10-000284.tar DONE ./mini-imagenet-10/shuffled/mi10-000284.tar -> ./mini-imagenet-10/images/mi10-000284.tar START ./mini-imagenet-10/shuffled/mi10-000285.tar -> ./mini-imagenet-10/images/mi10-000285.tar DONE ./mini-imagenet-10/shuffled/mi10-000285.tar -> ./mini-imagenet-10/images/mi10-000285.tar START ./mini-imagenet-10/shuffled/mi10-000286.tar -> ./mini-imagenet-10/images/mi10-000286.tar DONE ./mini-imagenet-10/shuffled/mi10-000286.tar -> ./mini-imagenet-10/images/mi10-000286.tar START ./mini-imagenet-10/shuffled/mi10-000287.tar -> ./mini-imagenet-10/images/mi10-000287.tar DONE ./mini-imagenet-10/shuffled/mi10-000287.tar -> ./mini-imagenet-10/images/mi10-000287.tar START ./mini-imagenet-10/shuffled/mi10-000288.tar -> ./mini-imagenet-10/images/mi10-000288.tar DONE ./mini-imagenet-10/shuffled/mi10-000288.tar -> ./mini-imagenet-10/images/mi10-000288.tar START ./mini-imagenet-10/shuffled/mi10-000289.tar -> ./mini-imagenet-10/images/mi10-000289.tar DONE ./mini-imagenet-10/shuffled/mi10-000289.tar -> ./mini-imagenet-10/images/mi10-000289.tar START ./mini-imagenet-10/shuffled/mi10-000290.tar -> ./mini-imagenet-10/images/mi10-000290.tar DONE ./mini-imagenet-10/shuffled/mi10-000290.tar -> ./mini-imagenet-10/images/mi10-000290.tar START ./mini-imagenet-10/shuffled/mi10-000291.tar -> ./mini-imagenet-10/images/mi10-000291.tar DONE ./mini-imagenet-10/shuffled/mi10-000291.tar -> ./mini-imagenet-10/images/mi10-000291.tar START ./mini-imagenet-10/shuffled/mi10-000292.tar -> ./mini-imagenet-10/images/mi10-000292.tar DONE ./mini-imagenet-10/shuffled/mi10-000292.tar -> ./mini-imagenet-10/images/mi10-000292.tar START ./mini-imagenet-10/shuffled/mi10-000293.tar -> ./mini-imagenet-10/images/mi10-000293.tar DONE ./mini-imagenet-10/shuffled/mi10-000293.tar -> ./mini-imagenet-10/images/mi10-000293.tar START ./mini-imagenet-10/shuffled/mi10-000294.tar -> ./mini-imagenet-10/images/mi10-000294.tar DONE ./mini-imagenet-10/shuffled/mi10-000294.tar -> ./mini-imagenet-10/images/mi10-000294.tar START ./mini-imagenet-10/shuffled/mi10-000295.tar -> ./mini-imagenet-10/images/mi10-000295.tar DONE ./mini-imagenet-10/shuffled/mi10-000295.tar -> ./mini-imagenet-10/images/mi10-000295.tar START ./mini-imagenet-10/shuffled/mi10-000296.tar -> ./mini-imagenet-10/images/mi10-000296.tar DONE ./mini-imagenet-10/shuffled/mi10-000296.tar -> ./mini-imagenet-10/images/mi10-000296.tar START ./mini-imagenet-10/shuffled/mi10-000297.tar -> ./mini-imagenet-10/images/mi10-000297.tar DONE ./mini-imagenet-10/shuffled/mi10-000297.tar -> ./mini-imagenet-10/images/mi10-000297.tar START ./mini-imagenet-10/shuffled/mi10-000298.tar -> ./mini-imagenet-10/images/mi10-000298.tar DONE ./mini-imagenet-10/shuffled/mi10-000298.tar -> ./mini-imagenet-10/images/mi10-000298.tar START ./mini-imagenet-10/shuffled/mi10-000299.tar -> ./mini-imagenet-10/images/mi10-000299.tar DONE ./mini-imagenet-10/shuffled/mi10-000299.tar -> ./mini-imagenet-10/images/mi10-000299.tar START ./mini-imagenet-10/shuffled/mi10-000300.tar -> ./mini-imagenet-10/images/mi10-000300.tar DONE ./mini-imagenet-10/shuffled/mi10-000300.tar -> ./mini-imagenet-10/images/mi10-000300.tar START ./mini-imagenet-10/shuffled/mi10-000301.tar -> ./mini-imagenet-10/images/mi10-000301.tar DONE ./mini-imagenet-10/shuffled/mi10-000301.tar -> ./mini-imagenet-10/images/mi10-000301.tar START ./mini-imagenet-10/shuffled/mi10-000302.tar -> ./mini-imagenet-10/images/mi10-000302.tar DONE ./mini-imagenet-10/shuffled/mi10-000302.tar -> ./mini-imagenet-10/images/mi10-000302.tar START ./mini-imagenet-10/shuffled/mi10-000303.tar -> ./mini-imagenet-10/images/mi10-000303.tar DONE ./mini-imagenet-10/shuffled/mi10-000303.tar -> ./mini-imagenet-10/images/mi10-000303.tar START ./mini-imagenet-10/shuffled/mi10-000304.tar -> ./mini-imagenet-10/images/mi10-000304.tar DONE ./mini-imagenet-10/shuffled/mi10-000304.tar -> ./mini-imagenet-10/images/mi10-000304.tar START ./mini-imagenet-10/shuffled/mi10-000305.tar -> ./mini-imagenet-10/images/mi10-000305.tar DONE ./mini-imagenet-10/shuffled/mi10-000305.tar -> ./mini-imagenet-10/images/mi10-000305.tar START ./mini-imagenet-10/shuffled/mi10-000306.tar -> ./mini-imagenet-10/images/mi10-000306.tar DONE ./mini-imagenet-10/shuffled/mi10-000306.tar -> ./mini-imagenet-10/images/mi10-000306.tar START ./mini-imagenet-10/shuffled/mi10-000307.tar -> ./mini-imagenet-10/images/mi10-000307.tar DONE ./mini-imagenet-10/shuffled/mi10-000307.tar -> ./mini-imagenet-10/images/mi10-000307.tar","title":"Mini Imagenet Generation"},{"location":"mi-images/#mini-imagenet-generation","text":"This is one of a pair of notebooks used for generating an ImageNet-like dataset of training data using stable diffusion models. The difficulty of such artificial datasets can be easily tuned, and they are useful for debugging and testing deep learning applications. The first notebook uses Mistral-7B for taking class labels and generating descriptive prompts for image generation. The prompts are written out as shards to disk and shuffled. The process is parallelized using Ray. The second notebook uses Stable Diffustion to take descriptive prompts/image captions and renders them as image. This is a straightfowrard shard-to-shard transformation. Note that we are using explicit parallelization over shard files in the initial generation and the image generation, while we are using ray.data for the actual shuffling. That is because using explicit parallelization over shards makes it easier to restart jobs that have failed halfway through for some reason. %matplotlib inline import matplotlib.pyplot as plt from pprint import pprint import webdataset as wds from diffusers import AutoPipelineForText2Image import torch import warnings import logging import logging import tqdm from IPython.display import display, clear_output from PIL import Image as PILImage from itertools import islice import glob import os import io from contextlib import contextmanager import sys class SuppressWarning: def __enter__(self): logging.disable(logging.WARNING) def __exit__(self, type, value, traceback): logging.disable(logging.NOTSET) tqdm.tqdm.disable = True def get_num_gpus(): cluster_resources = ray.cluster_resources() return cluster_resources[\"GPU\"] @contextmanager def suppress_outputs(redirect): old_stdout = sys.stdout old_stderr = sys.stderr sys.stdout = redirect sys.stderr = redirect try: yield finally: sys.stdout = old_stdout sys.stderr = old_stderr # parameters odir = \"./mini-imagenet-10\" nactors = -1 check_sufficient = True actor_startup_wait = 10","title":"Mini Imagenet Generation"},{"location":"mi-images/#transformation-class","text":"We encapsulate the rendering into a RenderPrompts class. This class is instantiated once per GPU, loads the model, and then is ready to transform shards. \"\"\" class ShardTransformer: def __init__(self): self.pipe = AutoPipelineForText2Image.from_pretrained( \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\" ).to(\"cuda\") def transform_shard(self, input_shard, output_shard, display_in_notebook=False): ds = wds.WebDataset(input_shard).decode() output = wds.TarWriter(output_shard) for sample in ds: sample = dict(sample) text = sample[\"json\"][\"response\"] with SuppressWarning(): image = self.pipe(prompt=text, num_inference_steps=4, guidance_scale=0.1).images[0] sample[\"jpg\"] = image output.write(sample) if display_in_notebook: clear_output(wait=True) display(image) pprint(text) output.close() \"\"\" def maybe_clear_output(): try: clear_output(wait=True) except: pass class RenderPrompts: def __init__(self, display_in_notebook=False): self.display_in_notebook = display_in_notebook def gpu_is_sufficient(self): return torch.cuda.get_device_properties(0).total_memory > 10**10 def load_model(self): self.pipe = AutoPipelineForText2Image.from_pretrained( \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\" ).to(\"cuda\") def transform_sample(self, sample): text = sample[\"json\"][\"response\"] with SuppressWarning(): image = self.pipe(prompt=text, num_inference_steps=4, guidance_scale=0.1).images[0] sample[\"jpg\"] = image return sample def transform_sample_with_redirect(self, sample): stdout = io.StringIO() with suppress_outputs(stdout): sample = self.transform_sample(sample) sample[\"stdout\"] = stdout.getvalue() return sample def transform_shard(self, input_shard, output_shard, maxcount=999999999): ds = wds.WebDataset(input_shard).decode() output = wds.TarWriter(output_shard+\".temp\") for sample in islice(ds, maxcount): transformed_sample = self.transform_sample_with_redirect(dict(sample)) del transformed_sample[\"stdout\"] maybe_clear_output() output.write(transformed_sample) if self.display_in_notebook: clear_output(wait=True) display(transformed_sample['jpg']) pprint(transformed_sample[\"json\"][\"response\"]) output.close() os.rename(output_shard+\".temp\", output_shard) transformer = RenderPrompts(display_in_notebook=True) transformer.load_model() shards = glob.glob(f\"{odir}/shuffled/*.tar\") transformer.transform_shard(shards[0], \"temp.tar\", maxcount=10) del transformer ('\"A stunning black convertible car cruising down the picturesque countryside ' 'highway. The sleek silhouette of the car blends beautifully into the setting ' 'sun, leaving a trail of wanderers admiring its beauty and sophistication. ' 'The soft top is folded down, letting the sweet sound of the wind and sun-in ' 'take its occupants away')","title":"Transformation Class"},{"location":"mi-images/#parallelization-with-ray","text":"For parallel rendering, we use a Ray cluster. This will also work on a single machine with just one GPU. import ray if not ray.is_initialized(): ray.init(log_to_driver=False) @ray.remote(num_gpus=1) class RayRenderPrompts(RenderPrompts): def __init__(self): super().__init__() 2023-12-30 03:30:54,938 INFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m # Start up and create the actor pool. # This tries to adapt to the number of GPUs available. # It also checks that each actor has sufficient memory. # If not, set up your cluster differently by excluding GPUs that are too small. # (Ray's facilities for heterogenous clusters are somewhat limited) ngpus = get_num_gpus() if nactors == -1 else nactors print(f\"using {ngpus} actors\") actors = [RayRenderPrompts.remote() for i in range(int(ngpus))] print(\"loading the models\") for actor in actors: assert ray.get(actor.gpu_is_sufficient.remote()), \"GPU memory insufficient\" ray.get(actor.load_model.remote()) print(\"creating the pool\") pool = ray.util.ActorPool(actors) using 1.0 actors loading the models creating the pool import glob import os def apply_actor(actor, action): src, dst = action print(f\"START {src} -> {dst}\") result = actor.transform_shard.remote(src, dst) print(f\"DONE {src} -> {dst}\") return result !mkdir -p $odir/images shards = [os.path.basename(p) for p in sorted(glob.glob(f\"{odir}/shuffled/*.tar\"))] actions = [(f\"{odir}/shuffled/{shard}\", f\"{odir}/images/{shard}\") for shard in shards] result = list(pool.map(apply_actor, actions)) START ./mini-imagenet-10/shuffled/mi10-000000.tar -> ./mini-imagenet-10/images/mi10-000000.tar DONE ./mini-imagenet-10/shuffled/mi10-000000.tar -> ./mini-imagenet-10/images/mi10-000000.tar START ./mini-imagenet-10/shuffled/mi10-000001.tar -> ./mini-imagenet-10/images/mi10-000001.tar DONE ./mini-imagenet-10/shuffled/mi10-000001.tar -> ./mini-imagenet-10/images/mi10-000001.tar START ./mini-imagenet-10/shuffled/mi10-000002.tar -> ./mini-imagenet-10/images/mi10-000002.tar DONE ./mini-imagenet-10/shuffled/mi10-000002.tar -> ./mini-imagenet-10/images/mi10-000002.tar START ./mini-imagenet-10/shuffled/mi10-000003.tar -> ./mini-imagenet-10/images/mi10-000003.tar DONE ./mini-imagenet-10/shuffled/mi10-000003.tar -> ./mini-imagenet-10/images/mi10-000003.tar START ./mini-imagenet-10/shuffled/mi10-000004.tar -> ./mini-imagenet-10/images/mi10-000004.tar DONE ./mini-imagenet-10/shuffled/mi10-000004.tar -> ./mini-imagenet-10/images/mi10-000004.tar START ./mini-imagenet-10/shuffled/mi10-000005.tar -> ./mini-imagenet-10/images/mi10-000005.tar DONE ./mini-imagenet-10/shuffled/mi10-000005.tar -> ./mini-imagenet-10/images/mi10-000005.tar START ./mini-imagenet-10/shuffled/mi10-000006.tar -> ./mini-imagenet-10/images/mi10-000006.tar DONE ./mini-imagenet-10/shuffled/mi10-000006.tar -> ./mini-imagenet-10/images/mi10-000006.tar START ./mini-imagenet-10/shuffled/mi10-000007.tar -> ./mini-imagenet-10/images/mi10-000007.tar DONE ./mini-imagenet-10/shuffled/mi10-000007.tar -> ./mini-imagenet-10/images/mi10-000007.tar START ./mini-imagenet-10/shuffled/mi10-000008.tar -> ./mini-imagenet-10/images/mi10-000008.tar DONE ./mini-imagenet-10/shuffled/mi10-000008.tar -> ./mini-imagenet-10/images/mi10-000008.tar START ./mini-imagenet-10/shuffled/mi10-000009.tar -> ./mini-imagenet-10/images/mi10-000009.tar DONE ./mini-imagenet-10/shuffled/mi10-000009.tar -> ./mini-imagenet-10/images/mi10-000009.tar START ./mini-imagenet-10/shuffled/mi10-000010.tar -> ./mini-imagenet-10/images/mi10-000010.tar DONE ./mini-imagenet-10/shuffled/mi10-000010.tar -> ./mini-imagenet-10/images/mi10-000010.tar START ./mini-imagenet-10/shuffled/mi10-000011.tar -> ./mini-imagenet-10/images/mi10-000011.tar DONE ./mini-imagenet-10/shuffled/mi10-000011.tar -> ./mini-imagenet-10/images/mi10-000011.tar START ./mini-imagenet-10/shuffled/mi10-000012.tar -> ./mini-imagenet-10/images/mi10-000012.tar DONE ./mini-imagenet-10/shuffled/mi10-000012.tar -> ./mini-imagenet-10/images/mi10-000012.tar START ./mini-imagenet-10/shuffled/mi10-000013.tar -> ./mini-imagenet-10/images/mi10-000013.tar DONE ./mini-imagenet-10/shuffled/mi10-000013.tar -> ./mini-imagenet-10/images/mi10-000013.tar START ./mini-imagenet-10/shuffled/mi10-000014.tar -> ./mini-imagenet-10/images/mi10-000014.tar DONE ./mini-imagenet-10/shuffled/mi10-000014.tar -> ./mini-imagenet-10/images/mi10-000014.tar START ./mini-imagenet-10/shuffled/mi10-000015.tar -> ./mini-imagenet-10/images/mi10-000015.tar DONE ./mini-imagenet-10/shuffled/mi10-000015.tar -> ./mini-imagenet-10/images/mi10-000015.tar START ./mini-imagenet-10/shuffled/mi10-000016.tar -> ./mini-imagenet-10/images/mi10-000016.tar DONE ./mini-imagenet-10/shuffled/mi10-000016.tar -> ./mini-imagenet-10/images/mi10-000016.tar START ./mini-imagenet-10/shuffled/mi10-000017.tar -> ./mini-imagenet-10/images/mi10-000017.tar DONE ./mini-imagenet-10/shuffled/mi10-000017.tar -> ./mini-imagenet-10/images/mi10-000017.tar START ./mini-imagenet-10/shuffled/mi10-000018.tar -> ./mini-imagenet-10/images/mi10-000018.tar DONE ./mini-imagenet-10/shuffled/mi10-000018.tar -> ./mini-imagenet-10/images/mi10-000018.tar START ./mini-imagenet-10/shuffled/mi10-000019.tar -> ./mini-imagenet-10/images/mi10-000019.tar DONE ./mini-imagenet-10/shuffled/mi10-000019.tar -> ./mini-imagenet-10/images/mi10-000019.tar START ./mini-imagenet-10/shuffled/mi10-000020.tar -> ./mini-imagenet-10/images/mi10-000020.tar DONE ./mini-imagenet-10/shuffled/mi10-000020.tar -> ./mini-imagenet-10/images/mi10-000020.tar START ./mini-imagenet-10/shuffled/mi10-000021.tar -> ./mini-imagenet-10/images/mi10-000021.tar DONE ./mini-imagenet-10/shuffled/mi10-000021.tar -> ./mini-imagenet-10/images/mi10-000021.tar START ./mini-imagenet-10/shuffled/mi10-000022.tar -> ./mini-imagenet-10/images/mi10-000022.tar DONE ./mini-imagenet-10/shuffled/mi10-000022.tar -> ./mini-imagenet-10/images/mi10-000022.tar START ./mini-imagenet-10/shuffled/mi10-000023.tar -> ./mini-imagenet-10/images/mi10-000023.tar DONE ./mini-imagenet-10/shuffled/mi10-000023.tar -> ./mini-imagenet-10/images/mi10-000023.tar START ./mini-imagenet-10/shuffled/mi10-000024.tar -> ./mini-imagenet-10/images/mi10-000024.tar DONE ./mini-imagenet-10/shuffled/mi10-000024.tar -> ./mini-imagenet-10/images/mi10-000024.tar START ./mini-imagenet-10/shuffled/mi10-000025.tar -> ./mini-imagenet-10/images/mi10-000025.tar DONE ./mini-imagenet-10/shuffled/mi10-000025.tar -> ./mini-imagenet-10/images/mi10-000025.tar START ./mini-imagenet-10/shuffled/mi10-000026.tar -> ./mini-imagenet-10/images/mi10-000026.tar DONE ./mini-imagenet-10/shuffled/mi10-000026.tar -> ./mini-imagenet-10/images/mi10-000026.tar START ./mini-imagenet-10/shuffled/mi10-000027.tar -> ./mini-imagenet-10/images/mi10-000027.tar DONE ./mini-imagenet-10/shuffled/mi10-000027.tar -> ./mini-imagenet-10/images/mi10-000027.tar START ./mini-imagenet-10/shuffled/mi10-000028.tar -> ./mini-imagenet-10/images/mi10-000028.tar DONE ./mini-imagenet-10/shuffled/mi10-000028.tar -> ./mini-imagenet-10/images/mi10-000028.tar START ./mini-imagenet-10/shuffled/mi10-000029.tar -> ./mini-imagenet-10/images/mi10-000029.tar DONE ./mini-imagenet-10/shuffled/mi10-000029.tar -> ./mini-imagenet-10/images/mi10-000029.tar START ./mini-imagenet-10/shuffled/mi10-000030.tar -> ./mini-imagenet-10/images/mi10-000030.tar DONE ./mini-imagenet-10/shuffled/mi10-000030.tar -> ./mini-imagenet-10/images/mi10-000030.tar START ./mini-imagenet-10/shuffled/mi10-000031.tar -> ./mini-imagenet-10/images/mi10-000031.tar DONE ./mini-imagenet-10/shuffled/mi10-000031.tar -> ./mini-imagenet-10/images/mi10-000031.tar START ./mini-imagenet-10/shuffled/mi10-000032.tar -> ./mini-imagenet-10/images/mi10-000032.tar DONE ./mini-imagenet-10/shuffled/mi10-000032.tar -> ./mini-imagenet-10/images/mi10-000032.tar START ./mini-imagenet-10/shuffled/mi10-000033.tar -> ./mini-imagenet-10/images/mi10-000033.tar DONE ./mini-imagenet-10/shuffled/mi10-000033.tar -> ./mini-imagenet-10/images/mi10-000033.tar START ./mini-imagenet-10/shuffled/mi10-000034.tar -> ./mini-imagenet-10/images/mi10-000034.tar DONE ./mini-imagenet-10/shuffled/mi10-000034.tar -> ./mini-imagenet-10/images/mi10-000034.tar START ./mini-imagenet-10/shuffled/mi10-000035.tar -> ./mini-imagenet-10/images/mi10-000035.tar DONE ./mini-imagenet-10/shuffled/mi10-000035.tar -> ./mini-imagenet-10/images/mi10-000035.tar START ./mini-imagenet-10/shuffled/mi10-000036.tar -> ./mini-imagenet-10/images/mi10-000036.tar DONE ./mini-imagenet-10/shuffled/mi10-000036.tar -> ./mini-imagenet-10/images/mi10-000036.tar START ./mini-imagenet-10/shuffled/mi10-000037.tar -> ./mini-imagenet-10/images/mi10-000037.tar DONE ./mini-imagenet-10/shuffled/mi10-000037.tar -> ./mini-imagenet-10/images/mi10-000037.tar START ./mini-imagenet-10/shuffled/mi10-000038.tar -> ./mini-imagenet-10/images/mi10-000038.tar DONE ./mini-imagenet-10/shuffled/mi10-000038.tar -> ./mini-imagenet-10/images/mi10-000038.tar START ./mini-imagenet-10/shuffled/mi10-000039.tar -> ./mini-imagenet-10/images/mi10-000039.tar DONE ./mini-imagenet-10/shuffled/mi10-000039.tar -> ./mini-imagenet-10/images/mi10-000039.tar START ./mini-imagenet-10/shuffled/mi10-000040.tar -> ./mini-imagenet-10/images/mi10-000040.tar DONE ./mini-imagenet-10/shuffled/mi10-000040.tar -> ./mini-imagenet-10/images/mi10-000040.tar START ./mini-imagenet-10/shuffled/mi10-000041.tar -> ./mini-imagenet-10/images/mi10-000041.tar DONE ./mini-imagenet-10/shuffled/mi10-000041.tar -> ./mini-imagenet-10/images/mi10-000041.tar START ./mini-imagenet-10/shuffled/mi10-000042.tar -> ./mini-imagenet-10/images/mi10-000042.tar DONE ./mini-imagenet-10/shuffled/mi10-000042.tar -> ./mini-imagenet-10/images/mi10-000042.tar START ./mini-imagenet-10/shuffled/mi10-000043.tar -> ./mini-imagenet-10/images/mi10-000043.tar DONE ./mini-imagenet-10/shuffled/mi10-000043.tar -> ./mini-imagenet-10/images/mi10-000043.tar START ./mini-imagenet-10/shuffled/mi10-000044.tar -> ./mini-imagenet-10/images/mi10-000044.tar DONE ./mini-imagenet-10/shuffled/mi10-000044.tar -> ./mini-imagenet-10/images/mi10-000044.tar START ./mini-imagenet-10/shuffled/mi10-000045.tar -> ./mini-imagenet-10/images/mi10-000045.tar DONE ./mini-imagenet-10/shuffled/mi10-000045.tar -> ./mini-imagenet-10/images/mi10-000045.tar START ./mini-imagenet-10/shuffled/mi10-000046.tar -> ./mini-imagenet-10/images/mi10-000046.tar DONE ./mini-imagenet-10/shuffled/mi10-000046.tar -> ./mini-imagenet-10/images/mi10-000046.tar START ./mini-imagenet-10/shuffled/mi10-000047.tar -> ./mini-imagenet-10/images/mi10-000047.tar DONE ./mini-imagenet-10/shuffled/mi10-000047.tar -> ./mini-imagenet-10/images/mi10-000047.tar START ./mini-imagenet-10/shuffled/mi10-000048.tar -> ./mini-imagenet-10/images/mi10-000048.tar DONE ./mini-imagenet-10/shuffled/mi10-000048.tar -> ./mini-imagenet-10/images/mi10-000048.tar START ./mini-imagenet-10/shuffled/mi10-000049.tar -> ./mini-imagenet-10/images/mi10-000049.tar DONE ./mini-imagenet-10/shuffled/mi10-000049.tar -> ./mini-imagenet-10/images/mi10-000049.tar START ./mini-imagenet-10/shuffled/mi10-000050.tar -> ./mini-imagenet-10/images/mi10-000050.tar DONE ./mini-imagenet-10/shuffled/mi10-000050.tar -> ./mini-imagenet-10/images/mi10-000050.tar START ./mini-imagenet-10/shuffled/mi10-000051.tar -> ./mini-imagenet-10/images/mi10-000051.tar DONE ./mini-imagenet-10/shuffled/mi10-000051.tar -> ./mini-imagenet-10/images/mi10-000051.tar START ./mini-imagenet-10/shuffled/mi10-000052.tar -> ./mini-imagenet-10/images/mi10-000052.tar DONE ./mini-imagenet-10/shuffled/mi10-000052.tar -> ./mini-imagenet-10/images/mi10-000052.tar START ./mini-imagenet-10/shuffled/mi10-000053.tar -> ./mini-imagenet-10/images/mi10-000053.tar DONE ./mini-imagenet-10/shuffled/mi10-000053.tar -> ./mini-imagenet-10/images/mi10-000053.tar START ./mini-imagenet-10/shuffled/mi10-000054.tar -> ./mini-imagenet-10/images/mi10-000054.tar DONE ./mini-imagenet-10/shuffled/mi10-000054.tar -> ./mini-imagenet-10/images/mi10-000054.tar START ./mini-imagenet-10/shuffled/mi10-000055.tar -> ./mini-imagenet-10/images/mi10-000055.tar DONE ./mini-imagenet-10/shuffled/mi10-000055.tar -> ./mini-imagenet-10/images/mi10-000055.tar START ./mini-imagenet-10/shuffled/mi10-000056.tar -> ./mini-imagenet-10/images/mi10-000056.tar DONE ./mini-imagenet-10/shuffled/mi10-000056.tar -> ./mini-imagenet-10/images/mi10-000056.tar START ./mini-imagenet-10/shuffled/mi10-000057.tar -> ./mini-imagenet-10/images/mi10-000057.tar DONE ./mini-imagenet-10/shuffled/mi10-000057.tar -> ./mini-imagenet-10/images/mi10-000057.tar START ./mini-imagenet-10/shuffled/mi10-000058.tar -> ./mini-imagenet-10/images/mi10-000058.tar DONE ./mini-imagenet-10/shuffled/mi10-000058.tar -> ./mini-imagenet-10/images/mi10-000058.tar START ./mini-imagenet-10/shuffled/mi10-000059.tar -> ./mini-imagenet-10/images/mi10-000059.tar DONE ./mini-imagenet-10/shuffled/mi10-000059.tar -> ./mini-imagenet-10/images/mi10-000059.tar START ./mini-imagenet-10/shuffled/mi10-000060.tar -> ./mini-imagenet-10/images/mi10-000060.tar DONE ./mini-imagenet-10/shuffled/mi10-000060.tar -> ./mini-imagenet-10/images/mi10-000060.tar START ./mini-imagenet-10/shuffled/mi10-000061.tar -> ./mini-imagenet-10/images/mi10-000061.tar DONE ./mini-imagenet-10/shuffled/mi10-000061.tar -> ./mini-imagenet-10/images/mi10-000061.tar START ./mini-imagenet-10/shuffled/mi10-000062.tar -> ./mini-imagenet-10/images/mi10-000062.tar DONE ./mini-imagenet-10/shuffled/mi10-000062.tar -> ./mini-imagenet-10/images/mi10-000062.tar START ./mini-imagenet-10/shuffled/mi10-000063.tar -> ./mini-imagenet-10/images/mi10-000063.tar DONE ./mini-imagenet-10/shuffled/mi10-000063.tar -> ./mini-imagenet-10/images/mi10-000063.tar START ./mini-imagenet-10/shuffled/mi10-000064.tar -> ./mini-imagenet-10/images/mi10-000064.tar DONE ./mini-imagenet-10/shuffled/mi10-000064.tar -> ./mini-imagenet-10/images/mi10-000064.tar START ./mini-imagenet-10/shuffled/mi10-000065.tar -> ./mini-imagenet-10/images/mi10-000065.tar DONE ./mini-imagenet-10/shuffled/mi10-000065.tar -> ./mini-imagenet-10/images/mi10-000065.tar START ./mini-imagenet-10/shuffled/mi10-000066.tar -> ./mini-imagenet-10/images/mi10-000066.tar DONE ./mini-imagenet-10/shuffled/mi10-000066.tar -> ./mini-imagenet-10/images/mi10-000066.tar START ./mini-imagenet-10/shuffled/mi10-000067.tar -> ./mini-imagenet-10/images/mi10-000067.tar DONE ./mini-imagenet-10/shuffled/mi10-000067.tar -> ./mini-imagenet-10/images/mi10-000067.tar START ./mini-imagenet-10/shuffled/mi10-000068.tar -> ./mini-imagenet-10/images/mi10-000068.tar DONE ./mini-imagenet-10/shuffled/mi10-000068.tar -> ./mini-imagenet-10/images/mi10-000068.tar START ./mini-imagenet-10/shuffled/mi10-000069.tar -> ./mini-imagenet-10/images/mi10-000069.tar DONE ./mini-imagenet-10/shuffled/mi10-000069.tar -> ./mini-imagenet-10/images/mi10-000069.tar START ./mini-imagenet-10/shuffled/mi10-000070.tar -> ./mini-imagenet-10/images/mi10-000070.tar DONE ./mini-imagenet-10/shuffled/mi10-000070.tar -> ./mini-imagenet-10/images/mi10-000070.tar START ./mini-imagenet-10/shuffled/mi10-000071.tar -> ./mini-imagenet-10/images/mi10-000071.tar DONE ./mini-imagenet-10/shuffled/mi10-000071.tar -> ./mini-imagenet-10/images/mi10-000071.tar START ./mini-imagenet-10/shuffled/mi10-000072.tar -> ./mini-imagenet-10/images/mi10-000072.tar DONE ./mini-imagenet-10/shuffled/mi10-000072.tar -> ./mini-imagenet-10/images/mi10-000072.tar START ./mini-imagenet-10/shuffled/mi10-000073.tar -> ./mini-imagenet-10/images/mi10-000073.tar DONE ./mini-imagenet-10/shuffled/mi10-000073.tar -> ./mini-imagenet-10/images/mi10-000073.tar START ./mini-imagenet-10/shuffled/mi10-000074.tar -> ./mini-imagenet-10/images/mi10-000074.tar DONE ./mini-imagenet-10/shuffled/mi10-000074.tar -> ./mini-imagenet-10/images/mi10-000074.tar START ./mini-imagenet-10/shuffled/mi10-000075.tar -> ./mini-imagenet-10/images/mi10-000075.tar DONE ./mini-imagenet-10/shuffled/mi10-000075.tar -> ./mini-imagenet-10/images/mi10-000075.tar START ./mini-imagenet-10/shuffled/mi10-000076.tar -> ./mini-imagenet-10/images/mi10-000076.tar DONE ./mini-imagenet-10/shuffled/mi10-000076.tar -> ./mini-imagenet-10/images/mi10-000076.tar START ./mini-imagenet-10/shuffled/mi10-000077.tar -> ./mini-imagenet-10/images/mi10-000077.tar DONE ./mini-imagenet-10/shuffled/mi10-000077.tar -> ./mini-imagenet-10/images/mi10-000077.tar START ./mini-imagenet-10/shuffled/mi10-000078.tar -> ./mini-imagenet-10/images/mi10-000078.tar DONE ./mini-imagenet-10/shuffled/mi10-000078.tar -> ./mini-imagenet-10/images/mi10-000078.tar START ./mini-imagenet-10/shuffled/mi10-000079.tar -> ./mini-imagenet-10/images/mi10-000079.tar DONE ./mini-imagenet-10/shuffled/mi10-000079.tar -> ./mini-imagenet-10/images/mi10-000079.tar START ./mini-imagenet-10/shuffled/mi10-000080.tar -> ./mini-imagenet-10/images/mi10-000080.tar DONE ./mini-imagenet-10/shuffled/mi10-000080.tar -> ./mini-imagenet-10/images/mi10-000080.tar START ./mini-imagenet-10/shuffled/mi10-000081.tar -> ./mini-imagenet-10/images/mi10-000081.tar DONE ./mini-imagenet-10/shuffled/mi10-000081.tar -> ./mini-imagenet-10/images/mi10-000081.tar START ./mini-imagenet-10/shuffled/mi10-000082.tar -> ./mini-imagenet-10/images/mi10-000082.tar DONE ./mini-imagenet-10/shuffled/mi10-000082.tar -> ./mini-imagenet-10/images/mi10-000082.tar START ./mini-imagenet-10/shuffled/mi10-000083.tar -> ./mini-imagenet-10/images/mi10-000083.tar DONE ./mini-imagenet-10/shuffled/mi10-000083.tar -> ./mini-imagenet-10/images/mi10-000083.tar START ./mini-imagenet-10/shuffled/mi10-000084.tar -> ./mini-imagenet-10/images/mi10-000084.tar DONE ./mini-imagenet-10/shuffled/mi10-000084.tar -> ./mini-imagenet-10/images/mi10-000084.tar START ./mini-imagenet-10/shuffled/mi10-000085.tar -> ./mini-imagenet-10/images/mi10-000085.tar DONE ./mini-imagenet-10/shuffled/mi10-000085.tar -> ./mini-imagenet-10/images/mi10-000085.tar START ./mini-imagenet-10/shuffled/mi10-000086.tar -> ./mini-imagenet-10/images/mi10-000086.tar DONE ./mini-imagenet-10/shuffled/mi10-000086.tar -> ./mini-imagenet-10/images/mi10-000086.tar START ./mini-imagenet-10/shuffled/mi10-000087.tar -> ./mini-imagenet-10/images/mi10-000087.tar DONE ./mini-imagenet-10/shuffled/mi10-000087.tar -> ./mini-imagenet-10/images/mi10-000087.tar START ./mini-imagenet-10/shuffled/mi10-000088.tar -> ./mini-imagenet-10/images/mi10-000088.tar DONE ./mini-imagenet-10/shuffled/mi10-000088.tar -> ./mini-imagenet-10/images/mi10-000088.tar START ./mini-imagenet-10/shuffled/mi10-000089.tar -> ./mini-imagenet-10/images/mi10-000089.tar DONE ./mini-imagenet-10/shuffled/mi10-000089.tar -> ./mini-imagenet-10/images/mi10-000089.tar START ./mini-imagenet-10/shuffled/mi10-000090.tar -> ./mini-imagenet-10/images/mi10-000090.tar DONE ./mini-imagenet-10/shuffled/mi10-000090.tar -> ./mini-imagenet-10/images/mi10-000090.tar START ./mini-imagenet-10/shuffled/mi10-000091.tar -> ./mini-imagenet-10/images/mi10-000091.tar DONE ./mini-imagenet-10/shuffled/mi10-000091.tar -> ./mini-imagenet-10/images/mi10-000091.tar START ./mini-imagenet-10/shuffled/mi10-000092.tar -> ./mini-imagenet-10/images/mi10-000092.tar DONE ./mini-imagenet-10/shuffled/mi10-000092.tar -> ./mini-imagenet-10/images/mi10-000092.tar START ./mini-imagenet-10/shuffled/mi10-000093.tar -> ./mini-imagenet-10/images/mi10-000093.tar DONE ./mini-imagenet-10/shuffled/mi10-000093.tar -> ./mini-imagenet-10/images/mi10-000093.tar START ./mini-imagenet-10/shuffled/mi10-000094.tar -> ./mini-imagenet-10/images/mi10-000094.tar DONE ./mini-imagenet-10/shuffled/mi10-000094.tar -> ./mini-imagenet-10/images/mi10-000094.tar START ./mini-imagenet-10/shuffled/mi10-000095.tar -> ./mini-imagenet-10/images/mi10-000095.tar DONE ./mini-imagenet-10/shuffled/mi10-000095.tar -> ./mini-imagenet-10/images/mi10-000095.tar START ./mini-imagenet-10/shuffled/mi10-000096.tar -> ./mini-imagenet-10/images/mi10-000096.tar DONE ./mini-imagenet-10/shuffled/mi10-000096.tar -> ./mini-imagenet-10/images/mi10-000096.tar START ./mini-imagenet-10/shuffled/mi10-000097.tar -> ./mini-imagenet-10/images/mi10-000097.tar DONE ./mini-imagenet-10/shuffled/mi10-000097.tar -> ./mini-imagenet-10/images/mi10-000097.tar START ./mini-imagenet-10/shuffled/mi10-000098.tar -> ./mini-imagenet-10/images/mi10-000098.tar DONE ./mini-imagenet-10/shuffled/mi10-000098.tar -> ./mini-imagenet-10/images/mi10-000098.tar START ./mini-imagenet-10/shuffled/mi10-000099.tar -> ./mini-imagenet-10/images/mi10-000099.tar DONE ./mini-imagenet-10/shuffled/mi10-000099.tar -> ./mini-imagenet-10/images/mi10-000099.tar START ./mini-imagenet-10/shuffled/mi10-000100.tar -> ./mini-imagenet-10/images/mi10-000100.tar DONE ./mini-imagenet-10/shuffled/mi10-000100.tar -> ./mini-imagenet-10/images/mi10-000100.tar START ./mini-imagenet-10/shuffled/mi10-000101.tar -> ./mini-imagenet-10/images/mi10-000101.tar DONE ./mini-imagenet-10/shuffled/mi10-000101.tar -> ./mini-imagenet-10/images/mi10-000101.tar START ./mini-imagenet-10/shuffled/mi10-000102.tar -> ./mini-imagenet-10/images/mi10-000102.tar DONE ./mini-imagenet-10/shuffled/mi10-000102.tar -> ./mini-imagenet-10/images/mi10-000102.tar START ./mini-imagenet-10/shuffled/mi10-000103.tar -> ./mini-imagenet-10/images/mi10-000103.tar DONE ./mini-imagenet-10/shuffled/mi10-000103.tar -> ./mini-imagenet-10/images/mi10-000103.tar START ./mini-imagenet-10/shuffled/mi10-000104.tar -> ./mini-imagenet-10/images/mi10-000104.tar DONE ./mini-imagenet-10/shuffled/mi10-000104.tar -> ./mini-imagenet-10/images/mi10-000104.tar START ./mini-imagenet-10/shuffled/mi10-000105.tar -> ./mini-imagenet-10/images/mi10-000105.tar DONE ./mini-imagenet-10/shuffled/mi10-000105.tar -> ./mini-imagenet-10/images/mi10-000105.tar START ./mini-imagenet-10/shuffled/mi10-000106.tar -> ./mini-imagenet-10/images/mi10-000106.tar DONE ./mini-imagenet-10/shuffled/mi10-000106.tar -> ./mini-imagenet-10/images/mi10-000106.tar START ./mini-imagenet-10/shuffled/mi10-000107.tar -> ./mini-imagenet-10/images/mi10-000107.tar DONE ./mini-imagenet-10/shuffled/mi10-000107.tar -> ./mini-imagenet-10/images/mi10-000107.tar START ./mini-imagenet-10/shuffled/mi10-000108.tar -> ./mini-imagenet-10/images/mi10-000108.tar DONE ./mini-imagenet-10/shuffled/mi10-000108.tar -> ./mini-imagenet-10/images/mi10-000108.tar START ./mini-imagenet-10/shuffled/mi10-000109.tar -> ./mini-imagenet-10/images/mi10-000109.tar DONE ./mini-imagenet-10/shuffled/mi10-000109.tar -> ./mini-imagenet-10/images/mi10-000109.tar START ./mini-imagenet-10/shuffled/mi10-000110.tar -> ./mini-imagenet-10/images/mi10-000110.tar DONE ./mini-imagenet-10/shuffled/mi10-000110.tar -> ./mini-imagenet-10/images/mi10-000110.tar START ./mini-imagenet-10/shuffled/mi10-000111.tar -> ./mini-imagenet-10/images/mi10-000111.tar DONE ./mini-imagenet-10/shuffled/mi10-000111.tar -> ./mini-imagenet-10/images/mi10-000111.tar START ./mini-imagenet-10/shuffled/mi10-000112.tar -> ./mini-imagenet-10/images/mi10-000112.tar DONE ./mini-imagenet-10/shuffled/mi10-000112.tar -> ./mini-imagenet-10/images/mi10-000112.tar START ./mini-imagenet-10/shuffled/mi10-000113.tar -> ./mini-imagenet-10/images/mi10-000113.tar DONE ./mini-imagenet-10/shuffled/mi10-000113.tar -> ./mini-imagenet-10/images/mi10-000113.tar START ./mini-imagenet-10/shuffled/mi10-000114.tar -> ./mini-imagenet-10/images/mi10-000114.tar DONE ./mini-imagenet-10/shuffled/mi10-000114.tar -> ./mini-imagenet-10/images/mi10-000114.tar START ./mini-imagenet-10/shuffled/mi10-000115.tar -> ./mini-imagenet-10/images/mi10-000115.tar DONE ./mini-imagenet-10/shuffled/mi10-000115.tar -> ./mini-imagenet-10/images/mi10-000115.tar START ./mini-imagenet-10/shuffled/mi10-000116.tar -> ./mini-imagenet-10/images/mi10-000116.tar DONE ./mini-imagenet-10/shuffled/mi10-000116.tar -> ./mini-imagenet-10/images/mi10-000116.tar START ./mini-imagenet-10/shuffled/mi10-000117.tar -> ./mini-imagenet-10/images/mi10-000117.tar DONE ./mini-imagenet-10/shuffled/mi10-000117.tar -> ./mini-imagenet-10/images/mi10-000117.tar START ./mini-imagenet-10/shuffled/mi10-000118.tar -> ./mini-imagenet-10/images/mi10-000118.tar DONE ./mini-imagenet-10/shuffled/mi10-000118.tar -> ./mini-imagenet-10/images/mi10-000118.tar START ./mini-imagenet-10/shuffled/mi10-000119.tar -> ./mini-imagenet-10/images/mi10-000119.tar DONE ./mini-imagenet-10/shuffled/mi10-000119.tar -> ./mini-imagenet-10/images/mi10-000119.tar START ./mini-imagenet-10/shuffled/mi10-000120.tar -> ./mini-imagenet-10/images/mi10-000120.tar DONE ./mini-imagenet-10/shuffled/mi10-000120.tar -> ./mini-imagenet-10/images/mi10-000120.tar START ./mini-imagenet-10/shuffled/mi10-000121.tar -> ./mini-imagenet-10/images/mi10-000121.tar DONE ./mini-imagenet-10/shuffled/mi10-000121.tar -> ./mini-imagenet-10/images/mi10-000121.tar START ./mini-imagenet-10/shuffled/mi10-000122.tar -> ./mini-imagenet-10/images/mi10-000122.tar DONE ./mini-imagenet-10/shuffled/mi10-000122.tar -> ./mini-imagenet-10/images/mi10-000122.tar START ./mini-imagenet-10/shuffled/mi10-000123.tar -> ./mini-imagenet-10/images/mi10-000123.tar DONE ./mini-imagenet-10/shuffled/mi10-000123.tar -> ./mini-imagenet-10/images/mi10-000123.tar START ./mini-imagenet-10/shuffled/mi10-000124.tar -> ./mini-imagenet-10/images/mi10-000124.tar DONE ./mini-imagenet-10/shuffled/mi10-000124.tar -> ./mini-imagenet-10/images/mi10-000124.tar START ./mini-imagenet-10/shuffled/mi10-000125.tar -> ./mini-imagenet-10/images/mi10-000125.tar DONE ./mini-imagenet-10/shuffled/mi10-000125.tar -> ./mini-imagenet-10/images/mi10-000125.tar START ./mini-imagenet-10/shuffled/mi10-000126.tar -> ./mini-imagenet-10/images/mi10-000126.tar DONE ./mini-imagenet-10/shuffled/mi10-000126.tar -> ./mini-imagenet-10/images/mi10-000126.tar START ./mini-imagenet-10/shuffled/mi10-000127.tar -> ./mini-imagenet-10/images/mi10-000127.tar DONE ./mini-imagenet-10/shuffled/mi10-000127.tar -> ./mini-imagenet-10/images/mi10-000127.tar START ./mini-imagenet-10/shuffled/mi10-000128.tar -> ./mini-imagenet-10/images/mi10-000128.tar DONE ./mini-imagenet-10/shuffled/mi10-000128.tar -> ./mini-imagenet-10/images/mi10-000128.tar START ./mini-imagenet-10/shuffled/mi10-000129.tar -> ./mini-imagenet-10/images/mi10-000129.tar DONE ./mini-imagenet-10/shuffled/mi10-000129.tar -> ./mini-imagenet-10/images/mi10-000129.tar START ./mini-imagenet-10/shuffled/mi10-000130.tar -> ./mini-imagenet-10/images/mi10-000130.tar DONE ./mini-imagenet-10/shuffled/mi10-000130.tar -> ./mini-imagenet-10/images/mi10-000130.tar START ./mini-imagenet-10/shuffled/mi10-000131.tar -> ./mini-imagenet-10/images/mi10-000131.tar DONE ./mini-imagenet-10/shuffled/mi10-000131.tar -> ./mini-imagenet-10/images/mi10-000131.tar START ./mini-imagenet-10/shuffled/mi10-000132.tar -> ./mini-imagenet-10/images/mi10-000132.tar DONE ./mini-imagenet-10/shuffled/mi10-000132.tar -> ./mini-imagenet-10/images/mi10-000132.tar START ./mini-imagenet-10/shuffled/mi10-000133.tar -> ./mini-imagenet-10/images/mi10-000133.tar DONE ./mini-imagenet-10/shuffled/mi10-000133.tar -> ./mini-imagenet-10/images/mi10-000133.tar START ./mini-imagenet-10/shuffled/mi10-000134.tar -> ./mini-imagenet-10/images/mi10-000134.tar DONE ./mini-imagenet-10/shuffled/mi10-000134.tar -> ./mini-imagenet-10/images/mi10-000134.tar START ./mini-imagenet-10/shuffled/mi10-000135.tar -> ./mini-imagenet-10/images/mi10-000135.tar DONE ./mini-imagenet-10/shuffled/mi10-000135.tar -> ./mini-imagenet-10/images/mi10-000135.tar START ./mini-imagenet-10/shuffled/mi10-000136.tar -> ./mini-imagenet-10/images/mi10-000136.tar DONE ./mini-imagenet-10/shuffled/mi10-000136.tar -> ./mini-imagenet-10/images/mi10-000136.tar START ./mini-imagenet-10/shuffled/mi10-000137.tar -> ./mini-imagenet-10/images/mi10-000137.tar DONE ./mini-imagenet-10/shuffled/mi10-000137.tar -> ./mini-imagenet-10/images/mi10-000137.tar START ./mini-imagenet-10/shuffled/mi10-000138.tar -> ./mini-imagenet-10/images/mi10-000138.tar DONE ./mini-imagenet-10/shuffled/mi10-000138.tar -> ./mini-imagenet-10/images/mi10-000138.tar START ./mini-imagenet-10/shuffled/mi10-000139.tar -> ./mini-imagenet-10/images/mi10-000139.tar DONE ./mini-imagenet-10/shuffled/mi10-000139.tar -> ./mini-imagenet-10/images/mi10-000139.tar START ./mini-imagenet-10/shuffled/mi10-000140.tar -> ./mini-imagenet-10/images/mi10-000140.tar DONE ./mini-imagenet-10/shuffled/mi10-000140.tar -> ./mini-imagenet-10/images/mi10-000140.tar START ./mini-imagenet-10/shuffled/mi10-000141.tar -> ./mini-imagenet-10/images/mi10-000141.tar DONE ./mini-imagenet-10/shuffled/mi10-000141.tar -> ./mini-imagenet-10/images/mi10-000141.tar START ./mini-imagenet-10/shuffled/mi10-000142.tar -> ./mini-imagenet-10/images/mi10-000142.tar DONE ./mini-imagenet-10/shuffled/mi10-000142.tar -> ./mini-imagenet-10/images/mi10-000142.tar START ./mini-imagenet-10/shuffled/mi10-000143.tar -> ./mini-imagenet-10/images/mi10-000143.tar DONE ./mini-imagenet-10/shuffled/mi10-000143.tar -> ./mini-imagenet-10/images/mi10-000143.tar START ./mini-imagenet-10/shuffled/mi10-000144.tar -> ./mini-imagenet-10/images/mi10-000144.tar DONE ./mini-imagenet-10/shuffled/mi10-000144.tar -> ./mini-imagenet-10/images/mi10-000144.tar START ./mini-imagenet-10/shuffled/mi10-000145.tar -> ./mini-imagenet-10/images/mi10-000145.tar DONE ./mini-imagenet-10/shuffled/mi10-000145.tar -> ./mini-imagenet-10/images/mi10-000145.tar START ./mini-imagenet-10/shuffled/mi10-000146.tar -> ./mini-imagenet-10/images/mi10-000146.tar DONE ./mini-imagenet-10/shuffled/mi10-000146.tar -> ./mini-imagenet-10/images/mi10-000146.tar START ./mini-imagenet-10/shuffled/mi10-000147.tar -> ./mini-imagenet-10/images/mi10-000147.tar DONE ./mini-imagenet-10/shuffled/mi10-000147.tar -> ./mini-imagenet-10/images/mi10-000147.tar START ./mini-imagenet-10/shuffled/mi10-000148.tar -> ./mini-imagenet-10/images/mi10-000148.tar DONE ./mini-imagenet-10/shuffled/mi10-000148.tar -> ./mini-imagenet-10/images/mi10-000148.tar START ./mini-imagenet-10/shuffled/mi10-000149.tar -> ./mini-imagenet-10/images/mi10-000149.tar DONE ./mini-imagenet-10/shuffled/mi10-000149.tar -> ./mini-imagenet-10/images/mi10-000149.tar START ./mini-imagenet-10/shuffled/mi10-000150.tar -> ./mini-imagenet-10/images/mi10-000150.tar DONE ./mini-imagenet-10/shuffled/mi10-000150.tar -> ./mini-imagenet-10/images/mi10-000150.tar START ./mini-imagenet-10/shuffled/mi10-000151.tar -> ./mini-imagenet-10/images/mi10-000151.tar DONE ./mini-imagenet-10/shuffled/mi10-000151.tar -> ./mini-imagenet-10/images/mi10-000151.tar START ./mini-imagenet-10/shuffled/mi10-000152.tar -> ./mini-imagenet-10/images/mi10-000152.tar DONE ./mini-imagenet-10/shuffled/mi10-000152.tar -> ./mini-imagenet-10/images/mi10-000152.tar START ./mini-imagenet-10/shuffled/mi10-000153.tar -> ./mini-imagenet-10/images/mi10-000153.tar DONE ./mini-imagenet-10/shuffled/mi10-000153.tar -> ./mini-imagenet-10/images/mi10-000153.tar START ./mini-imagenet-10/shuffled/mi10-000154.tar -> ./mini-imagenet-10/images/mi10-000154.tar DONE ./mini-imagenet-10/shuffled/mi10-000154.tar -> ./mini-imagenet-10/images/mi10-000154.tar START ./mini-imagenet-10/shuffled/mi10-000155.tar -> ./mini-imagenet-10/images/mi10-000155.tar DONE ./mini-imagenet-10/shuffled/mi10-000155.tar -> ./mini-imagenet-10/images/mi10-000155.tar START ./mini-imagenet-10/shuffled/mi10-000156.tar -> ./mini-imagenet-10/images/mi10-000156.tar DONE ./mini-imagenet-10/shuffled/mi10-000156.tar -> ./mini-imagenet-10/images/mi10-000156.tar START ./mini-imagenet-10/shuffled/mi10-000157.tar -> ./mini-imagenet-10/images/mi10-000157.tar DONE ./mini-imagenet-10/shuffled/mi10-000157.tar -> ./mini-imagenet-10/images/mi10-000157.tar START ./mini-imagenet-10/shuffled/mi10-000158.tar -> ./mini-imagenet-10/images/mi10-000158.tar DONE ./mini-imagenet-10/shuffled/mi10-000158.tar -> ./mini-imagenet-10/images/mi10-000158.tar START ./mini-imagenet-10/shuffled/mi10-000159.tar -> ./mini-imagenet-10/images/mi10-000159.tar DONE ./mini-imagenet-10/shuffled/mi10-000159.tar -> ./mini-imagenet-10/images/mi10-000159.tar START ./mini-imagenet-10/shuffled/mi10-000160.tar -> ./mini-imagenet-10/images/mi10-000160.tar DONE ./mini-imagenet-10/shuffled/mi10-000160.tar -> ./mini-imagenet-10/images/mi10-000160.tar START ./mini-imagenet-10/shuffled/mi10-000161.tar -> ./mini-imagenet-10/images/mi10-000161.tar DONE ./mini-imagenet-10/shuffled/mi10-000161.tar -> ./mini-imagenet-10/images/mi10-000161.tar START ./mini-imagenet-10/shuffled/mi10-000162.tar -> ./mini-imagenet-10/images/mi10-000162.tar DONE ./mini-imagenet-10/shuffled/mi10-000162.tar -> ./mini-imagenet-10/images/mi10-000162.tar START ./mini-imagenet-10/shuffled/mi10-000163.tar -> ./mini-imagenet-10/images/mi10-000163.tar DONE ./mini-imagenet-10/shuffled/mi10-000163.tar -> ./mini-imagenet-10/images/mi10-000163.tar START ./mini-imagenet-10/shuffled/mi10-000164.tar -> ./mini-imagenet-10/images/mi10-000164.tar DONE ./mini-imagenet-10/shuffled/mi10-000164.tar -> ./mini-imagenet-10/images/mi10-000164.tar START ./mini-imagenet-10/shuffled/mi10-000165.tar -> ./mini-imagenet-10/images/mi10-000165.tar DONE ./mini-imagenet-10/shuffled/mi10-000165.tar -> ./mini-imagenet-10/images/mi10-000165.tar START ./mini-imagenet-10/shuffled/mi10-000166.tar -> ./mini-imagenet-10/images/mi10-000166.tar DONE ./mini-imagenet-10/shuffled/mi10-000166.tar -> ./mini-imagenet-10/images/mi10-000166.tar START ./mini-imagenet-10/shuffled/mi10-000167.tar -> ./mini-imagenet-10/images/mi10-000167.tar DONE ./mini-imagenet-10/shuffled/mi10-000167.tar -> ./mini-imagenet-10/images/mi10-000167.tar START ./mini-imagenet-10/shuffled/mi10-000168.tar -> ./mini-imagenet-10/images/mi10-000168.tar DONE ./mini-imagenet-10/shuffled/mi10-000168.tar -> ./mini-imagenet-10/images/mi10-000168.tar START ./mini-imagenet-10/shuffled/mi10-000169.tar -> ./mini-imagenet-10/images/mi10-000169.tar DONE ./mini-imagenet-10/shuffled/mi10-000169.tar -> ./mini-imagenet-10/images/mi10-000169.tar START ./mini-imagenet-10/shuffled/mi10-000170.tar -> ./mini-imagenet-10/images/mi10-000170.tar DONE ./mini-imagenet-10/shuffled/mi10-000170.tar -> ./mini-imagenet-10/images/mi10-000170.tar START ./mini-imagenet-10/shuffled/mi10-000171.tar -> ./mini-imagenet-10/images/mi10-000171.tar DONE ./mini-imagenet-10/shuffled/mi10-000171.tar -> ./mini-imagenet-10/images/mi10-000171.tar START ./mini-imagenet-10/shuffled/mi10-000172.tar -> ./mini-imagenet-10/images/mi10-000172.tar DONE ./mini-imagenet-10/shuffled/mi10-000172.tar -> ./mini-imagenet-10/images/mi10-000172.tar START ./mini-imagenet-10/shuffled/mi10-000173.tar -> ./mini-imagenet-10/images/mi10-000173.tar DONE ./mini-imagenet-10/shuffled/mi10-000173.tar -> ./mini-imagenet-10/images/mi10-000173.tar START ./mini-imagenet-10/shuffled/mi10-000174.tar -> ./mini-imagenet-10/images/mi10-000174.tar DONE ./mini-imagenet-10/shuffled/mi10-000174.tar -> ./mini-imagenet-10/images/mi10-000174.tar START ./mini-imagenet-10/shuffled/mi10-000175.tar -> ./mini-imagenet-10/images/mi10-000175.tar DONE ./mini-imagenet-10/shuffled/mi10-000175.tar -> ./mini-imagenet-10/images/mi10-000175.tar START ./mini-imagenet-10/shuffled/mi10-000176.tar -> ./mini-imagenet-10/images/mi10-000176.tar DONE ./mini-imagenet-10/shuffled/mi10-000176.tar -> ./mini-imagenet-10/images/mi10-000176.tar START ./mini-imagenet-10/shuffled/mi10-000177.tar -> ./mini-imagenet-10/images/mi10-000177.tar DONE ./mini-imagenet-10/shuffled/mi10-000177.tar -> ./mini-imagenet-10/images/mi10-000177.tar START ./mini-imagenet-10/shuffled/mi10-000178.tar -> ./mini-imagenet-10/images/mi10-000178.tar DONE ./mini-imagenet-10/shuffled/mi10-000178.tar -> ./mini-imagenet-10/images/mi10-000178.tar START ./mini-imagenet-10/shuffled/mi10-000179.tar -> ./mini-imagenet-10/images/mi10-000179.tar DONE ./mini-imagenet-10/shuffled/mi10-000179.tar -> ./mini-imagenet-10/images/mi10-000179.tar START ./mini-imagenet-10/shuffled/mi10-000180.tar -> ./mini-imagenet-10/images/mi10-000180.tar DONE ./mini-imagenet-10/shuffled/mi10-000180.tar -> ./mini-imagenet-10/images/mi10-000180.tar START ./mini-imagenet-10/shuffled/mi10-000181.tar -> ./mini-imagenet-10/images/mi10-000181.tar DONE ./mini-imagenet-10/shuffled/mi10-000181.tar -> ./mini-imagenet-10/images/mi10-000181.tar START ./mini-imagenet-10/shuffled/mi10-000182.tar -> ./mini-imagenet-10/images/mi10-000182.tar DONE ./mini-imagenet-10/shuffled/mi10-000182.tar -> ./mini-imagenet-10/images/mi10-000182.tar START ./mini-imagenet-10/shuffled/mi10-000183.tar -> ./mini-imagenet-10/images/mi10-000183.tar DONE ./mini-imagenet-10/shuffled/mi10-000183.tar -> ./mini-imagenet-10/images/mi10-000183.tar START ./mini-imagenet-10/shuffled/mi10-000184.tar -> ./mini-imagenet-10/images/mi10-000184.tar DONE ./mini-imagenet-10/shuffled/mi10-000184.tar -> ./mini-imagenet-10/images/mi10-000184.tar START ./mini-imagenet-10/shuffled/mi10-000185.tar -> ./mini-imagenet-10/images/mi10-000185.tar DONE ./mini-imagenet-10/shuffled/mi10-000185.tar -> ./mini-imagenet-10/images/mi10-000185.tar START ./mini-imagenet-10/shuffled/mi10-000186.tar -> ./mini-imagenet-10/images/mi10-000186.tar DONE ./mini-imagenet-10/shuffled/mi10-000186.tar -> ./mini-imagenet-10/images/mi10-000186.tar START ./mini-imagenet-10/shuffled/mi10-000187.tar -> ./mini-imagenet-10/images/mi10-000187.tar DONE ./mini-imagenet-10/shuffled/mi10-000187.tar -> ./mini-imagenet-10/images/mi10-000187.tar START ./mini-imagenet-10/shuffled/mi10-000188.tar -> ./mini-imagenet-10/images/mi10-000188.tar DONE ./mini-imagenet-10/shuffled/mi10-000188.tar -> ./mini-imagenet-10/images/mi10-000188.tar START ./mini-imagenet-10/shuffled/mi10-000189.tar -> ./mini-imagenet-10/images/mi10-000189.tar DONE ./mini-imagenet-10/shuffled/mi10-000189.tar -> ./mini-imagenet-10/images/mi10-000189.tar START ./mini-imagenet-10/shuffled/mi10-000190.tar -> ./mini-imagenet-10/images/mi10-000190.tar DONE ./mini-imagenet-10/shuffled/mi10-000190.tar -> ./mini-imagenet-10/images/mi10-000190.tar START ./mini-imagenet-10/shuffled/mi10-000191.tar -> ./mini-imagenet-10/images/mi10-000191.tar DONE ./mini-imagenet-10/shuffled/mi10-000191.tar -> ./mini-imagenet-10/images/mi10-000191.tar START ./mini-imagenet-10/shuffled/mi10-000192.tar -> ./mini-imagenet-10/images/mi10-000192.tar DONE ./mini-imagenet-10/shuffled/mi10-000192.tar -> ./mini-imagenet-10/images/mi10-000192.tar START ./mini-imagenet-10/shuffled/mi10-000193.tar -> ./mini-imagenet-10/images/mi10-000193.tar DONE ./mini-imagenet-10/shuffled/mi10-000193.tar -> ./mini-imagenet-10/images/mi10-000193.tar START ./mini-imagenet-10/shuffled/mi10-000194.tar -> ./mini-imagenet-10/images/mi10-000194.tar DONE ./mini-imagenet-10/shuffled/mi10-000194.tar -> ./mini-imagenet-10/images/mi10-000194.tar START ./mini-imagenet-10/shuffled/mi10-000195.tar -> ./mini-imagenet-10/images/mi10-000195.tar DONE ./mini-imagenet-10/shuffled/mi10-000195.tar -> ./mini-imagenet-10/images/mi10-000195.tar START ./mini-imagenet-10/shuffled/mi10-000196.tar -> ./mini-imagenet-10/images/mi10-000196.tar DONE ./mini-imagenet-10/shuffled/mi10-000196.tar -> ./mini-imagenet-10/images/mi10-000196.tar START ./mini-imagenet-10/shuffled/mi10-000197.tar -> ./mini-imagenet-10/images/mi10-000197.tar DONE ./mini-imagenet-10/shuffled/mi10-000197.tar -> ./mini-imagenet-10/images/mi10-000197.tar START ./mini-imagenet-10/shuffled/mi10-000198.tar -> ./mini-imagenet-10/images/mi10-000198.tar DONE ./mini-imagenet-10/shuffled/mi10-000198.tar -> ./mini-imagenet-10/images/mi10-000198.tar START ./mini-imagenet-10/shuffled/mi10-000199.tar -> ./mini-imagenet-10/images/mi10-000199.tar DONE ./mini-imagenet-10/shuffled/mi10-000199.tar -> ./mini-imagenet-10/images/mi10-000199.tar START ./mini-imagenet-10/shuffled/mi10-000200.tar -> ./mini-imagenet-10/images/mi10-000200.tar DONE ./mini-imagenet-10/shuffled/mi10-000200.tar -> ./mini-imagenet-10/images/mi10-000200.tar START ./mini-imagenet-10/shuffled/mi10-000201.tar -> ./mini-imagenet-10/images/mi10-000201.tar DONE ./mini-imagenet-10/shuffled/mi10-000201.tar -> ./mini-imagenet-10/images/mi10-000201.tar START ./mini-imagenet-10/shuffled/mi10-000202.tar -> ./mini-imagenet-10/images/mi10-000202.tar DONE ./mini-imagenet-10/shuffled/mi10-000202.tar -> ./mini-imagenet-10/images/mi10-000202.tar START ./mini-imagenet-10/shuffled/mi10-000203.tar -> ./mini-imagenet-10/images/mi10-000203.tar DONE ./mini-imagenet-10/shuffled/mi10-000203.tar -> ./mini-imagenet-10/images/mi10-000203.tar START ./mini-imagenet-10/shuffled/mi10-000204.tar -> ./mini-imagenet-10/images/mi10-000204.tar DONE ./mini-imagenet-10/shuffled/mi10-000204.tar -> ./mini-imagenet-10/images/mi10-000204.tar START ./mini-imagenet-10/shuffled/mi10-000205.tar -> ./mini-imagenet-10/images/mi10-000205.tar DONE ./mini-imagenet-10/shuffled/mi10-000205.tar -> ./mini-imagenet-10/images/mi10-000205.tar START ./mini-imagenet-10/shuffled/mi10-000206.tar -> ./mini-imagenet-10/images/mi10-000206.tar DONE ./mini-imagenet-10/shuffled/mi10-000206.tar -> ./mini-imagenet-10/images/mi10-000206.tar START ./mini-imagenet-10/shuffled/mi10-000207.tar -> ./mini-imagenet-10/images/mi10-000207.tar DONE ./mini-imagenet-10/shuffled/mi10-000207.tar -> ./mini-imagenet-10/images/mi10-000207.tar START ./mini-imagenet-10/shuffled/mi10-000208.tar -> ./mini-imagenet-10/images/mi10-000208.tar DONE ./mini-imagenet-10/shuffled/mi10-000208.tar -> ./mini-imagenet-10/images/mi10-000208.tar START ./mini-imagenet-10/shuffled/mi10-000209.tar -> ./mini-imagenet-10/images/mi10-000209.tar DONE ./mini-imagenet-10/shuffled/mi10-000209.tar -> ./mini-imagenet-10/images/mi10-000209.tar START ./mini-imagenet-10/shuffled/mi10-000210.tar -> ./mini-imagenet-10/images/mi10-000210.tar DONE ./mini-imagenet-10/shuffled/mi10-000210.tar -> ./mini-imagenet-10/images/mi10-000210.tar START ./mini-imagenet-10/shuffled/mi10-000211.tar -> ./mini-imagenet-10/images/mi10-000211.tar DONE ./mini-imagenet-10/shuffled/mi10-000211.tar -> ./mini-imagenet-10/images/mi10-000211.tar START ./mini-imagenet-10/shuffled/mi10-000212.tar -> ./mini-imagenet-10/images/mi10-000212.tar DONE ./mini-imagenet-10/shuffled/mi10-000212.tar -> ./mini-imagenet-10/images/mi10-000212.tar START ./mini-imagenet-10/shuffled/mi10-000213.tar -> ./mini-imagenet-10/images/mi10-000213.tar DONE ./mini-imagenet-10/shuffled/mi10-000213.tar -> ./mini-imagenet-10/images/mi10-000213.tar START ./mini-imagenet-10/shuffled/mi10-000214.tar -> ./mini-imagenet-10/images/mi10-000214.tar DONE ./mini-imagenet-10/shuffled/mi10-000214.tar -> ./mini-imagenet-10/images/mi10-000214.tar START ./mini-imagenet-10/shuffled/mi10-000215.tar -> ./mini-imagenet-10/images/mi10-000215.tar DONE ./mini-imagenet-10/shuffled/mi10-000215.tar -> ./mini-imagenet-10/images/mi10-000215.tar START ./mini-imagenet-10/shuffled/mi10-000216.tar -> ./mini-imagenet-10/images/mi10-000216.tar DONE ./mini-imagenet-10/shuffled/mi10-000216.tar -> ./mini-imagenet-10/images/mi10-000216.tar START ./mini-imagenet-10/shuffled/mi10-000217.tar -> ./mini-imagenet-10/images/mi10-000217.tar DONE ./mini-imagenet-10/shuffled/mi10-000217.tar -> ./mini-imagenet-10/images/mi10-000217.tar START ./mini-imagenet-10/shuffled/mi10-000218.tar -> ./mini-imagenet-10/images/mi10-000218.tar DONE ./mini-imagenet-10/shuffled/mi10-000218.tar -> ./mini-imagenet-10/images/mi10-000218.tar START ./mini-imagenet-10/shuffled/mi10-000219.tar -> ./mini-imagenet-10/images/mi10-000219.tar DONE ./mini-imagenet-10/shuffled/mi10-000219.tar -> ./mini-imagenet-10/images/mi10-000219.tar START ./mini-imagenet-10/shuffled/mi10-000220.tar -> ./mini-imagenet-10/images/mi10-000220.tar DONE ./mini-imagenet-10/shuffled/mi10-000220.tar -> ./mini-imagenet-10/images/mi10-000220.tar START ./mini-imagenet-10/shuffled/mi10-000221.tar -> ./mini-imagenet-10/images/mi10-000221.tar DONE ./mini-imagenet-10/shuffled/mi10-000221.tar -> ./mini-imagenet-10/images/mi10-000221.tar START ./mini-imagenet-10/shuffled/mi10-000222.tar -> ./mini-imagenet-10/images/mi10-000222.tar DONE ./mini-imagenet-10/shuffled/mi10-000222.tar -> ./mini-imagenet-10/images/mi10-000222.tar START ./mini-imagenet-10/shuffled/mi10-000223.tar -> ./mini-imagenet-10/images/mi10-000223.tar DONE ./mini-imagenet-10/shuffled/mi10-000223.tar -> ./mini-imagenet-10/images/mi10-000223.tar START ./mini-imagenet-10/shuffled/mi10-000224.tar -> ./mini-imagenet-10/images/mi10-000224.tar DONE ./mini-imagenet-10/shuffled/mi10-000224.tar -> ./mini-imagenet-10/images/mi10-000224.tar START ./mini-imagenet-10/shuffled/mi10-000225.tar -> ./mini-imagenet-10/images/mi10-000225.tar DONE ./mini-imagenet-10/shuffled/mi10-000225.tar -> ./mini-imagenet-10/images/mi10-000225.tar START ./mini-imagenet-10/shuffled/mi10-000226.tar -> ./mini-imagenet-10/images/mi10-000226.tar DONE ./mini-imagenet-10/shuffled/mi10-000226.tar -> ./mini-imagenet-10/images/mi10-000226.tar START ./mini-imagenet-10/shuffled/mi10-000227.tar -> ./mini-imagenet-10/images/mi10-000227.tar DONE ./mini-imagenet-10/shuffled/mi10-000227.tar -> ./mini-imagenet-10/images/mi10-000227.tar START ./mini-imagenet-10/shuffled/mi10-000228.tar -> ./mini-imagenet-10/images/mi10-000228.tar DONE ./mini-imagenet-10/shuffled/mi10-000228.tar -> ./mini-imagenet-10/images/mi10-000228.tar START ./mini-imagenet-10/shuffled/mi10-000229.tar -> ./mini-imagenet-10/images/mi10-000229.tar DONE ./mini-imagenet-10/shuffled/mi10-000229.tar -> ./mini-imagenet-10/images/mi10-000229.tar START ./mini-imagenet-10/shuffled/mi10-000230.tar -> ./mini-imagenet-10/images/mi10-000230.tar DONE ./mini-imagenet-10/shuffled/mi10-000230.tar -> ./mini-imagenet-10/images/mi10-000230.tar START ./mini-imagenet-10/shuffled/mi10-000231.tar -> ./mini-imagenet-10/images/mi10-000231.tar DONE ./mini-imagenet-10/shuffled/mi10-000231.tar -> ./mini-imagenet-10/images/mi10-000231.tar START ./mini-imagenet-10/shuffled/mi10-000232.tar -> ./mini-imagenet-10/images/mi10-000232.tar DONE ./mini-imagenet-10/shuffled/mi10-000232.tar -> ./mini-imagenet-10/images/mi10-000232.tar START ./mini-imagenet-10/shuffled/mi10-000233.tar -> ./mini-imagenet-10/images/mi10-000233.tar DONE ./mini-imagenet-10/shuffled/mi10-000233.tar -> ./mini-imagenet-10/images/mi10-000233.tar START ./mini-imagenet-10/shuffled/mi10-000234.tar -> ./mini-imagenet-10/images/mi10-000234.tar DONE ./mini-imagenet-10/shuffled/mi10-000234.tar -> ./mini-imagenet-10/images/mi10-000234.tar START ./mini-imagenet-10/shuffled/mi10-000235.tar -> ./mini-imagenet-10/images/mi10-000235.tar DONE ./mini-imagenet-10/shuffled/mi10-000235.tar -> ./mini-imagenet-10/images/mi10-000235.tar START ./mini-imagenet-10/shuffled/mi10-000236.tar -> ./mini-imagenet-10/images/mi10-000236.tar DONE ./mini-imagenet-10/shuffled/mi10-000236.tar -> ./mini-imagenet-10/images/mi10-000236.tar START ./mini-imagenet-10/shuffled/mi10-000237.tar -> ./mini-imagenet-10/images/mi10-000237.tar DONE ./mini-imagenet-10/shuffled/mi10-000237.tar -> ./mini-imagenet-10/images/mi10-000237.tar START ./mini-imagenet-10/shuffled/mi10-000238.tar -> ./mini-imagenet-10/images/mi10-000238.tar DONE ./mini-imagenet-10/shuffled/mi10-000238.tar -> ./mini-imagenet-10/images/mi10-000238.tar START ./mini-imagenet-10/shuffled/mi10-000239.tar -> ./mini-imagenet-10/images/mi10-000239.tar DONE ./mini-imagenet-10/shuffled/mi10-000239.tar -> ./mini-imagenet-10/images/mi10-000239.tar START ./mini-imagenet-10/shuffled/mi10-000240.tar -> ./mini-imagenet-10/images/mi10-000240.tar DONE ./mini-imagenet-10/shuffled/mi10-000240.tar -> ./mini-imagenet-10/images/mi10-000240.tar START ./mini-imagenet-10/shuffled/mi10-000241.tar -> ./mini-imagenet-10/images/mi10-000241.tar DONE ./mini-imagenet-10/shuffled/mi10-000241.tar -> ./mini-imagenet-10/images/mi10-000241.tar START ./mini-imagenet-10/shuffled/mi10-000242.tar -> ./mini-imagenet-10/images/mi10-000242.tar DONE ./mini-imagenet-10/shuffled/mi10-000242.tar -> ./mini-imagenet-10/images/mi10-000242.tar START ./mini-imagenet-10/shuffled/mi10-000243.tar -> ./mini-imagenet-10/images/mi10-000243.tar DONE ./mini-imagenet-10/shuffled/mi10-000243.tar -> ./mini-imagenet-10/images/mi10-000243.tar START ./mini-imagenet-10/shuffled/mi10-000244.tar -> ./mini-imagenet-10/images/mi10-000244.tar DONE ./mini-imagenet-10/shuffled/mi10-000244.tar -> ./mini-imagenet-10/images/mi10-000244.tar START ./mini-imagenet-10/shuffled/mi10-000245.tar -> ./mini-imagenet-10/images/mi10-000245.tar DONE ./mini-imagenet-10/shuffled/mi10-000245.tar -> ./mini-imagenet-10/images/mi10-000245.tar START ./mini-imagenet-10/shuffled/mi10-000246.tar -> ./mini-imagenet-10/images/mi10-000246.tar DONE ./mini-imagenet-10/shuffled/mi10-000246.tar -> ./mini-imagenet-10/images/mi10-000246.tar START ./mini-imagenet-10/shuffled/mi10-000247.tar -> ./mini-imagenet-10/images/mi10-000247.tar DONE ./mini-imagenet-10/shuffled/mi10-000247.tar -> ./mini-imagenet-10/images/mi10-000247.tar START ./mini-imagenet-10/shuffled/mi10-000248.tar -> ./mini-imagenet-10/images/mi10-000248.tar DONE ./mini-imagenet-10/shuffled/mi10-000248.tar -> ./mini-imagenet-10/images/mi10-000248.tar START ./mini-imagenet-10/shuffled/mi10-000249.tar -> ./mini-imagenet-10/images/mi10-000249.tar DONE ./mini-imagenet-10/shuffled/mi10-000249.tar -> ./mini-imagenet-10/images/mi10-000249.tar START ./mini-imagenet-10/shuffled/mi10-000250.tar -> ./mini-imagenet-10/images/mi10-000250.tar DONE ./mini-imagenet-10/shuffled/mi10-000250.tar -> ./mini-imagenet-10/images/mi10-000250.tar START ./mini-imagenet-10/shuffled/mi10-000251.tar -> ./mini-imagenet-10/images/mi10-000251.tar DONE ./mini-imagenet-10/shuffled/mi10-000251.tar -> ./mini-imagenet-10/images/mi10-000251.tar START ./mini-imagenet-10/shuffled/mi10-000252.tar -> ./mini-imagenet-10/images/mi10-000252.tar DONE ./mini-imagenet-10/shuffled/mi10-000252.tar -> ./mini-imagenet-10/images/mi10-000252.tar START ./mini-imagenet-10/shuffled/mi10-000253.tar -> ./mini-imagenet-10/images/mi10-000253.tar DONE ./mini-imagenet-10/shuffled/mi10-000253.tar -> ./mini-imagenet-10/images/mi10-000253.tar START ./mini-imagenet-10/shuffled/mi10-000254.tar -> ./mini-imagenet-10/images/mi10-000254.tar DONE ./mini-imagenet-10/shuffled/mi10-000254.tar -> ./mini-imagenet-10/images/mi10-000254.tar START ./mini-imagenet-10/shuffled/mi10-000255.tar -> ./mini-imagenet-10/images/mi10-000255.tar DONE ./mini-imagenet-10/shuffled/mi10-000255.tar -> ./mini-imagenet-10/images/mi10-000255.tar START ./mini-imagenet-10/shuffled/mi10-000256.tar -> ./mini-imagenet-10/images/mi10-000256.tar DONE ./mini-imagenet-10/shuffled/mi10-000256.tar -> ./mini-imagenet-10/images/mi10-000256.tar START ./mini-imagenet-10/shuffled/mi10-000257.tar -> ./mini-imagenet-10/images/mi10-000257.tar DONE ./mini-imagenet-10/shuffled/mi10-000257.tar -> ./mini-imagenet-10/images/mi10-000257.tar START ./mini-imagenet-10/shuffled/mi10-000258.tar -> ./mini-imagenet-10/images/mi10-000258.tar DONE ./mini-imagenet-10/shuffled/mi10-000258.tar -> ./mini-imagenet-10/images/mi10-000258.tar START ./mini-imagenet-10/shuffled/mi10-000259.tar -> ./mini-imagenet-10/images/mi10-000259.tar DONE ./mini-imagenet-10/shuffled/mi10-000259.tar -> ./mini-imagenet-10/images/mi10-000259.tar START ./mini-imagenet-10/shuffled/mi10-000260.tar -> ./mini-imagenet-10/images/mi10-000260.tar DONE ./mini-imagenet-10/shuffled/mi10-000260.tar -> ./mini-imagenet-10/images/mi10-000260.tar START ./mini-imagenet-10/shuffled/mi10-000261.tar -> ./mini-imagenet-10/images/mi10-000261.tar DONE ./mini-imagenet-10/shuffled/mi10-000261.tar -> ./mini-imagenet-10/images/mi10-000261.tar START ./mini-imagenet-10/shuffled/mi10-000262.tar -> ./mini-imagenet-10/images/mi10-000262.tar DONE ./mini-imagenet-10/shuffled/mi10-000262.tar -> ./mini-imagenet-10/images/mi10-000262.tar START ./mini-imagenet-10/shuffled/mi10-000263.tar -> ./mini-imagenet-10/images/mi10-000263.tar DONE ./mini-imagenet-10/shuffled/mi10-000263.tar -> ./mini-imagenet-10/images/mi10-000263.tar START ./mini-imagenet-10/shuffled/mi10-000264.tar -> ./mini-imagenet-10/images/mi10-000264.tar DONE ./mini-imagenet-10/shuffled/mi10-000264.tar -> ./mini-imagenet-10/images/mi10-000264.tar START ./mini-imagenet-10/shuffled/mi10-000265.tar -> ./mini-imagenet-10/images/mi10-000265.tar DONE ./mini-imagenet-10/shuffled/mi10-000265.tar -> ./mini-imagenet-10/images/mi10-000265.tar START ./mini-imagenet-10/shuffled/mi10-000266.tar -> ./mini-imagenet-10/images/mi10-000266.tar DONE ./mini-imagenet-10/shuffled/mi10-000266.tar -> ./mini-imagenet-10/images/mi10-000266.tar START ./mini-imagenet-10/shuffled/mi10-000267.tar -> ./mini-imagenet-10/images/mi10-000267.tar DONE ./mini-imagenet-10/shuffled/mi10-000267.tar -> ./mini-imagenet-10/images/mi10-000267.tar START ./mini-imagenet-10/shuffled/mi10-000268.tar -> ./mini-imagenet-10/images/mi10-000268.tar DONE ./mini-imagenet-10/shuffled/mi10-000268.tar -> ./mini-imagenet-10/images/mi10-000268.tar START ./mini-imagenet-10/shuffled/mi10-000269.tar -> ./mini-imagenet-10/images/mi10-000269.tar DONE ./mini-imagenet-10/shuffled/mi10-000269.tar -> ./mini-imagenet-10/images/mi10-000269.tar START ./mini-imagenet-10/shuffled/mi10-000270.tar -> ./mini-imagenet-10/images/mi10-000270.tar DONE ./mini-imagenet-10/shuffled/mi10-000270.tar -> ./mini-imagenet-10/images/mi10-000270.tar START ./mini-imagenet-10/shuffled/mi10-000271.tar -> ./mini-imagenet-10/images/mi10-000271.tar DONE ./mini-imagenet-10/shuffled/mi10-000271.tar -> ./mini-imagenet-10/images/mi10-000271.tar START ./mini-imagenet-10/shuffled/mi10-000272.tar -> ./mini-imagenet-10/images/mi10-000272.tar DONE ./mini-imagenet-10/shuffled/mi10-000272.tar -> ./mini-imagenet-10/images/mi10-000272.tar START ./mini-imagenet-10/shuffled/mi10-000273.tar -> ./mini-imagenet-10/images/mi10-000273.tar DONE ./mini-imagenet-10/shuffled/mi10-000273.tar -> ./mini-imagenet-10/images/mi10-000273.tar START ./mini-imagenet-10/shuffled/mi10-000274.tar -> ./mini-imagenet-10/images/mi10-000274.tar DONE ./mini-imagenet-10/shuffled/mi10-000274.tar -> ./mini-imagenet-10/images/mi10-000274.tar START ./mini-imagenet-10/shuffled/mi10-000275.tar -> ./mini-imagenet-10/images/mi10-000275.tar DONE ./mini-imagenet-10/shuffled/mi10-000275.tar -> ./mini-imagenet-10/images/mi10-000275.tar START ./mini-imagenet-10/shuffled/mi10-000276.tar -> ./mini-imagenet-10/images/mi10-000276.tar DONE ./mini-imagenet-10/shuffled/mi10-000276.tar -> ./mini-imagenet-10/images/mi10-000276.tar START ./mini-imagenet-10/shuffled/mi10-000277.tar -> ./mini-imagenet-10/images/mi10-000277.tar DONE ./mini-imagenet-10/shuffled/mi10-000277.tar -> ./mini-imagenet-10/images/mi10-000277.tar START ./mini-imagenet-10/shuffled/mi10-000278.tar -> ./mini-imagenet-10/images/mi10-000278.tar DONE ./mini-imagenet-10/shuffled/mi10-000278.tar -> ./mini-imagenet-10/images/mi10-000278.tar START ./mini-imagenet-10/shuffled/mi10-000279.tar -> ./mini-imagenet-10/images/mi10-000279.tar DONE ./mini-imagenet-10/shuffled/mi10-000279.tar -> ./mini-imagenet-10/images/mi10-000279.tar START ./mini-imagenet-10/shuffled/mi10-000280.tar -> ./mini-imagenet-10/images/mi10-000280.tar DONE ./mini-imagenet-10/shuffled/mi10-000280.tar -> ./mini-imagenet-10/images/mi10-000280.tar START ./mini-imagenet-10/shuffled/mi10-000281.tar -> ./mini-imagenet-10/images/mi10-000281.tar DONE ./mini-imagenet-10/shuffled/mi10-000281.tar -> ./mini-imagenet-10/images/mi10-000281.tar START ./mini-imagenet-10/shuffled/mi10-000282.tar -> ./mini-imagenet-10/images/mi10-000282.tar DONE ./mini-imagenet-10/shuffled/mi10-000282.tar -> ./mini-imagenet-10/images/mi10-000282.tar START ./mini-imagenet-10/shuffled/mi10-000283.tar -> ./mini-imagenet-10/images/mi10-000283.tar DONE ./mini-imagenet-10/shuffled/mi10-000283.tar -> ./mini-imagenet-10/images/mi10-000283.tar START ./mini-imagenet-10/shuffled/mi10-000284.tar -> ./mini-imagenet-10/images/mi10-000284.tar DONE ./mini-imagenet-10/shuffled/mi10-000284.tar -> ./mini-imagenet-10/images/mi10-000284.tar START ./mini-imagenet-10/shuffled/mi10-000285.tar -> ./mini-imagenet-10/images/mi10-000285.tar DONE ./mini-imagenet-10/shuffled/mi10-000285.tar -> ./mini-imagenet-10/images/mi10-000285.tar START ./mini-imagenet-10/shuffled/mi10-000286.tar -> ./mini-imagenet-10/images/mi10-000286.tar DONE ./mini-imagenet-10/shuffled/mi10-000286.tar -> ./mini-imagenet-10/images/mi10-000286.tar START ./mini-imagenet-10/shuffled/mi10-000287.tar -> ./mini-imagenet-10/images/mi10-000287.tar DONE ./mini-imagenet-10/shuffled/mi10-000287.tar -> ./mini-imagenet-10/images/mi10-000287.tar START ./mini-imagenet-10/shuffled/mi10-000288.tar -> ./mini-imagenet-10/images/mi10-000288.tar DONE ./mini-imagenet-10/shuffled/mi10-000288.tar -> ./mini-imagenet-10/images/mi10-000288.tar START ./mini-imagenet-10/shuffled/mi10-000289.tar -> ./mini-imagenet-10/images/mi10-000289.tar DONE ./mini-imagenet-10/shuffled/mi10-000289.tar -> ./mini-imagenet-10/images/mi10-000289.tar START ./mini-imagenet-10/shuffled/mi10-000290.tar -> ./mini-imagenet-10/images/mi10-000290.tar DONE ./mini-imagenet-10/shuffled/mi10-000290.tar -> ./mini-imagenet-10/images/mi10-000290.tar START ./mini-imagenet-10/shuffled/mi10-000291.tar -> ./mini-imagenet-10/images/mi10-000291.tar DONE ./mini-imagenet-10/shuffled/mi10-000291.tar -> ./mini-imagenet-10/images/mi10-000291.tar START ./mini-imagenet-10/shuffled/mi10-000292.tar -> ./mini-imagenet-10/images/mi10-000292.tar DONE ./mini-imagenet-10/shuffled/mi10-000292.tar -> ./mini-imagenet-10/images/mi10-000292.tar START ./mini-imagenet-10/shuffled/mi10-000293.tar -> ./mini-imagenet-10/images/mi10-000293.tar DONE ./mini-imagenet-10/shuffled/mi10-000293.tar -> ./mini-imagenet-10/images/mi10-000293.tar START ./mini-imagenet-10/shuffled/mi10-000294.tar -> ./mini-imagenet-10/images/mi10-000294.tar DONE ./mini-imagenet-10/shuffled/mi10-000294.tar -> ./mini-imagenet-10/images/mi10-000294.tar START ./mini-imagenet-10/shuffled/mi10-000295.tar -> ./mini-imagenet-10/images/mi10-000295.tar DONE ./mini-imagenet-10/shuffled/mi10-000295.tar -> ./mini-imagenet-10/images/mi10-000295.tar START ./mini-imagenet-10/shuffled/mi10-000296.tar -> ./mini-imagenet-10/images/mi10-000296.tar DONE ./mini-imagenet-10/shuffled/mi10-000296.tar -> ./mini-imagenet-10/images/mi10-000296.tar START ./mini-imagenet-10/shuffled/mi10-000297.tar -> ./mini-imagenet-10/images/mi10-000297.tar DONE ./mini-imagenet-10/shuffled/mi10-000297.tar -> ./mini-imagenet-10/images/mi10-000297.tar START ./mini-imagenet-10/shuffled/mi10-000298.tar -> ./mini-imagenet-10/images/mi10-000298.tar DONE ./mini-imagenet-10/shuffled/mi10-000298.tar -> ./mini-imagenet-10/images/mi10-000298.tar START ./mini-imagenet-10/shuffled/mi10-000299.tar -> ./mini-imagenet-10/images/mi10-000299.tar DONE ./mini-imagenet-10/shuffled/mi10-000299.tar -> ./mini-imagenet-10/images/mi10-000299.tar START ./mini-imagenet-10/shuffled/mi10-000300.tar -> ./mini-imagenet-10/images/mi10-000300.tar DONE ./mini-imagenet-10/shuffled/mi10-000300.tar -> ./mini-imagenet-10/images/mi10-000300.tar START ./mini-imagenet-10/shuffled/mi10-000301.tar -> ./mini-imagenet-10/images/mi10-000301.tar DONE ./mini-imagenet-10/shuffled/mi10-000301.tar -> ./mini-imagenet-10/images/mi10-000301.tar START ./mini-imagenet-10/shuffled/mi10-000302.tar -> ./mini-imagenet-10/images/mi10-000302.tar DONE ./mini-imagenet-10/shuffled/mi10-000302.tar -> ./mini-imagenet-10/images/mi10-000302.tar START ./mini-imagenet-10/shuffled/mi10-000303.tar -> ./mini-imagenet-10/images/mi10-000303.tar DONE ./mini-imagenet-10/shuffled/mi10-000303.tar -> ./mini-imagenet-10/images/mi10-000303.tar START ./mini-imagenet-10/shuffled/mi10-000304.tar -> ./mini-imagenet-10/images/mi10-000304.tar DONE ./mini-imagenet-10/shuffled/mi10-000304.tar -> ./mini-imagenet-10/images/mi10-000304.tar START ./mini-imagenet-10/shuffled/mi10-000305.tar -> ./mini-imagenet-10/images/mi10-000305.tar DONE ./mini-imagenet-10/shuffled/mi10-000305.tar -> ./mini-imagenet-10/images/mi10-000305.tar START ./mini-imagenet-10/shuffled/mi10-000306.tar -> ./mini-imagenet-10/images/mi10-000306.tar DONE ./mini-imagenet-10/shuffled/mi10-000306.tar -> ./mini-imagenet-10/images/mi10-000306.tar START ./mini-imagenet-10/shuffled/mi10-000307.tar -> ./mini-imagenet-10/images/mi10-000307.tar DONE ./mini-imagenet-10/shuffled/mi10-000307.tar -> ./mini-imagenet-10/images/mi10-000307.tar","title":"Parallelization with Ray"},{"location":"mi-prompts/","text":"Mini Imagenet Generation This is one of a pair of notebooks used for generating an ImageNet-like dataset of training data using stable diffusion models. The difficulty of such artificial datasets can be easily tuned, and they are useful for debugging and testing deep learning applications. The first notebook uses Mistral-7B for taking class labels and generating descriptive prompts for image generation. The prompts are written out as shards to disk and shuffled. The process is parallelized using Ray. The second notebook uses Stable Diffustion to take descriptive prompts/image captions and renders them as image. This is a straightfowrard shard-to-shard transformation. Note that we are using explicit parallelization over shard files in the initial generation and the image generation, while we are using ray.data for the actual shuffling. That is because using explicit parallelization over shards makes it easier to restart jobs that have failed halfway through for some reason. import itertools, random, uuid from pprint import pprint import os import torch import webdataset as wds from transformers import AutoModelForCausalLM, AutoTokenizer from webdataset import filters import textwrap import time from transformers import AutoTokenizer, AutoModelForCausalLM import torch from typing import List def take(n, iterable): \"\"\"Return first n items of the iterable as a list\"\"\" return list(itertools.islice(iterable, n)) def get_gpu_memories(): memory = [] if torch.cuda.is_available(): for i in range(torch.cuda.device_count()): memory.append(torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(i)) return memory def get_num_gpus(): cluster_resources = ray.cluster_resources() return cluster_resources[\"GPU\"] def ray_get(future, timeout=0.1): ready, not_ready = ray.wait([future], timeout=timeout) if not_ready: raise TimeoutError() return ray.get(future) def is_ready(actor, timeout=0.1): ready, not_ready = ray.wait([actor], timeout=timeout) if not_ready: return False return True def select_or_delete(actors, predicate): result = [] for actor in actors: if predicate(actor): result.append(actor) else: del actor return result # parameters # number of classes, must be 10, 100, or 1000 nclasses = 10 # number of images per shard nimages = 100 # number of prompts generated at once per class ngenerated = 20 # number of training shards nshards = 1281 # number of validation shards nvalshards = 50 # output directory odir = f\"./mini-imagenet-{nclasses}\" # output file prefix oprefix = f\"mi{nclasses}\" # number of actors to use, -1 for =number of GPUs nactors = -1 # check that each actor has sufficient memory check_sufficient = True # seconds to wait for actors to start up actor_startup_wait = 10 !echo \"odir=$odir\" !mkdir -p $odir if nclasses == 10: imagenet_classes = \"dog cat car plane bird fish frog horse sheep truck\".split() elif nclasses == 100: imagenet_classes = sorted(list(set(\"\"\" 3d_printer aircraft_carrier airplane apple backpack banana baseball_bat baseball_glove bat bear bed bench bird book bottle bowl broccoli cake camel car carrot cat cell_phone chair clock cloud couch cup dining_table dog donut elephant fire fish fork fox frisbee frog giraffe hair_drier handbag horse hot_dog hydrant kangaroo keyboard kite knife lamp laptop lion meteor microwave monitor monkey mouse mushroom octopus orange oven palm_tree panda parking_meter pear pizza plane potted_plant refrigerator remote rocket sandwich scissors sheep sink skateboard skis snowboard spoon sports_ball stop_sign street_sign suitcase surfboard sweet_pepper table teddy_bear telephone tennis_racket tie tiger toaster toilet toothbrush tree truck tv umbrella vase wine_glass zebra \"\"\".split()))) elif nclasses == 1000: imagenet_classes = open(\"imagenet1000.txt\").read().split() else: raise ValueError(f\"invalid number of classes: {nclasses}, must be 10, 100, or 1000\") assert len(imagenet_classes) == nclasses Generation Classes We encapsulate the model and the generation in a low level and high level class. We can then instantiate those classes once per GPU and call them to generate the shards. class TextGenerationModel: def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.1\", temperature: float = 2.0, top_p: float = 0.9, top_k: int = 10, max_length: int = 96, num_return_sequences: int = 10): \"\"\" Initialize the text generation model. Args: model_name: The name of the pretrained model. temperature: The temperature for the generation process. top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. top_k: The number of highest probability vocabulary tokens to keep for top-k filtering. max_length: The maximum length of the sequence to be generated. num_return_sequences: The number of independently computed returned sequences for each element in the batch. \"\"\" # Load the tokenizer and model self.tokenizer = AutoTokenizer.from_pretrained( model_name, padding_side=\"left\", ) self.tokenizer.pad_token = self.tokenizer.eos_token self.model = AutoModelForCausalLM.from_pretrained(model_name) # Ensure the model is on GPU self.model.to(\"cuda\").half() # Set generation parameters self.temperature = temperature self.top_p = top_p self.top_k = top_k self.max_length = max_length self.num_return_sequences = num_return_sequences def generate_responses(self, texts: List[str]) -> List[str]: \"\"\" Generate responses for the given texts. Args: texts: A list of texts to generate responses for. Returns: A list of generated responses. \"\"\" # Prepare the inputs inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length).to(\"cuda\") # Generate responses with torch.no_grad(): outputs = self.model.generate( input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, do_sample=True, temperature=self.temperature, top_p=self.top_p, top_k=self.top_k, max_length=self.max_length, num_return_sequences=self.num_return_sequences, ) # Decode the responses responses = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs] return responses from typing import Dict, List, Iterator class CaptionGenerator: def __init__(self): self.template = \"[INST] Generate a random, detailed visual caption/description of a photo showing: {object}. [/INST]\" def load_model(self): self.model = TextGenerationModel() def gpu_is_sufficient(self): gpu_memories = get_gpu_memories() assert len(gpu_memories) == 1, \"more than one GPU allocated to actor???\" return gpu_memories[0] / 1e9 > 32.0 def process_batch(self, batch: List[Dict], trim: bool = True) -> List[Dict]: \"\"\"Process a batch of samples, generating responses for each.\"\"\" n = len(batch) texts = [batch[i][\"text\"] for i in range(n)] responses = self.model.generate_responses(texts) if trim: responses = [response.split(\"[/INST]\")[-1].strip() for response in responses] responses = [responses[i : i + self.model.num_return_sequences] for i in range(0, len(responses), self.model.num_return_sequences)] for i in range(n): batch[i][\"responses\"] = responses[i] return batch def process_list_by_batches(self, samples: List[Dict], batch_size: int = 1) -> Iterator[Dict]: \"\"\"Process a list of samples by batches.\"\"\" samples_iter = iter(samples) while True: batch = take(batch_size, samples_iter) if not batch: break responses = self.process_batch(batch) yield from responses def make_samples(self, n: int) -> Iterator[Dict]: \"\"\"Generate a list of samples.\"\"\" for i in range(n): cls = random.randrange(len(imagenet_classes)) object = imagenet_classes[cls] yield dict(cls=cls, object=object, text=self.template.format(object=object)) def make_captions(self, samples: List[Dict]) -> Iterator[Dict]: \"\"\"Generate captions for a list of samples.\"\"\" for sample in self.process_list_by_batches(samples): for response in sample[\"responses\"]: yield dict( cls=sample[\"cls\"], object=sample[\"object\"], text=sample[\"text\"], response=response, ) def make_shard(self, output: str, n: int, k: int = 5): \"\"\" Generate a shard of samples with generated captions. Args: output: The output file to write the shard to. n: The number of samples to generate in the shard. k: The number of return sequences for each sample. \"\"\" if os.path.exists(output): return self.model.num_return_sequences = k writer = wds.TarWriter(output+\".temp\") captions = self.make_captions(self.make_samples(n // k + k)) for caption in itertools.islice(captions, n): sample = dict( __key__=uuid.uuid4().hex, json=caption, ) writer.write(sample) os.rename(output+\".temp\", output) Parallelization with Ra For parallel generation, we use a Ray cluster. This will also do the right thing with a single machine/single GPU setup. It automatically scales up. import ray if not ray.is_initialized(): ray.init() @ray.remote(num_gpus=1) class RayCaptionGenerator(CaptionGenerator): def __init__(self): super().__init__() # Start up and create the actor pool. # This tries to adapt to the number of GPUs available. # It also checks that each actor has sufficient memory. # If not, set up your cluster differently by excluding GPUs that are too small. # (Ray's facilities for heterogenous clusters are somewhat limited) ngpus = get_num_gpus() if nactors == -1 else nactors print(f\"using {ngpus} actors\") actors = [RayCaptionGenerator.remote() for i in range(int(ngpus))] print(\"loading the models\") for actor in actors: assert ray.get(actor.gpu_is_sufficient.remote()), \"GPU memory insufficient\" ray.get(actor.load_model.remote()) print(\"creating the pool\") pool = ray.util.ActorPool(actors) # It would be nice if there were a .map_with_actors method in pool, # but there isn't, so we use this workaround. def apply_actor(actor, dest): return actor.make_shard.remote(dest, nimages, ngenerated) !mkdir -p $odir/prompts # Perform the actual shard generation. dests = [f\"{odir}/prompts/prompts-{i:06d}.tar\" for i in range(nshards + nvalshards)] result = list(pool.map(apply_actor, dests)) del actors del pool Shuffle For shuffling the dataset, we use the ray.data read_webdataset and write_webdataset functions. import ray from ray.data import read_webdataset import glob !mkdir -p $odir/shuffled !rm -f $odir/shuffled/* shards = glob.glob(f\"{odir}/prompts/prompts-*.tar\") dataset = read_webdataset(shards) shuffled_dataset = dataset.random_shuffle() shuffled_dataset.repartition(len(shards)).write_webdataset(f\"{odir}/shuffled/\") # The output of write_webdataset is a directory of shards, but not following # the usual naming conventions. We rename the shards to follow typical # webdataset conventions. import glob import os shuffled = sorted(glob.glob(f\"{odir}/shuffled/*.tar\")) for i in range(nshards): os.rename(shuffled[i], f\"{odir}/shuffled/{oprefix}-{i:06d}.tar\") for i in range(nvalshards): os.rename(shuffled[nshards+i], f\"{odir}/shuffled/{oprefix}-val-{i:06d}.tar\")","title":"Mini Imagenet Generation"},{"location":"mi-prompts/#mini-imagenet-generation","text":"This is one of a pair of notebooks used for generating an ImageNet-like dataset of training data using stable diffusion models. The difficulty of such artificial datasets can be easily tuned, and they are useful for debugging and testing deep learning applications. The first notebook uses Mistral-7B for taking class labels and generating descriptive prompts for image generation. The prompts are written out as shards to disk and shuffled. The process is parallelized using Ray. The second notebook uses Stable Diffustion to take descriptive prompts/image captions and renders them as image. This is a straightfowrard shard-to-shard transformation. Note that we are using explicit parallelization over shard files in the initial generation and the image generation, while we are using ray.data for the actual shuffling. That is because using explicit parallelization over shards makes it easier to restart jobs that have failed halfway through for some reason. import itertools, random, uuid from pprint import pprint import os import torch import webdataset as wds from transformers import AutoModelForCausalLM, AutoTokenizer from webdataset import filters import textwrap import time from transformers import AutoTokenizer, AutoModelForCausalLM import torch from typing import List def take(n, iterable): \"\"\"Return first n items of the iterable as a list\"\"\" return list(itertools.islice(iterable, n)) def get_gpu_memories(): memory = [] if torch.cuda.is_available(): for i in range(torch.cuda.device_count()): memory.append(torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(i)) return memory def get_num_gpus(): cluster_resources = ray.cluster_resources() return cluster_resources[\"GPU\"] def ray_get(future, timeout=0.1): ready, not_ready = ray.wait([future], timeout=timeout) if not_ready: raise TimeoutError() return ray.get(future) def is_ready(actor, timeout=0.1): ready, not_ready = ray.wait([actor], timeout=timeout) if not_ready: return False return True def select_or_delete(actors, predicate): result = [] for actor in actors: if predicate(actor): result.append(actor) else: del actor return result # parameters # number of classes, must be 10, 100, or 1000 nclasses = 10 # number of images per shard nimages = 100 # number of prompts generated at once per class ngenerated = 20 # number of training shards nshards = 1281 # number of validation shards nvalshards = 50 # output directory odir = f\"./mini-imagenet-{nclasses}\" # output file prefix oprefix = f\"mi{nclasses}\" # number of actors to use, -1 for =number of GPUs nactors = -1 # check that each actor has sufficient memory check_sufficient = True # seconds to wait for actors to start up actor_startup_wait = 10 !echo \"odir=$odir\" !mkdir -p $odir if nclasses == 10: imagenet_classes = \"dog cat car plane bird fish frog horse sheep truck\".split() elif nclasses == 100: imagenet_classes = sorted(list(set(\"\"\" 3d_printer aircraft_carrier airplane apple backpack banana baseball_bat baseball_glove bat bear bed bench bird book bottle bowl broccoli cake camel car carrot cat cell_phone chair clock cloud couch cup dining_table dog donut elephant fire fish fork fox frisbee frog giraffe hair_drier handbag horse hot_dog hydrant kangaroo keyboard kite knife lamp laptop lion meteor microwave monitor monkey mouse mushroom octopus orange oven palm_tree panda parking_meter pear pizza plane potted_plant refrigerator remote rocket sandwich scissors sheep sink skateboard skis snowboard spoon sports_ball stop_sign street_sign suitcase surfboard sweet_pepper table teddy_bear telephone tennis_racket tie tiger toaster toilet toothbrush tree truck tv umbrella vase wine_glass zebra \"\"\".split()))) elif nclasses == 1000: imagenet_classes = open(\"imagenet1000.txt\").read().split() else: raise ValueError(f\"invalid number of classes: {nclasses}, must be 10, 100, or 1000\") assert len(imagenet_classes) == nclasses","title":"Mini Imagenet Generation"},{"location":"mi-prompts/#generation-classes","text":"We encapsulate the model and the generation in a low level and high level class. We can then instantiate those classes once per GPU and call them to generate the shards. class TextGenerationModel: def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.1\", temperature: float = 2.0, top_p: float = 0.9, top_k: int = 10, max_length: int = 96, num_return_sequences: int = 10): \"\"\" Initialize the text generation model. Args: model_name: The name of the pretrained model. temperature: The temperature for the generation process. top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. top_k: The number of highest probability vocabulary tokens to keep for top-k filtering. max_length: The maximum length of the sequence to be generated. num_return_sequences: The number of independently computed returned sequences for each element in the batch. \"\"\" # Load the tokenizer and model self.tokenizer = AutoTokenizer.from_pretrained( model_name, padding_side=\"left\", ) self.tokenizer.pad_token = self.tokenizer.eos_token self.model = AutoModelForCausalLM.from_pretrained(model_name) # Ensure the model is on GPU self.model.to(\"cuda\").half() # Set generation parameters self.temperature = temperature self.top_p = top_p self.top_k = top_k self.max_length = max_length self.num_return_sequences = num_return_sequences def generate_responses(self, texts: List[str]) -> List[str]: \"\"\" Generate responses for the given texts. Args: texts: A list of texts to generate responses for. Returns: A list of generated responses. \"\"\" # Prepare the inputs inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length).to(\"cuda\") # Generate responses with torch.no_grad(): outputs = self.model.generate( input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, do_sample=True, temperature=self.temperature, top_p=self.top_p, top_k=self.top_k, max_length=self.max_length, num_return_sequences=self.num_return_sequences, ) # Decode the responses responses = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs] return responses from typing import Dict, List, Iterator class CaptionGenerator: def __init__(self): self.template = \"[INST] Generate a random, detailed visual caption/description of a photo showing: {object}. [/INST]\" def load_model(self): self.model = TextGenerationModel() def gpu_is_sufficient(self): gpu_memories = get_gpu_memories() assert len(gpu_memories) == 1, \"more than one GPU allocated to actor???\" return gpu_memories[0] / 1e9 > 32.0 def process_batch(self, batch: List[Dict], trim: bool = True) -> List[Dict]: \"\"\"Process a batch of samples, generating responses for each.\"\"\" n = len(batch) texts = [batch[i][\"text\"] for i in range(n)] responses = self.model.generate_responses(texts) if trim: responses = [response.split(\"[/INST]\")[-1].strip() for response in responses] responses = [responses[i : i + self.model.num_return_sequences] for i in range(0, len(responses), self.model.num_return_sequences)] for i in range(n): batch[i][\"responses\"] = responses[i] return batch def process_list_by_batches(self, samples: List[Dict], batch_size: int = 1) -> Iterator[Dict]: \"\"\"Process a list of samples by batches.\"\"\" samples_iter = iter(samples) while True: batch = take(batch_size, samples_iter) if not batch: break responses = self.process_batch(batch) yield from responses def make_samples(self, n: int) -> Iterator[Dict]: \"\"\"Generate a list of samples.\"\"\" for i in range(n): cls = random.randrange(len(imagenet_classes)) object = imagenet_classes[cls] yield dict(cls=cls, object=object, text=self.template.format(object=object)) def make_captions(self, samples: List[Dict]) -> Iterator[Dict]: \"\"\"Generate captions for a list of samples.\"\"\" for sample in self.process_list_by_batches(samples): for response in sample[\"responses\"]: yield dict( cls=sample[\"cls\"], object=sample[\"object\"], text=sample[\"text\"], response=response, ) def make_shard(self, output: str, n: int, k: int = 5): \"\"\" Generate a shard of samples with generated captions. Args: output: The output file to write the shard to. n: The number of samples to generate in the shard. k: The number of return sequences for each sample. \"\"\" if os.path.exists(output): return self.model.num_return_sequences = k writer = wds.TarWriter(output+\".temp\") captions = self.make_captions(self.make_samples(n // k + k)) for caption in itertools.islice(captions, n): sample = dict( __key__=uuid.uuid4().hex, json=caption, ) writer.write(sample) os.rename(output+\".temp\", output)","title":"Generation Classes"},{"location":"mi-prompts/#parallelization-with-ra","text":"For parallel generation, we use a Ray cluster. This will also do the right thing with a single machine/single GPU setup. It automatically scales up. import ray if not ray.is_initialized(): ray.init() @ray.remote(num_gpus=1) class RayCaptionGenerator(CaptionGenerator): def __init__(self): super().__init__() # Start up and create the actor pool. # This tries to adapt to the number of GPUs available. # It also checks that each actor has sufficient memory. # If not, set up your cluster differently by excluding GPUs that are too small. # (Ray's facilities for heterogenous clusters are somewhat limited) ngpus = get_num_gpus() if nactors == -1 else nactors print(f\"using {ngpus} actors\") actors = [RayCaptionGenerator.remote() for i in range(int(ngpus))] print(\"loading the models\") for actor in actors: assert ray.get(actor.gpu_is_sufficient.remote()), \"GPU memory insufficient\" ray.get(actor.load_model.remote()) print(\"creating the pool\") pool = ray.util.ActorPool(actors) # It would be nice if there were a .map_with_actors method in pool, # but there isn't, so we use this workaround. def apply_actor(actor, dest): return actor.make_shard.remote(dest, nimages, ngenerated) !mkdir -p $odir/prompts # Perform the actual shard generation. dests = [f\"{odir}/prompts/prompts-{i:06d}.tar\" for i in range(nshards + nvalshards)] result = list(pool.map(apply_actor, dests)) del actors del pool","title":"Parallelization with Ra"},{"location":"mi-prompts/#shuffle","text":"For shuffling the dataset, we use the ray.data read_webdataset and write_webdataset functions. import ray from ray.data import read_webdataset import glob !mkdir -p $odir/shuffled !rm -f $odir/shuffled/* shards = glob.glob(f\"{odir}/prompts/prompts-*.tar\") dataset = read_webdataset(shards) shuffled_dataset = dataset.random_shuffle() shuffled_dataset.repartition(len(shards)).write_webdataset(f\"{odir}/shuffled/\") # The output of write_webdataset is a directory of shards, but not following # the usual naming conventions. We rename the shards to follow typical # webdataset conventions. import glob import os shuffled = sorted(glob.glob(f\"{odir}/shuffled/*.tar\")) for i in range(nshards): os.rename(shuffled[i], f\"{odir}/shuffled/{oprefix}-{i:06d}.tar\") for i in range(nvalshards): os.rename(shuffled[nshards+i], f\"{odir}/shuffled/{oprefix}-val-{i:06d}.tar\")","title":"Shuffle"},{"location":"tesseract-wds/","text":"# imports and helper functions import os import sys import webdataset as wds import braceexpand import tempfile import glob from itertools import islice import random def summarize(sample): for k, v in sample.items(): print(k, repr(v)[:100]) def read_binary(fname): with open(fname, \"rb\") as stream: return stream.read() Parallel Processing of Shards: Large Scale OCR This notebook illustrates how to take a large collection of shards consisting of PDFs and process them using pdftoppm and tessearact into a new dataset consisting of page images and corresponding OCR output. The general approach is to process each shard sequentially and to process multiple shards in parallel. The basic structure of such a job looks like: with WebDataset(srcname) as src: with TarWriter(dstname) as dst: for sample in src: ... do something with sample ... dst.write(sample) upload(dstname) The Arxiv Dataset of PDFs # The dataset is tar files containing PDFs, each using the Arxiv naming convention. !gsutil cat gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar | tar tf - | sed 5q 1808.00020v6.pdf 1511.05082v1.pdf 1610.08000v1.pdf 1506.03736v2.pdf 1909.03824v1.pdf tar: stdout: write error # Arxiv naming convenitions are incompatible with WebDataset, but we can add # a file renaming function to the WebDataset to fix this. def arxiv_rename(name): return name.replace(\".pdf\", \"\").replace(\".\", \"_\") + \".pdf\" # For this example, we just use two shards, but usually, you would have hundreds # or thousands of shards. dataset = \"gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar\" # Let's open the dataset and read the first sample. shardurls = list(braceexpand.braceexpand(dataset)) ds = wds.WebDataset(shardurls, rename_files=arxiv_rename) sample = next(iter(ds)) summarize(sample) GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {} __key__ '1808_00020v6' __url__ 'gs://webdataset/testdata/arxiv-pdfs-000000.tar' pdf b'%PDF-1.5\\n%\\x8f\\n18 0 obj\\n<< /Filter /FlateDecode /Length 5428 >>\\nstream\\nx\\xda\\xad[]\\xb3\\xe3\\xb Running Tesseract on a Single PDF def process_sample(sample, maxpages=9999, shuffle=True): \"\"\"Process a sample from the Arxiv dataset. This function converts the PDF file to a sequence of JPEG images and then invokes Tesseract to recognize the text in the images. It returns a sequence of samples, one per page, each containing the JPEG image and the hOCR output from Tesseract. \"\"\" # We work in a temporary directory; most operations are command line tools with tempfile.TemporaryDirectory() as dirname: # Write the PDF file to disk and convert it to a sequence of JPEGs using pdftoppm pdfpath = dirname + \"/sample.pdf\" with open(pdfpath, \"wb\") as stream: stream.write(sample[\"pdf\"]) assert os.system(f\"(cd {dirname} && pdftoppm -forcenum -jpeg -r 300 -l 9999 sample.pdf page)\") == 0 # Next, we are going to iterate over the pages, convert them to text using tesseract, pages = sorted(glob.glob(dirname + \"/page-*.jpg\")) if shuffle: random.shuffle(pages) for page in islice(pages, maxpages): page_without_suffix = page[:-4] base = os.path.basename(page_without_suffix) # Invoke Tesseract to convert the page image to hOCR. os.system(f\"tesseract {page} {page_without_suffix} hocr\") # Construct the output sample. nsample = { \"__key__\": sample[\"__key__\"] + f\"/{base}\", \"jpg\": read_binary(page_without_suffix + \".jpg\"), \"hocr\": read_binary(page_without_suffix + \".hocr\"), } # This function returns an iterator over the recognized pages. yield nsample output = next(process_sample(sample)) summarize(output) Tesseract Open Source OCR Engine v4.1.1 with Leptonica __key__ '1808_00020v6/page-19' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xdb\\x00C\\x00\\x08\\x06\\x06\\x07\\x0 hocr b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional/ Processing a Shard of PDF Files def process_shard(src, dst, maxpdfs=999999, maxpages=9999): \"\"\"Process a shard of the Arxiv dataset. This function reads a shard of the Arxiv dataset, processes each sample using the process_sample function, and writes the page images and corresponding hOCR output to a new shard, one sample per page. The maxpdfs and maxpages parameters can be used to limit the number of samples and pages processed. This is useful for testing, as well as limit the number of pages selected from very long PDF documents. \"\"\" with wds.TarWriter(dst) as sink: for sample in islice(wds.WebDataset(src, rename_files=arxiv_rename), maxpdfs): print(sample[\"__key__\"], sample.keys()) for nsample in process_sample(sample, maxpages=maxpages): print(\" \", nsample[\"__key__\"]) sink.write(nsample) !rm -f output.tar process_shard(shardurls[0], \"output.tar\", maxpdfs=2, maxpages=2) GOPEN output.tar {} GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {} 1808_00020v6 dict_keys(['__key__', '__url__', 'pdf']) Tesseract Open Source OCR Engine v4.1.1 with Leptonica 1808_00020v6/page-17 Tesseract Open Source OCR Engine v4.1.1 with Leptonica 1808_00020v6/page-01 1511_05082v1 dict_keys(['__key__', '__url__', 'pdf']) Tesseract Open Source OCR Engine v4.1.1 with Leptonica 1511_05082v1/page-05 Tesseract Open Source OCR Engine v4.1.1 with Leptonica 1511_05082v1/page-08 !tar tvf output.tar -r--r--r-- bigdata/bigdata 14248 2023-12-17 21:55 1808_00020v6/page-17.hocr -r--r--r-- bigdata/bigdata 277568 2023-12-17 21:55 1808_00020v6/page-17.jpg -r--r--r-- bigdata/bigdata 103527 2023-12-17 21:55 1808_00020v6/page-01.hocr -r--r--r-- bigdata/bigdata 1186871 2023-12-17 21:55 1808_00020v6/page-01.jpg -r--r--r-- bigdata/bigdata 49187 2023-12-17 21:55 1511_05082v1/page-05.hocr -r--r--r-- bigdata/bigdata 589829 2023-12-17 21:55 1511_05082v1/page-05.jpg -r--r--r-- bigdata/bigdata 44814 2023-12-17 21:55 1511_05082v1/page-08.hocr -r--r--r-- bigdata/bigdata 490804 2023-12-17 21:55 1511_05082v1/page-08.jpg Parallelizing Processing with Ray This illustrates how to use Ray to process many shards in parallel. You don't need to use Ray for this, you can also invoke process_shard in parallel using a job queueing system or using some other distributed computing framework. Generally, it is easiest to process each shard sequentially, and to process multiple shards in parallel. However, you could use additional parallelization to perform processing of the samples in parallel. maxpdfs = 2 # for testing, we just use two PDFs per shard maxpages = 2 # for testing, we just use two pages per PDF upload_cmd = \"echo gsutil cp {src} {dst}\" # for testing, we don't actually upload the completed shards import ray if not ray.is_initialized(): ray.init() @ray.remote(num_cpus=4) def process_shard_parallel(src, dstbucket, maxpdfs=999999, maxpages=9999): \"\"\"Process a shard of the Arxiv dataset and upload the output shard to a bucket. This function reads a shard of the Arxiv dataset, processes each sample using the process_sample function, and writes the page images and corresponding hOCR output to a new shard, one sample per page. The output shard is then uploaded to the specified bucket using `upload_cmd`. \"\"\" dst = dstbucket + \"/\" + os.path.basename(src) with tempfile.NamedTemporaryFile() as tmp: process_shard(src, tmp.name, maxpdfs=maxpdfs, maxpages=maxpages) assert os.system(upload_cmd.format(src=tmp.name, dst=dst)) == 0 !rm -f output.tar ray.get([process_shard_parallel.remote(src, \"gs://somebucket\", maxpdfs=maxpdfs, maxpages=maxpages) for src in shardurls]) 2023-12-17 21:55:57,786 INFO worker.py:1664 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m \u001b[36m(process_shard_parallel pid=677110)\u001b[0m GOPEN /tmp/tmpznj5reiz {} \u001b[36m(process_shard_parallel pid=677110)\u001b[0m GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {} \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1402_1973v2 dict_keys(['__key__', '__url__', 'pdf']) \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Warning:guessing pitch as xheight on row 4, block 2 \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Warning:guessing pitch as xheight on row 27, block 2 \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Warning:guessing pitch as xheight on row 1, block 10 \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1402_1973v2/page-10 \u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1808_00020v6 dict_keys(['__key__', '__url__', 'pdf']) \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica \u001b[36m(process_shard_parallel pid=677113)\u001b[0m GOPEN gs://webdataset/testdata/arxiv-pdfs-000001.tar {}\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1402_1973v2/page-09 \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1612_01474v3 dict_keys(['__key__', '__url__', 'pdf']) \u001b[36m(process_shard_parallel pid=677110)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica \u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1808_00020v6/page-08 \u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1808_00020v6/page-19 \u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1511_05082v1 dict_keys(['__key__', '__url__', 'pdf']) \u001b[36m(process_shard_parallel pid=677110)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica\u001b[32m [repeated 4x across cluster]\u001b[0m \u001b[36m(process_shard_parallel pid=677113)\u001b[0m gsutil cp /tmp/tmpezg3f2_5 gs://somebucket/arxiv-pdfs-000001.tar \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1612_01474v3/page-11\u001b[32m [repeated 3x across cluster]\u001b[0m [None, None]","title":"Tesseract wds"},{"location":"tesseract-wds/#parallel-processing-of-shards-large-scale-ocr","text":"This notebook illustrates how to take a large collection of shards consisting of PDFs and process them using pdftoppm and tessearact into a new dataset consisting of page images and corresponding OCR output. The general approach is to process each shard sequentially and to process multiple shards in parallel. The basic structure of such a job looks like: with WebDataset(srcname) as src: with TarWriter(dstname) as dst: for sample in src: ... do something with sample ... dst.write(sample) upload(dstname)","title":"Parallel Processing of Shards: Large Scale OCR"},{"location":"tesseract-wds/#the-arxiv-dataset-of-pdfs","text":"# The dataset is tar files containing PDFs, each using the Arxiv naming convention. !gsutil cat gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar | tar tf - | sed 5q 1808.00020v6.pdf 1511.05082v1.pdf 1610.08000v1.pdf 1506.03736v2.pdf 1909.03824v1.pdf tar: stdout: write error # Arxiv naming convenitions are incompatible with WebDataset, but we can add # a file renaming function to the WebDataset to fix this. def arxiv_rename(name): return name.replace(\".pdf\", \"\").replace(\".\", \"_\") + \".pdf\" # For this example, we just use two shards, but usually, you would have hundreds # or thousands of shards. dataset = \"gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar\" # Let's open the dataset and read the first sample. shardurls = list(braceexpand.braceexpand(dataset)) ds = wds.WebDataset(shardurls, rename_files=arxiv_rename) sample = next(iter(ds)) summarize(sample) GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {} __key__ '1808_00020v6' __url__ 'gs://webdataset/testdata/arxiv-pdfs-000000.tar' pdf b'%PDF-1.5\\n%\\x8f\\n18 0 obj\\n<< /Filter /FlateDecode /Length 5428 >>\\nstream\\nx\\xda\\xad[]\\xb3\\xe3\\xb","title":"The Arxiv Dataset of PDFs"},{"location":"tesseract-wds/#running-tesseract-on-a-single-pdf","text":"def process_sample(sample, maxpages=9999, shuffle=True): \"\"\"Process a sample from the Arxiv dataset. This function converts the PDF file to a sequence of JPEG images and then invokes Tesseract to recognize the text in the images. It returns a sequence of samples, one per page, each containing the JPEG image and the hOCR output from Tesseract. \"\"\" # We work in a temporary directory; most operations are command line tools with tempfile.TemporaryDirectory() as dirname: # Write the PDF file to disk and convert it to a sequence of JPEGs using pdftoppm pdfpath = dirname + \"/sample.pdf\" with open(pdfpath, \"wb\") as stream: stream.write(sample[\"pdf\"]) assert os.system(f\"(cd {dirname} && pdftoppm -forcenum -jpeg -r 300 -l 9999 sample.pdf page)\") == 0 # Next, we are going to iterate over the pages, convert them to text using tesseract, pages = sorted(glob.glob(dirname + \"/page-*.jpg\")) if shuffle: random.shuffle(pages) for page in islice(pages, maxpages): page_without_suffix = page[:-4] base = os.path.basename(page_without_suffix) # Invoke Tesseract to convert the page image to hOCR. os.system(f\"tesseract {page} {page_without_suffix} hocr\") # Construct the output sample. nsample = { \"__key__\": sample[\"__key__\"] + f\"/{base}\", \"jpg\": read_binary(page_without_suffix + \".jpg\"), \"hocr\": read_binary(page_without_suffix + \".hocr\"), } # This function returns an iterator over the recognized pages. yield nsample output = next(process_sample(sample)) summarize(output) Tesseract Open Source OCR Engine v4.1.1 with Leptonica __key__ '1808_00020v6/page-19' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xdb\\x00C\\x00\\x08\\x06\\x06\\x07\\x0 hocr b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional/","title":"Running Tesseract on a Single PDF"},{"location":"tesseract-wds/#processing-a-shard-of-pdf-files","text":"def process_shard(src, dst, maxpdfs=999999, maxpages=9999): \"\"\"Process a shard of the Arxiv dataset. This function reads a shard of the Arxiv dataset, processes each sample using the process_sample function, and writes the page images and corresponding hOCR output to a new shard, one sample per page. The maxpdfs and maxpages parameters can be used to limit the number of samples and pages processed. This is useful for testing, as well as limit the number of pages selected from very long PDF documents. \"\"\" with wds.TarWriter(dst) as sink: for sample in islice(wds.WebDataset(src, rename_files=arxiv_rename), maxpdfs): print(sample[\"__key__\"], sample.keys()) for nsample in process_sample(sample, maxpages=maxpages): print(\" \", nsample[\"__key__\"]) sink.write(nsample) !rm -f output.tar process_shard(shardurls[0], \"output.tar\", maxpdfs=2, maxpages=2) GOPEN output.tar {} GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {} 1808_00020v6 dict_keys(['__key__', '__url__', 'pdf']) Tesseract Open Source OCR Engine v4.1.1 with Leptonica 1808_00020v6/page-17 Tesseract Open Source OCR Engine v4.1.1 with Leptonica 1808_00020v6/page-01 1511_05082v1 dict_keys(['__key__', '__url__', 'pdf']) Tesseract Open Source OCR Engine v4.1.1 with Leptonica 1511_05082v1/page-05 Tesseract Open Source OCR Engine v4.1.1 with Leptonica 1511_05082v1/page-08 !tar tvf output.tar -r--r--r-- bigdata/bigdata 14248 2023-12-17 21:55 1808_00020v6/page-17.hocr -r--r--r-- bigdata/bigdata 277568 2023-12-17 21:55 1808_00020v6/page-17.jpg -r--r--r-- bigdata/bigdata 103527 2023-12-17 21:55 1808_00020v6/page-01.hocr -r--r--r-- bigdata/bigdata 1186871 2023-12-17 21:55 1808_00020v6/page-01.jpg -r--r--r-- bigdata/bigdata 49187 2023-12-17 21:55 1511_05082v1/page-05.hocr -r--r--r-- bigdata/bigdata 589829 2023-12-17 21:55 1511_05082v1/page-05.jpg -r--r--r-- bigdata/bigdata 44814 2023-12-17 21:55 1511_05082v1/page-08.hocr -r--r--r-- bigdata/bigdata 490804 2023-12-17 21:55 1511_05082v1/page-08.jpg","title":"Processing a Shard of PDF Files"},{"location":"tesseract-wds/#parallelizing-processing-with-ray","text":"This illustrates how to use Ray to process many shards in parallel. You don't need to use Ray for this, you can also invoke process_shard in parallel using a job queueing system or using some other distributed computing framework. Generally, it is easiest to process each shard sequentially, and to process multiple shards in parallel. However, you could use additional parallelization to perform processing of the samples in parallel. maxpdfs = 2 # for testing, we just use two PDFs per shard maxpages = 2 # for testing, we just use two pages per PDF upload_cmd = \"echo gsutil cp {src} {dst}\" # for testing, we don't actually upload the completed shards import ray if not ray.is_initialized(): ray.init() @ray.remote(num_cpus=4) def process_shard_parallel(src, dstbucket, maxpdfs=999999, maxpages=9999): \"\"\"Process a shard of the Arxiv dataset and upload the output shard to a bucket. This function reads a shard of the Arxiv dataset, processes each sample using the process_sample function, and writes the page images and corresponding hOCR output to a new shard, one sample per page. The output shard is then uploaded to the specified bucket using `upload_cmd`. \"\"\" dst = dstbucket + \"/\" + os.path.basename(src) with tempfile.NamedTemporaryFile() as tmp: process_shard(src, tmp.name, maxpdfs=maxpdfs, maxpages=maxpages) assert os.system(upload_cmd.format(src=tmp.name, dst=dst)) == 0 !rm -f output.tar ray.get([process_shard_parallel.remote(src, \"gs://somebucket\", maxpdfs=maxpdfs, maxpages=maxpages) for src in shardurls]) 2023-12-17 21:55:57,786 INFO worker.py:1664 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m \u001b[36m(process_shard_parallel pid=677110)\u001b[0m GOPEN /tmp/tmpznj5reiz {} \u001b[36m(process_shard_parallel pid=677110)\u001b[0m GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {} \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1402_1973v2 dict_keys(['__key__', '__url__', 'pdf']) \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Warning:guessing pitch as xheight on row 4, block 2 \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Warning:guessing pitch as xheight on row 27, block 2 \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Warning:guessing pitch as xheight on row 1, block 10 \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1402_1973v2/page-10 \u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1808_00020v6 dict_keys(['__key__', '__url__', 'pdf']) \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica \u001b[36m(process_shard_parallel pid=677113)\u001b[0m GOPEN gs://webdataset/testdata/arxiv-pdfs-000001.tar {}\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1402_1973v2/page-09 \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1612_01474v3 dict_keys(['__key__', '__url__', 'pdf']) \u001b[36m(process_shard_parallel pid=677110)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica \u001b[36m(process_shard_parallel pid=677113)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica \u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1808_00020v6/page-08 \u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1808_00020v6/page-19 \u001b[36m(process_shard_parallel pid=677110)\u001b[0m 1511_05082v1 dict_keys(['__key__', '__url__', 'pdf']) \u001b[36m(process_shard_parallel pid=677110)\u001b[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica\u001b[32m [repeated 4x across cluster]\u001b[0m \u001b[36m(process_shard_parallel pid=677113)\u001b[0m gsutil cp /tmp/tmpezg3f2_5 gs://somebucket/arxiv-pdfs-000001.tar \u001b[36m(process_shard_parallel pid=677113)\u001b[0m 1612_01474v3/page-11\u001b[32m [repeated 3x across cluster]\u001b[0m [None, None]","title":"Parallelizing Processing with Ray"},{"location":"train-ocr-errors-hf/","text":"Fine Tuning LLM with Huggingface and WebDataset This notebook illustrates the use of WebDataset together with Huggingface for fine-tuning large language models. Some features of note: training data is loaded directly from Huggingface data is downloaded and stored locally incrementally as needed a custom sampler is used in order to make remote data access more efficient # parameters base_model = \"google/flan-t5-base\" dataset_url = ( \"https://huggingface.co/tmbdev/d-tokens/resolve/main/d-tokens.json?download=true\" ) cache_dir = \"./_cache\" batch_size = 1 max_steps = 10000 epochs = 1 learning_rate = 3e-4 # imports import string import random import numpy as np import regex import unicodedata import logging import torch.utils.data from transformers import AutoTokenizer, AutoModelForSequenceClassification from transformers import AutoModelForSeq2SeqLM from transformers.adapters import LoRAConfig from transformers import TrainingArguments, AdapterTrainer, TrainerCallback # workaround for running this in the source tree, you usually don't need this try: import wids except: sys.path += [\"..\"] import wids def normalize_string(s): \"\"\"Take a string and normalize it. Normalization removes common typographic variants of punctuation characters that would otherwise have to be learned explicitly by the model. It also simplifies whitespace and removes long strings of punctuations used for graphical effect. \"\"\" # start with Unicode normalization s = unicodedata.normalize(\"NFKC\", s) s = regex.sub(r\"[*@`]\", \"\", s) s = regex.sub(r\"[\\u0027\\u2019\\u2018\\u201A\\u201B]\", \"'\", s) # replace all single quotes with ' s = regex.sub(r\"[\\u0022\\u201C\\u201D\\u201E\\u201F]\", '\"', s) # replace all double quotes with \" s = regex.sub(r\"[\\u2013\\u2014\\u2012\\u2015-]\", \"-\", s) # normalize dashes s = regex.sub(r\"(\\p{P})\\p{P}+\", r\"\\1\", s) # remove duplicate punctuation s = regex.sub(r\"[^\\p{L}\\p{N}\\p{Z}().,?!:;'\\\"\\n-]+\", \" \", s) s = regex.sub(r\"[ \\t]+\", \" \", s) s = s.strip() return s # Data augmentation. Actually, in this case, we generate a synthetic training sample from # a clean input string. replacements = list( set(string.ascii_letters + string.digits + \" \" + \"\" + string.punctuation) - set([\"*\"]) ) def degrade(s, prange=(0.05, 0.1), seed=None, special=\"*\"): \"\"\"Generate training samples by degrading a string. Our model is a sequence-to-sequence model that identifies the location of OCR errors in a text string. It is trained on a synthetic dataset that contains pairs of strings, one of which is a degraded string, and the other is the degraded string with errors marked by asterisks. The model is trained to predict the location of the asterisks. \"\"\" seed = random.randint(0, 1000000) if seed is None else seed rng = random.Random(seed) s = normalize_string(s) if len(s) < 2: return s, s for _ in range(100): if rng.random() < 0.5: # use regex to delete the first k words, where k is random between 1 and 2 # we do this because otherwise the model will flag lower case letters at the beginning # of a string as errors k = rng.randint(1, 4) expr = r\"^([^\\p{Z}]+?\\p{Z}+){%d}\" % k s = regex.sub(expr, \"\", s, count=1) if len(s) > 1: break result = \"\" target = \"\" p = rng.uniform(*prange) for c in s: if c == special: continue if c != \"\\n\" and rng.random() < p: r = rng.choice(replacements) result += r target += special else: result += c target += c result = normalize_string(result) return result, target degrade(\"Hello, world's biggest ball-of-yarn!\") # We use Flan T5 as the base model. Other models might work better. tokenizer = AutoTokenizer.from_pretrained(base_model) # This is a helper function that takes a sample, unpacks it, applies the degradation, # and then returns a dictionary with the input_ids and the labels as required by Huggingface. def make_sample(sample, *, prange=(0.05, 0.1), seed=None, prefix=\"ocr-errors: \"): \"\"\"Given a sample consisting of a clean text string, generate a training sample. Args: sample: a sample from the webdataset prange: range of error probability seed: random seed or None for random seed prefix: prefix (prompt) to add to the input stringf \"\"\" clean = sample[\".txt.gz\"] clean = normalize_string(clean) text, target = degrade(clean, prange=prange, seed=seed) text_ids = torch.tensor( tokenizer.encode(prefix + text, max_length=512, truncation=True, padding=\"max_length\") ) target_ids = torch.tensor(tokenizer.encode(target, max_length=512, truncation=True, padding=\"max_length\")) return dict(input_ids=text_ids, labels=target_ids) # This is really all that is WebDataset specific: # - we specify a URL for the JSON index file # - we specify a local cache directory # - we instantiate a ShardListDataset with keep=True # - we add the make_sample transform to the dataset # - we create a custom sampler that respects shard boundaries dataset = wids.ShardListDataset( dataset_url, cache_dir=cache_dir, cache_size=int(1e10), keep=True ) dataset.add_transform(make_sample) dataset[999] sampler = wids.ShardedSampler(dataset) # This plot illustrates the behavior of the shard sampler: it generates a sequence # of samples from each shard in turn, and then moves on to the next shard. import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline plt.subplot(121) plt.plot(list(sampler)[:10000]) plt.subplot(122) plt.plot(list(sampler)[:500]); # Standard Hugginface LoRA setup. # start with the pretrained base model model = AutoModelForSeq2SeqLM.from_pretrained(base_model) # set the parameters for LoRA config = LoRAConfig( r=8, alpha=16, # use it on all of the layers intermediate_lora=True, output_lora=True, ) # make a new adapter for the xerr dataset model.add_adapter(\"xerr\", config=config) # enable the adapter for training model.train_adapter(\"xerr\") model.set_active_adapters([\"xerr\"]) # Standard Huggingface adapter training, except for the custom sampler. training_args = TrainingArguments( learning_rate=learning_rate, num_train_epochs=epochs, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, logging_steps=2000, save_steps=5000, output_dir=\"./training_output\", overwrite_output_dir=True, remove_unused_columns=False, max_steps=max_steps, ) # create the trainer trainer = AdapterTrainer( model=model, args=training_args, tokenizer=tokenizer, train_dataset=dataset, # eval_dataset=OCRDataset(\"test\", maxsize=100), ) # to set the sampler, we override the get_train_sampler method # Huggingface doesn't provide a better way to do this trainer._get_train_sampler = lambda: sampler # Run the bulk of the training. trainer.train() # Show some examples (this isn't really \"validation\"). num_validation = 10 validation_dataset = dataset logging.getLogger(\"transformers\").setLevel(logging.ERROR) for i in range(num_validation): # load the input and label (note: we get a different degradation each time) sample = validation_dataset[i] # convert the input and label to tensors input_ids = sample[\"input_ids\"].unsqueeze(0).to(0) label_ids = sample[\"labels\"].unsqueeze(0).to(0) # use the model to generate the output output = model.generate(input_ids, max_length=1024) # convert the tokens to text input_text = ( tokenizer.decode(input_ids[0], skip_special_tokens=True) .replace(\"ocr-errors:\", \"\") .strip() ) output_text = tokenizer.decode(output[0], skip_special_tokens=True).strip() label_text = tokenizer.decode(label_ids[0], skip_special_tokens=True).strip() print(f\"[{i}]\") print(\"Input: \", input_text) print(\"Output:\", output_text) print(\"Label: \", label_text) print(\"---\")","title":"Fine Tuning LLM with Huggingface and WebDataset"},{"location":"train-ocr-errors-hf/#fine-tuning-llm-with-huggingface-and-webdataset","text":"This notebook illustrates the use of WebDataset together with Huggingface for fine-tuning large language models. Some features of note: training data is loaded directly from Huggingface data is downloaded and stored locally incrementally as needed a custom sampler is used in order to make remote data access more efficient # parameters base_model = \"google/flan-t5-base\" dataset_url = ( \"https://huggingface.co/tmbdev/d-tokens/resolve/main/d-tokens.json?download=true\" ) cache_dir = \"./_cache\" batch_size = 1 max_steps = 10000 epochs = 1 learning_rate = 3e-4 # imports import string import random import numpy as np import regex import unicodedata import logging import torch.utils.data from transformers import AutoTokenizer, AutoModelForSequenceClassification from transformers import AutoModelForSeq2SeqLM from transformers.adapters import LoRAConfig from transformers import TrainingArguments, AdapterTrainer, TrainerCallback # workaround for running this in the source tree, you usually don't need this try: import wids except: sys.path += [\"..\"] import wids def normalize_string(s): \"\"\"Take a string and normalize it. Normalization removes common typographic variants of punctuation characters that would otherwise have to be learned explicitly by the model. It also simplifies whitespace and removes long strings of punctuations used for graphical effect. \"\"\" # start with Unicode normalization s = unicodedata.normalize(\"NFKC\", s) s = regex.sub(r\"[*@`]\", \"\", s) s = regex.sub(r\"[\\u0027\\u2019\\u2018\\u201A\\u201B]\", \"'\", s) # replace all single quotes with ' s = regex.sub(r\"[\\u0022\\u201C\\u201D\\u201E\\u201F]\", '\"', s) # replace all double quotes with \" s = regex.sub(r\"[\\u2013\\u2014\\u2012\\u2015-]\", \"-\", s) # normalize dashes s = regex.sub(r\"(\\p{P})\\p{P}+\", r\"\\1\", s) # remove duplicate punctuation s = regex.sub(r\"[^\\p{L}\\p{N}\\p{Z}().,?!:;'\\\"\\n-]+\", \" \", s) s = regex.sub(r\"[ \\t]+\", \" \", s) s = s.strip() return s # Data augmentation. Actually, in this case, we generate a synthetic training sample from # a clean input string. replacements = list( set(string.ascii_letters + string.digits + \" \" + \"\" + string.punctuation) - set([\"*\"]) ) def degrade(s, prange=(0.05, 0.1), seed=None, special=\"*\"): \"\"\"Generate training samples by degrading a string. Our model is a sequence-to-sequence model that identifies the location of OCR errors in a text string. It is trained on a synthetic dataset that contains pairs of strings, one of which is a degraded string, and the other is the degraded string with errors marked by asterisks. The model is trained to predict the location of the asterisks. \"\"\" seed = random.randint(0, 1000000) if seed is None else seed rng = random.Random(seed) s = normalize_string(s) if len(s) < 2: return s, s for _ in range(100): if rng.random() < 0.5: # use regex to delete the first k words, where k is random between 1 and 2 # we do this because otherwise the model will flag lower case letters at the beginning # of a string as errors k = rng.randint(1, 4) expr = r\"^([^\\p{Z}]+?\\p{Z}+){%d}\" % k s = regex.sub(expr, \"\", s, count=1) if len(s) > 1: break result = \"\" target = \"\" p = rng.uniform(*prange) for c in s: if c == special: continue if c != \"\\n\" and rng.random() < p: r = rng.choice(replacements) result += r target += special else: result += c target += c result = normalize_string(result) return result, target degrade(\"Hello, world's biggest ball-of-yarn!\") # We use Flan T5 as the base model. Other models might work better. tokenizer = AutoTokenizer.from_pretrained(base_model) # This is a helper function that takes a sample, unpacks it, applies the degradation, # and then returns a dictionary with the input_ids and the labels as required by Huggingface. def make_sample(sample, *, prange=(0.05, 0.1), seed=None, prefix=\"ocr-errors: \"): \"\"\"Given a sample consisting of a clean text string, generate a training sample. Args: sample: a sample from the webdataset prange: range of error probability seed: random seed or None for random seed prefix: prefix (prompt) to add to the input stringf \"\"\" clean = sample[\".txt.gz\"] clean = normalize_string(clean) text, target = degrade(clean, prange=prange, seed=seed) text_ids = torch.tensor( tokenizer.encode(prefix + text, max_length=512, truncation=True, padding=\"max_length\") ) target_ids = torch.tensor(tokenizer.encode(target, max_length=512, truncation=True, padding=\"max_length\")) return dict(input_ids=text_ids, labels=target_ids) # This is really all that is WebDataset specific: # - we specify a URL for the JSON index file # - we specify a local cache directory # - we instantiate a ShardListDataset with keep=True # - we add the make_sample transform to the dataset # - we create a custom sampler that respects shard boundaries dataset = wids.ShardListDataset( dataset_url, cache_dir=cache_dir, cache_size=int(1e10), keep=True ) dataset.add_transform(make_sample) dataset[999] sampler = wids.ShardedSampler(dataset) # This plot illustrates the behavior of the shard sampler: it generates a sequence # of samples from each shard in turn, and then moves on to the next shard. import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline plt.subplot(121) plt.plot(list(sampler)[:10000]) plt.subplot(122) plt.plot(list(sampler)[:500]); # Standard Hugginface LoRA setup. # start with the pretrained base model model = AutoModelForSeq2SeqLM.from_pretrained(base_model) # set the parameters for LoRA config = LoRAConfig( r=8, alpha=16, # use it on all of the layers intermediate_lora=True, output_lora=True, ) # make a new adapter for the xerr dataset model.add_adapter(\"xerr\", config=config) # enable the adapter for training model.train_adapter(\"xerr\") model.set_active_adapters([\"xerr\"]) # Standard Huggingface adapter training, except for the custom sampler. training_args = TrainingArguments( learning_rate=learning_rate, num_train_epochs=epochs, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, logging_steps=2000, save_steps=5000, output_dir=\"./training_output\", overwrite_output_dir=True, remove_unused_columns=False, max_steps=max_steps, ) # create the trainer trainer = AdapterTrainer( model=model, args=training_args, tokenizer=tokenizer, train_dataset=dataset, # eval_dataset=OCRDataset(\"test\", maxsize=100), ) # to set the sampler, we override the get_train_sampler method # Huggingface doesn't provide a better way to do this trainer._get_train_sampler = lambda: sampler # Run the bulk of the training. trainer.train() # Show some examples (this isn't really \"validation\"). num_validation = 10 validation_dataset = dataset logging.getLogger(\"transformers\").setLevel(logging.ERROR) for i in range(num_validation): # load the input and label (note: we get a different degradation each time) sample = validation_dataset[i] # convert the input and label to tensors input_ids = sample[\"input_ids\"].unsqueeze(0).to(0) label_ids = sample[\"labels\"].unsqueeze(0).to(0) # use the model to generate the output output = model.generate(input_ids, max_length=1024) # convert the tokens to text input_text = ( tokenizer.decode(input_ids[0], skip_special_tokens=True) .replace(\"ocr-errors:\", \"\") .strip() ) output_text = tokenizer.decode(output[0], skip_special_tokens=True).strip() label_text = tokenizer.decode(label_ids[0], skip_special_tokens=True).strip() print(f\"[{i}]\") print(\"Input: \", input_text) print(\"Output:\", output_text) print(\"Label: \", label_text) print(\"---\")","title":"Fine Tuning LLM with Huggingface and WebDataset"},{"location":"train-resnet50-multiray-wds/","text":"WebDataset + Distributed PyTorch Training This notebook illustrates how to use the Web Indexed Dataset ( wids ) library for distributed PyTorch training using DistributedDataParallel . Using webdataset results in training code that is almost identical to plain PyTorch except for the dataset creation. Since WebDataset is an iterable dataset, you need to account for that when creating the DataLoader . Furthermore, for distributed training, easy restarts, etc., it is convenient to use a resampled dataset; this is in contrast to sampling without replacement for each epoch as used more commonly for small, local training. (If you want to use sampling without replacement with webdataset format datasets, see the companion wids -based training notebooks.) Training with WebDataset can be carried out completely without local storage; this is the usual setup in the cloud and on high speed compute clusters. When running locally on a desktop, you may want to cache the data, and for that, you set a cache_dir directory. import os import sys import torch import torch.nn as nn import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torchvision.models import resnet50 from torchvision import datasets, transforms import ray import webdataset as wds import dataclasses import time from collections import deque from typing import Optional def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # Parameters epochs = 10 maxsteps = int(1e12) batch_size = 32 Data Loading for Distributed Training # Datasets are just collections of shards in the cloud. We usually specify # them using {lo..hi} brace notation (there is also a YAML spec for more complex # datasets). bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" trainset_url = bucket + \"/imagenet-train-{000000..001281}.tar\" valset_url = bucket + \"/imagenet-val-{000000..000049}.tar\" batch_size = 32 # If running in the cloud or with a fast network storage system, we don't # need any local storage. if \"google.colab\" in sys.modules: cache_dir = None print(\"running on colab, streaming data directly from storage\") else: cache_dir = \"./_cache\" print(f\"not running in colab, caching data locally in {cache_dir}\") # The dataloader pipeline is a fairly typical `IterableDataset` pipeline # for PyTorch def make_dataloader_train(): \"\"\"Create a DataLoader for training on the ImageNet dataset using WebDataset.\"\"\" transform = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), ] ) def make_sample(sample): return transform(sample[\"jpg\"]), sample[\"cls\"] # This is the basic WebDataset definition: it starts with a URL and add shuffling, # decoding, and augmentation. Note `resampled=True`; this is essential for # distributed training to work correctly. trainset = wds.WebDataset(trainset_url, resampled=True, shardshuffle=True, cache_dir=cache_dir, nodesplitter=wds.split_by_node) trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample) # For IterableDataset objects, the batching needs to happen in the dataset. trainset = trainset.batched(64) trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4) # We unbatch, shuffle, and rebatch to mix samples from different workers. trainloader = trainloader.unbatched().shuffle(1000).batched(batch_size) # A resampled dataset is infinite size, but we can recreate a fixed epoch length. trainloader = trainloader.with_epoch(1282 * 100 // 64) return trainloader # Let's try it out def make_dataloader(split=\"train\"): \"\"\"Make a dataloader for training or validation.\"\"\" if split == \"train\": return make_dataloader_train() elif split == \"val\": return make_dataloader_val() # not implemented for this notebook else: raise ValueError(f\"unknown split {split}\") # Try it out. os.environ[\"GOPEN_VERBOSE\"] = \"1\" sample = next(iter(make_dataloader())) print(sample[0].shape, sample[1].shape) os.environ[\"GOPEN_VERBOSE\"] = \"0\" Standard PyTorch Training This is completely standard PyTorch training; nothing changes by using WebDataset. # We gather all the configuration info into a single typed dataclass. @dataclasses.dataclass class Config: epochs: int = 1 max_steps: int = int(1e18) lr: float = 0.001 momentum: float = 0.9 rank: Optional[int] = None world_size: int = 2 backend: str = \"nccl\" master_addr: str = \"localhost\" master_port: str = \"12355\" report_s: float = 15.0 report_growth: float = 1.1 def train(config): # Define the model, loss function, and optimizer model = resnet50(pretrained=False).cuda() if config.rank is not None: model = DistributedDataParallel(model) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=config.lr) # Data loading code trainloader = make_dataloader(split=\"train\") losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0 # Training loop for epoch in range(config.epochs): for i, data, verbose in enumerate_report(trainloader, config.report_s): inputs, labels = data[0].cuda(), data[1].cuda() # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) # update statistics loss = loss_fn(outputs, labels) accuracy = ( (outputs.argmax(1) == labels).float().mean() ) # calculate accuracy losses.append(loss.item()) accuracies.append(accuracy.item()) if verbose and len(losses) > 0: avgloss = sum(losses) / len(losses) avgaccuracy = sum(accuracies) / len(accuracies) print( f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr, ) loss.backward() optimizer.step() steps += len(labels) if steps > config.max_steps: print( \"finished training (max_steps)\", steps, config.max_steps, file=sys.stderr, ) return print(\"finished Training\", steps) # A quick smoke test of the training function. config = Config() config.epochs = 1 config.max_steps = 1000 train(config) Setting up Distributed Training with Ray Ray is a convenient distributed computing framework. We are using it here to start up the training jobs on multiple GPUs. You can use torch.distributed.launch or other such tools as well with the above code. Ray has the advantage that it is runtime environment independent; you set up your Ray cluster in whatever way works for your environment, and afterwards, this code will run in it without change. @ray.remote(num_gpus=1) def train_on_ray(rank, config): \"\"\"Set up distributed torch env and train the model on this node.\"\"\" # Set up distributed PyTorch. if rank is not None: os.environ[\"MASTER_ADDR\"] = config.master_addr os.environ[\"MASTER_PORT\"] = config.master_port dist.init_process_group( backend=config.backend, rank=rank, world_size=config.world_size ) config.rank = rank # Ray will automatically set CUDA_VISIBLE_DEVICES for each task. train(config) if not ray.is_initialized(): ray.init() ray.available_resources()[\"GPU\"] def distributed_training(config): \"\"\"Perform distributed training with the given config.\"\"\" num_gpus = ray.available_resources()[\"GPU\"] config.world_size = min(config.world_size, num_gpus) results = ray.get( [train_on_ray.remote(i, config) for i in range(config.world_size)] ) print(results) config = Config() config.epochs = epochs config.max_steps = max_steps config.batch_size = batch_size print(config) distributed_training(config)","title":"WebDataset + Distributed PyTorch Training"},{"location":"train-resnet50-multiray-wds/#webdataset-distributed-pytorch-training","text":"This notebook illustrates how to use the Web Indexed Dataset ( wids ) library for distributed PyTorch training using DistributedDataParallel . Using webdataset results in training code that is almost identical to plain PyTorch except for the dataset creation. Since WebDataset is an iterable dataset, you need to account for that when creating the DataLoader . Furthermore, for distributed training, easy restarts, etc., it is convenient to use a resampled dataset; this is in contrast to sampling without replacement for each epoch as used more commonly for small, local training. (If you want to use sampling without replacement with webdataset format datasets, see the companion wids -based training notebooks.) Training with WebDataset can be carried out completely without local storage; this is the usual setup in the cloud and on high speed compute clusters. When running locally on a desktop, you may want to cache the data, and for that, you set a cache_dir directory. import os import sys import torch import torch.nn as nn import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torchvision.models import resnet50 from torchvision import datasets, transforms import ray import webdataset as wds import dataclasses import time from collections import deque from typing import Optional def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # Parameters epochs = 10 maxsteps = int(1e12) batch_size = 32","title":"WebDataset + Distributed PyTorch Training"},{"location":"train-resnet50-multiray-wds/#data-loading-for-distributed-training","text":"# Datasets are just collections of shards in the cloud. We usually specify # them using {lo..hi} brace notation (there is also a YAML spec for more complex # datasets). bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" trainset_url = bucket + \"/imagenet-train-{000000..001281}.tar\" valset_url = bucket + \"/imagenet-val-{000000..000049}.tar\" batch_size = 32 # If running in the cloud or with a fast network storage system, we don't # need any local storage. if \"google.colab\" in sys.modules: cache_dir = None print(\"running on colab, streaming data directly from storage\") else: cache_dir = \"./_cache\" print(f\"not running in colab, caching data locally in {cache_dir}\") # The dataloader pipeline is a fairly typical `IterableDataset` pipeline # for PyTorch def make_dataloader_train(): \"\"\"Create a DataLoader for training on the ImageNet dataset using WebDataset.\"\"\" transform = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), ] ) def make_sample(sample): return transform(sample[\"jpg\"]), sample[\"cls\"] # This is the basic WebDataset definition: it starts with a URL and add shuffling, # decoding, and augmentation. Note `resampled=True`; this is essential for # distributed training to work correctly. trainset = wds.WebDataset(trainset_url, resampled=True, shardshuffle=True, cache_dir=cache_dir, nodesplitter=wds.split_by_node) trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample) # For IterableDataset objects, the batching needs to happen in the dataset. trainset = trainset.batched(64) trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4) # We unbatch, shuffle, and rebatch to mix samples from different workers. trainloader = trainloader.unbatched().shuffle(1000).batched(batch_size) # A resampled dataset is infinite size, but we can recreate a fixed epoch length. trainloader = trainloader.with_epoch(1282 * 100 // 64) return trainloader # Let's try it out def make_dataloader(split=\"train\"): \"\"\"Make a dataloader for training or validation.\"\"\" if split == \"train\": return make_dataloader_train() elif split == \"val\": return make_dataloader_val() # not implemented for this notebook else: raise ValueError(f\"unknown split {split}\") # Try it out. os.environ[\"GOPEN_VERBOSE\"] = \"1\" sample = next(iter(make_dataloader())) print(sample[0].shape, sample[1].shape) os.environ[\"GOPEN_VERBOSE\"] = \"0\"","title":"Data Loading for Distributed Training"},{"location":"train-resnet50-multiray-wds/#standard-pytorch-training","text":"This is completely standard PyTorch training; nothing changes by using WebDataset. # We gather all the configuration info into a single typed dataclass. @dataclasses.dataclass class Config: epochs: int = 1 max_steps: int = int(1e18) lr: float = 0.001 momentum: float = 0.9 rank: Optional[int] = None world_size: int = 2 backend: str = \"nccl\" master_addr: str = \"localhost\" master_port: str = \"12355\" report_s: float = 15.0 report_growth: float = 1.1 def train(config): # Define the model, loss function, and optimizer model = resnet50(pretrained=False).cuda() if config.rank is not None: model = DistributedDataParallel(model) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=config.lr) # Data loading code trainloader = make_dataloader(split=\"train\") losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0 # Training loop for epoch in range(config.epochs): for i, data, verbose in enumerate_report(trainloader, config.report_s): inputs, labels = data[0].cuda(), data[1].cuda() # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) # update statistics loss = loss_fn(outputs, labels) accuracy = ( (outputs.argmax(1) == labels).float().mean() ) # calculate accuracy losses.append(loss.item()) accuracies.append(accuracy.item()) if verbose and len(losses) > 0: avgloss = sum(losses) / len(losses) avgaccuracy = sum(accuracies) / len(accuracies) print( f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr, ) loss.backward() optimizer.step() steps += len(labels) if steps > config.max_steps: print( \"finished training (max_steps)\", steps, config.max_steps, file=sys.stderr, ) return print(\"finished Training\", steps) # A quick smoke test of the training function. config = Config() config.epochs = 1 config.max_steps = 1000 train(config)","title":"Standard PyTorch Training"},{"location":"train-resnet50-multiray-wds/#setting-up-distributed-training-with-ray","text":"Ray is a convenient distributed computing framework. We are using it here to start up the training jobs on multiple GPUs. You can use torch.distributed.launch or other such tools as well with the above code. Ray has the advantage that it is runtime environment independent; you set up your Ray cluster in whatever way works for your environment, and afterwards, this code will run in it without change. @ray.remote(num_gpus=1) def train_on_ray(rank, config): \"\"\"Set up distributed torch env and train the model on this node.\"\"\" # Set up distributed PyTorch. if rank is not None: os.environ[\"MASTER_ADDR\"] = config.master_addr os.environ[\"MASTER_PORT\"] = config.master_port dist.init_process_group( backend=config.backend, rank=rank, world_size=config.world_size ) config.rank = rank # Ray will automatically set CUDA_VISIBLE_DEVICES for each task. train(config) if not ray.is_initialized(): ray.init() ray.available_resources()[\"GPU\"] def distributed_training(config): \"\"\"Perform distributed training with the given config.\"\"\" num_gpus = ray.available_resources()[\"GPU\"] config.world_size = min(config.world_size, num_gpus) results = ray.get( [train_on_ray.remote(i, config) for i in range(config.world_size)] ) print(results) config = Config() config.epochs = epochs config.max_steps = max_steps config.batch_size = batch_size print(config) distributed_training(config)","title":"Setting up Distributed Training with Ray"},{"location":"train-resnet50-multiray-wids/","text":"WebIndexedDataset + Distributed PyTorch Training This notebook illustrates how to use the Web Indexed Dataset ( wids ) library for distributed PyTorch training using DistributedDataParallel . Using wids results in training code that is almost identical to plain PyTorch, with the only changes being the use of ShardListDataset for the dataset construction, and the use of the DistributedChunkedSampler for generating random samples from the dataset. ShardListDataset requires some local storage. By default, that local storage just grows as shards are downloaded, but if you have limited space, you can run create_cleanup_background_process to clean up the cache; shards will be re-downloaded as necessary. import os import sys from typing import ( List, Tuple, Dict, Optional, Any, Union, Callable, Iterable, Iterator, NamedTuple, Set, Sequence, ) import torch import torch.nn as nn import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torchvision.models import resnet50 from torchvision import datasets, transforms from torch.utils.data import DataLoader import ray import wids import dataclasses import time from collections import deque from pprint import pprint def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth Data Loading for Distributed Training The datasets we use for training are stored in the cloud. We use fake-imagenet , which is 1/10th the size of Imagenet and artificially generated, but it has the same number of shards and trains quickly. Note that unlike the webdataset library, wids always needs a local cache directory (it will use /tmp if you don't give it anything explicitly). # Parameters epochs = 1 max_steps = int(1e12) batch_size = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet/\" trainset_url = bucket+\"imagenet-train.json\" valset_url = bucket+\"imagenet-val.json\" cache_dir = \"./_cache\" # This is a typical PyTorch dataset, except that we read from the cloud. def make_dataset_train(): transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) def make_sample(sample): image = sample[\".jpg\"] label = sample[\".cls\"] return transform_train(image), label trainset = wids.ShardListDataset(trainset_url, cache_dir=\"./_cache\", keep=True) trainset = trainset.add_transform(make_sample) return trainset This is really the only thing that is ever so slightly special about the wids library: you should use the special DistributedChunkedSampler for sampling. The regular DistributedSampler will technically work, but because of its poor locality of reference, will be significantly slower. # To keep locality of reference in the dataloader, we use a special sampler # for distributed training, DistributedChunkedSampler. def make_dataloader_train(): dataset = make_dataset_train() sampler = wids.DistributedChunkedSampler(dataset, chunksize=1000, shuffle=True) dataloader = DataLoader( dataset, batch_size=batch_size, sampler=sampler, num_workers=4 ) return dataloader def make_dataloader(split=\"train\"): \"\"\"Make a dataloader for training or validation.\"\"\" if split == \"train\": return make_dataloader_train() elif split == \"val\": return make_dataloader_val() else: raise ValueError(f\"unknown split {split}\") # Try it out. sample = next(iter(make_dataloader())) print(sample[0].shape, sample[1].shape) PyTorch Distributed Training Code Really, all that's needed for distributed training is the DistributedDataParallel wrapper around the model. # For convenience, we collect all the configuration parameters into # a dataclass. @dataclasses.dataclass class Config: rank: Optional[int] = None epochs: int = 1 max_steps: int = int(1e18) lr: float = 0.001 momentum: float = 0.9 world_size: int = 8 backend: str = \"nccl\" master_addr: str = \"localhost\" master_port: str = \"12355\" report_s: float = 15.0 report_growth: float = 1.1 Config() # A typical PyTorch training function. def train(config): # Define the model, loss function, and optimizer model = resnet50(pretrained=False).cuda() if config.rank is not None: model = DistributedDataParallel(model) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=config.lr) # Data loading code trainloader = make_dataloader(split=\"train\") losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0 # Training loop for epoch in range(config.epochs): for i, data, verbose in enumerate_report(trainloader, config.report_s): inputs, labels = data[0].cuda(), data[1].cuda() # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() # just bookkeping and progress report steps += len(labels) accuracy = ( (outputs.argmax(1) == labels).float().mean() ) # calculate accuracy losses.append(loss.item()) accuracies.append(accuracy.item()) if verbose and len(losses) > 0: avgloss = sum(losses) / len(losses) avgaccuracy = sum(accuracies) / len(accuracies) print( f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr, ) if steps > config.max_steps: print( \"finished training (max_steps)\", steps, config.max_steps, file=sys.stderr, ) return print(\"finished Training\", steps) # A quick smoke test. os.environ[\"GOPEN_VERBOSE\"] = \"1\" config = Config() config.epochs = 1 config.max_steps = 1000 train(config) os.environ[\"GOPEN_VERBOSE\"] = \"0\" Distributed Training in Ray The code above can be used with any distributed computing framwork, including torch.distributed.launch . Below is simply an example of how to launch the training jobs with the Ray framework. Ray is nice for distributed training because it makes the Python code independent of the runtime environment (Kubernetes, Slurm, ad-hoc networking, etc.). Meaning, the code below will work regardless of how you start up your Ray cluster. # The distributed training function to be used with Ray. # Since this is started via Ray remote, we set up the distributed # training environment here. @ray.remote(num_gpus=1) def train_in_ray(rank, config): if rank is not None: # Set up distributed PyTorch. config.rank = rank os.environ[\"MASTER_ADDR\"] = config.master_addr os.environ[\"MASTER_PORT\"] = config.master_port dist.init_process_group( backend=config.backend, rank=rank, world_size=config.world_size ) # Ray will automatically set CUDA_VISIBLE_DEVICES for each task. train(config) if not ray.is_initialized(): ray.init() print(\"#gpus available in the cluster\", ray.available_resources()[\"GPU\"]) def distributed_training(config): num_gpus = ray.available_resources()[\"GPU\"] config.world_size = int(min(config.world_size, num_gpus)) pprint(config) results = ray.get( [train_in_ray.remote(i, config) for i in range(config.world_size)] ) print(results) config = Config() config.epochs = epochs config.max_steps = max_steps config.batch_size = batch_size print(config) distributed_training(config)","title":"WebIndexedDataset + Distributed PyTorch Training"},{"location":"train-resnet50-multiray-wids/#webindexeddataset-distributed-pytorch-training","text":"This notebook illustrates how to use the Web Indexed Dataset ( wids ) library for distributed PyTorch training using DistributedDataParallel . Using wids results in training code that is almost identical to plain PyTorch, with the only changes being the use of ShardListDataset for the dataset construction, and the use of the DistributedChunkedSampler for generating random samples from the dataset. ShardListDataset requires some local storage. By default, that local storage just grows as shards are downloaded, but if you have limited space, you can run create_cleanup_background_process to clean up the cache; shards will be re-downloaded as necessary. import os import sys from typing import ( List, Tuple, Dict, Optional, Any, Union, Callable, Iterable, Iterator, NamedTuple, Set, Sequence, ) import torch import torch.nn as nn import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torchvision.models import resnet50 from torchvision import datasets, transforms from torch.utils.data import DataLoader import ray import wids import dataclasses import time from collections import deque from pprint import pprint def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth","title":"WebIndexedDataset + Distributed PyTorch Training"},{"location":"train-resnet50-multiray-wids/#data-loading-for-distributed-training","text":"The datasets we use for training are stored in the cloud. We use fake-imagenet , which is 1/10th the size of Imagenet and artificially generated, but it has the same number of shards and trains quickly. Note that unlike the webdataset library, wids always needs a local cache directory (it will use /tmp if you don't give it anything explicitly). # Parameters epochs = 1 max_steps = int(1e12) batch_size = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet/\" trainset_url = bucket+\"imagenet-train.json\" valset_url = bucket+\"imagenet-val.json\" cache_dir = \"./_cache\" # This is a typical PyTorch dataset, except that we read from the cloud. def make_dataset_train(): transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) def make_sample(sample): image = sample[\".jpg\"] label = sample[\".cls\"] return transform_train(image), label trainset = wids.ShardListDataset(trainset_url, cache_dir=\"./_cache\", keep=True) trainset = trainset.add_transform(make_sample) return trainset This is really the only thing that is ever so slightly special about the wids library: you should use the special DistributedChunkedSampler for sampling. The regular DistributedSampler will technically work, but because of its poor locality of reference, will be significantly slower. # To keep locality of reference in the dataloader, we use a special sampler # for distributed training, DistributedChunkedSampler. def make_dataloader_train(): dataset = make_dataset_train() sampler = wids.DistributedChunkedSampler(dataset, chunksize=1000, shuffle=True) dataloader = DataLoader( dataset, batch_size=batch_size, sampler=sampler, num_workers=4 ) return dataloader def make_dataloader(split=\"train\"): \"\"\"Make a dataloader for training or validation.\"\"\" if split == \"train\": return make_dataloader_train() elif split == \"val\": return make_dataloader_val() else: raise ValueError(f\"unknown split {split}\") # Try it out. sample = next(iter(make_dataloader())) print(sample[0].shape, sample[1].shape)","title":"Data Loading for Distributed Training"},{"location":"train-resnet50-multiray-wids/#pytorch-distributed-training-code","text":"Really, all that's needed for distributed training is the DistributedDataParallel wrapper around the model. # For convenience, we collect all the configuration parameters into # a dataclass. @dataclasses.dataclass class Config: rank: Optional[int] = None epochs: int = 1 max_steps: int = int(1e18) lr: float = 0.001 momentum: float = 0.9 world_size: int = 8 backend: str = \"nccl\" master_addr: str = \"localhost\" master_port: str = \"12355\" report_s: float = 15.0 report_growth: float = 1.1 Config() # A typical PyTorch training function. def train(config): # Define the model, loss function, and optimizer model = resnet50(pretrained=False).cuda() if config.rank is not None: model = DistributedDataParallel(model) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=config.lr) # Data loading code trainloader = make_dataloader(split=\"train\") losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0 # Training loop for epoch in range(config.epochs): for i, data, verbose in enumerate_report(trainloader, config.report_s): inputs, labels = data[0].cuda(), data[1].cuda() # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() # just bookkeping and progress report steps += len(labels) accuracy = ( (outputs.argmax(1) == labels).float().mean() ) # calculate accuracy losses.append(loss.item()) accuracies.append(accuracy.item()) if verbose and len(losses) > 0: avgloss = sum(losses) / len(losses) avgaccuracy = sum(accuracies) / len(accuracies) print( f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr, ) if steps > config.max_steps: print( \"finished training (max_steps)\", steps, config.max_steps, file=sys.stderr, ) return print(\"finished Training\", steps) # A quick smoke test. os.environ[\"GOPEN_VERBOSE\"] = \"1\" config = Config() config.epochs = 1 config.max_steps = 1000 train(config) os.environ[\"GOPEN_VERBOSE\"] = \"0\"","title":"PyTorch Distributed Training Code"},{"location":"train-resnet50-multiray-wids/#distributed-training-in-ray","text":"The code above can be used with any distributed computing framwork, including torch.distributed.launch . Below is simply an example of how to launch the training jobs with the Ray framework. Ray is nice for distributed training because it makes the Python code independent of the runtime environment (Kubernetes, Slurm, ad-hoc networking, etc.). Meaning, the code below will work regardless of how you start up your Ray cluster. # The distributed training function to be used with Ray. # Since this is started via Ray remote, we set up the distributed # training environment here. @ray.remote(num_gpus=1) def train_in_ray(rank, config): if rank is not None: # Set up distributed PyTorch. config.rank = rank os.environ[\"MASTER_ADDR\"] = config.master_addr os.environ[\"MASTER_PORT\"] = config.master_port dist.init_process_group( backend=config.backend, rank=rank, world_size=config.world_size ) # Ray will automatically set CUDA_VISIBLE_DEVICES for each task. train(config) if not ray.is_initialized(): ray.init() print(\"#gpus available in the cluster\", ray.available_resources()[\"GPU\"]) def distributed_training(config): num_gpus = ray.available_resources()[\"GPU\"] config.world_size = int(min(config.world_size, num_gpus)) pprint(config) results = ray.get( [train_in_ray.remote(i, config) for i in range(config.world_size)] ) print(results) config = Config() config.epochs = epochs config.max_steps = max_steps config.batch_size = batch_size print(config) distributed_training(config)","title":"Distributed Training in Ray"},{"location":"train-resnet50-wds/","text":"Resnet 50 Training on (Fake)Imagenet with WebDataset This notebook illustrates how to use WebDataset with PyTorch training. # imports %matplotlib inline from functools import partial from pprint import pprint import random from collections import deque import numpy as np from matplotlib import pyplot as plt import sys import os import torch import torchvision import torchvision.transforms as transforms from torchvision.models import resnet50 from torch.utils.data import DataLoader from torch import nn, optim # helpers import time def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # We usually abbreviate webdataset as wds import webdataset as wds # parameters epochs = 1 max_steps = int(1e12) batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\" Data Loader Construction # WebDataset is designed to work without any local storage. Use caching # only if you are on a desktop with slow networking. if 'google.colab' in sys.modules: cache_dir = None print(\"running on colab, streaming data directly from storage\") else: !mkdir -p ./_cache cache_dir = \"./_cache\" print(f\"not running in colab, caching data locally in {cache_dir}\") # The standard TorchVision transformations. transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) def make_sample(sample, val=False): \"\"\"Take a decoded sample dictionary, augment it, and return an (image, label) tuple.\"\"\" assert not val, \"only implemented training dataset for this notebook\" image = sample[\"jpg\"] label = sample[\"cls\"] return transform_train(image), label # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset( training_urls, resampled=True, cache_dir=cache_dir, shardshuffle=True ) trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample) # Since this is an IterableDataset, PyTorch requires that we batch in the dataset. # WebLoader is PyTorch DataLoader with some convenience methods. trainset = trainset.batched(64) trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4) # Unbatch, shuffle between workers, then rebatch. trainloader = trainloader.unbatched().shuffle(1000).batched(64) # Since we are using resampling, the dataset is infinite; set an artificial epoch size. trainloader = trainloader.with_epoch(1282 * 100 // 64) # Smoke test it. os.environ[\"GOPEN_VERBOSE\"] = \"1\" images, classes = next(iter(trainloader)) print(images.shape, classes.shape) os.environ[\"GOPEN_VERBOSE\"] = \"0\" PyTorch Training This is a typical PyTorch training pipeline. # The usual PyTorch model definition. We use an uninitialized ResNet50 model. model = resnet50(pretrained=False) # Define the loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) # Move the model to the GPU if available device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") model = model.to(device) losses, accuracies = deque(maxlen=100), deque(maxlen=100) steps = 0 # Train the model for epoch in range(epochs): for i, data, verbose in enumerate_report(trainloader, 5): # get the inputs; data is a list of [inputs, labels] inputs, labels = data[0].to(device), data[1].to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() pred = outputs.cpu().detach().argmax(dim=1, keepdim=True) correct = pred.eq(labels.cpu().view_as(pred)).sum().item() accuracy = correct / float(len(labels)) losses.append(loss.item()) accuracies.append(accuracy) steps += len(inputs) if verbose and len(losses) > 5: print( \"[%d, %5d] loss: %.5f correct: %.5f\" % (epoch + 1, i + 1, np.mean(losses), np.mean(accuracies)) ) running_loss = 0.0 if steps > max_steps: break if steps > max_steps: break print(\"Finished Training\")","title":"Resnet 50 Training on (Fake)Imagenet with WebDataset"},{"location":"train-resnet50-wds/#resnet-50-training-on-fakeimagenet-with-webdataset","text":"This notebook illustrates how to use WebDataset with PyTorch training. # imports %matplotlib inline from functools import partial from pprint import pprint import random from collections import deque import numpy as np from matplotlib import pyplot as plt import sys import os import torch import torchvision import torchvision.transforms as transforms from torchvision.models import resnet50 from torch.utils.data import DataLoader from torch import nn, optim # helpers import time def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # We usually abbreviate webdataset as wds import webdataset as wds # parameters epochs = 1 max_steps = int(1e12) batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\"","title":"Resnet 50 Training on (Fake)Imagenet with WebDataset"},{"location":"train-resnet50-wds/#data-loader-construction","text":"# WebDataset is designed to work without any local storage. Use caching # only if you are on a desktop with slow networking. if 'google.colab' in sys.modules: cache_dir = None print(\"running on colab, streaming data directly from storage\") else: !mkdir -p ./_cache cache_dir = \"./_cache\" print(f\"not running in colab, caching data locally in {cache_dir}\") # The standard TorchVision transformations. transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) def make_sample(sample, val=False): \"\"\"Take a decoded sample dictionary, augment it, and return an (image, label) tuple.\"\"\" assert not val, \"only implemented training dataset for this notebook\" image = sample[\"jpg\"] label = sample[\"cls\"] return transform_train(image), label # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset( training_urls, resampled=True, cache_dir=cache_dir, shardshuffle=True ) trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample) # Since this is an IterableDataset, PyTorch requires that we batch in the dataset. # WebLoader is PyTorch DataLoader with some convenience methods. trainset = trainset.batched(64) trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4) # Unbatch, shuffle between workers, then rebatch. trainloader = trainloader.unbatched().shuffle(1000).batched(64) # Since we are using resampling, the dataset is infinite; set an artificial epoch size. trainloader = trainloader.with_epoch(1282 * 100 // 64) # Smoke test it. os.environ[\"GOPEN_VERBOSE\"] = \"1\" images, classes = next(iter(trainloader)) print(images.shape, classes.shape) os.environ[\"GOPEN_VERBOSE\"] = \"0\"","title":"Data Loader Construction"},{"location":"train-resnet50-wds/#pytorch-training","text":"This is a typical PyTorch training pipeline. # The usual PyTorch model definition. We use an uninitialized ResNet50 model. model = resnet50(pretrained=False) # Define the loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) # Move the model to the GPU if available device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") model = model.to(device) losses, accuracies = deque(maxlen=100), deque(maxlen=100) steps = 0 # Train the model for epoch in range(epochs): for i, data, verbose in enumerate_report(trainloader, 5): # get the inputs; data is a list of [inputs, labels] inputs, labels = data[0].to(device), data[1].to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() pred = outputs.cpu().detach().argmax(dim=1, keepdim=True) correct = pred.eq(labels.cpu().view_as(pred)).sum().item() accuracy = correct / float(len(labels)) losses.append(loss.item()) accuracies.append(accuracy) steps += len(inputs) if verbose and len(losses) > 5: print( \"[%d, %5d] loss: %.5f correct: %.5f\" % (epoch + 1, i + 1, np.mean(losses), np.mean(accuracies)) ) running_loss = 0.0 if steps > max_steps: break if steps > max_steps: break print(\"Finished Training\")","title":"PyTorch Training"},{"location":"train-resnet50-wids/","text":"%matplotlib inline from functools import partial from pprint import pprint import random from collections import deque import numpy as np from matplotlib import pyplot as plt import torch import torchvision import torchvision.transforms as transforms from torchvision.models import resnet50 from torch.utils.data import DataLoader from torch import nn, optim import wids # parameters epochs = 3 max_steps = 100000 batch_size = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet/\" num_workers = 4 cache_dir = \"./_cache\" # helpers import time def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # The standard TorchVision transformations. transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) transform_val = transforms.Compose( [ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) # The dataset returns dictionaries. This is a small function we transform it # with to get the augmented image and the label. def make_sample(sample, val=False): image = sample[\".jpg\"] label = sample[\".cls\"] if val: return transform_val(image), label else: return transform_train(image), label # These are standard PyTorch datasets. Download is incremental into the cache. trainset = wids.ShardListDataset( bucket+\"imagenet-train.json\", cache_dir=cache_dir, keep=True ) valset = wids.ShardListDataset( bucket+\"imagenet-val.json\", cache_dir=cache_dir, keep=True ) trainset[0] # Next, we add the transformation to the dataset. Transformations # are executed in sequence. In fact, by default, there is a transformation # that reads and decodes images. trainset.add_transform(make_sample) valset.add_transform(partial(make_sample, val=True)) print(trainset[0][0].shape, trainset[0][1]) # We also need a sampler for the training set. There are three # special samplers in the `wids` package that work particularly # well with sharded datasets: # - `wids.ShardedSampler` shuffles shards and then samples in shards; # it guarantees that only one shard is used at a time # - `wids.ChunkedSampler` samples by fixed sized chunks, shuffles # the chunks, and the the samples within each chunk # - `wids.DistributedChunkedSampler` is like `ChunkedSampler` but # works with distributed training (it first divides the entire # dataset into per-node chunks, then the per-node chunks into # smaller chunks, then shuffles the smaller chunks) # trainsampler = wids.ShardedSampler(trainset) # trainsampler = wids.ChunkedSampler(trainset, chunksize=1000, shuffle=True) trainsampler = wids.DistributedChunkedSampler(trainset, chunksize=1000, shuffle=True) plt.plot(list(trainsampler)[:2500]) # Note that the sampler shuffles within each shard before moving on to # the next shard. Furthermore, on the first epoch, the sampler # uses the shards in order, but on subsequent epochs, it shuffles # them. This makes testing and debugging easier. If you don't like # this behavior, you can use shufflefirst=True trainsampler.set_epoch(0) # Create data loaders for the training and validation datasets trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=4, sampler=trainsampler) valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4) images, classes = next(iter(trainloader)) print(images.shape, classes.shape) # The usual PyTorch model definition. We use an uninitialized ResNet50 model. model = resnet50(pretrained=False) # Define the loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) # Move the model to the GPU if available device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") model = model.to(device) losses, accuracies = deque(maxlen=100), deque(maxlen=100) steps = 0 # Train the model for epoch in range(epochs): for i, data, verbose in enumerate_report(trainloader, 5): # get the inputs; data is a list of [inputs, labels] inputs, labels = data[0].to(device), data[1].to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() pred = outputs.cpu().detach().argmax(dim=1, keepdim=True) correct = pred.eq(labels.cpu().view_as(pred)).sum().item() accuracy = correct / float(len(labels)) losses.append(loss.item()) accuracies.append(accuracy) steps += len(labels) if verbose and len(losses) > 5: print( \"[%d, %5d] loss: %.5f correct: %.5f\" % (epoch + 1, i + 1, np.mean(losses), np.mean(accuracies)) ) running_loss = 0.0 if steps > max_steps: break if steps > max_steps: break print(\"Finished Training\")","title":"Train resnet50 wids"},{"location":"wds-notes/","text":"%matplotlib inline import matplotlib.pyplot as plt import torch.utils.data import torch.nn from random import randrange import os os.environ[\"WDS_VERBOSE_CACHE\"] = \"1\" os.environ[\"GOPEN_VERBOSE\"] = \"0\" The WebDataset Format WebDataset format files are tar files, with two conventions: within each tar file, files that belong together and make up a training sample share the same basename when stripped of all filename extensions the shards of a tar file are numbered like something-000000.tar to something-012345.tar , usually specified using brace notation something-{000000..012345}.tar WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. WebDataset can also read concatenated MsgPack and CBORs sources. The WebDataset representation allows writing purely sequential I/O pipelines for large scale deep learning. This is important for achieving high I/O rates from local storage (3x-10x for local drives compared to random access) and for using object stores and cloud storage for training. The WebDataset format represents images, movies, audio, etc. in their native file formats, making the creation of WebDataset format data as easy as just creating a tar archive. Because of the way data is aligned, WebDataset works well with block deduplication as well and aligns data on predictable boundaries. Standard tools can be used for accessing and processing WebDataset-format files. bucket = \"https://storage.googleapis.com/webdataset/testdata/\" dataset = \"publaynet-train-{000000..000009}.tar\" url = bucket + dataset !curl -s {url} | tar tf - | sed 10q PMC4991227_00003.json PMC4991227_00003.png PMC4537884_00002.json PMC4537884_00002.png PMC4323233_00003.json PMC4323233_00003.png PMC5429906_00004.json PMC5429906_00004.png PMC5592712_00002.json PMC5592712_00002.png tar: stdout: write error Note that in these .tar files, we have pairs of .json and .png files; each such pair makes up a training sample. WebDataset Libraries There are several libraries supporting the WebDataset format: webdataset for Python3 (includes the wids library), this repository Webdataset.jl a Julia implementation tarp , a Golang implementation and command line tool Ray Data sources and sinks The webdataset library can be used with PyTorch, Tensorflow, and Jax. The webdataset Library The webdataset library is an implementation of PyTorch IterableDataset (or a mock implementation thereof if you aren't using PyTorch). It implements as form of stream processing. Some of its features are: large scale parallel data access through sharding high performance disk I/O due to purely sequential reads latency insensitive due to big fat pipes no local storage required instant startup for training jobs only requires reading from file descriptors/network streams, no special APIs its API encourages high performance I/O pipelines scalable from tiny desktop datasets to petascale datasets provides local caching if desired requires no dataset metadata; any collection of shards can be read and used instantly The main limitations people run into are related to the fact that IterableDataset is less commonly used in PyTorch and some existing code may not support it as well, and that achieving an exactly balanced number of training samples across many compute nodes for a fixed epoch size is tricky; for multinode training, webdataset is usually used with shard resampling. There are two interfaces, the concise \"fluid\" interface and a longer \"pipeline\" interface. We'll show examples using the fluid interface, which is usually what you want. import webdataset as wds pil_dataset = wds.WebDataset(url).shuffle(1000).decode(\"pil\").to_tuple(\"png\", \"json\") The resulting datasets are standard PyTorch IterableDataset instances. isinstance(pil_dataset, torch.utils.data.IterableDataset) True for image, json in pil_dataset: break plt.imshow(image) <matplotlib.image.AxesImage at 0x7f6e34732dd0> We can add onto the existing pipeline for augmentation and data preparation. import torchvision.transforms as transforms from PIL import Image preproc = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), lambda x: 1-x, ]) def preprocess(sample): image, json = sample try: label = json[\"annotations\"][0][\"category_id\"] except: label = 0 return preproc(image), label dataset = pil_dataset.map(preprocess) for image, label in dataset: break plt.imshow(image.numpy().transpose(1, 2, 0)) <matplotlib.image.AxesImage at 0x7f6e347a1c90> WebDataset is just an instance of a standard IterableDataset . It's a single-threaded way of iterating over a dataset. Since image decompression and data augmentation can be compute intensive, PyTorch usually uses the DataLoader class to parallelize data loading and preprocessing. WebDataset is fully compatible with the standard DataLoader . The wds.WebDataset fluid interface is just a convenient shorthand for writing down pipelines. The underlying pipeline is an instance of the wds.DataPipeline class, and you can construct data pipelines explicitly, similar to the way you use nn.Sequential inside models. dataset = wds.DataPipeline( wds.SimpleShardList(url), # at this point we have an iterator over all the shards wds.shuffle(100), # add wds.split_by_node here if you are using multiple nodes wds.split_by_worker, # at this point, we have an iterator over the shards assigned to each worker wds.tarfile_to_samples(), # this shuffles the samples in memory wds.shuffle(1000), # this decodes the images and json wds.decode(\"pil\"), wds.to_tuple(\"png\", \"json\"), wds.map(preprocess), wds.shuffle(1000), wds.batched(16) ) batch = next(iter(dataset)) batch[0].shape, batch[1].shape (torch.Size([16, 3, 224, 224]), (16,)) Here are a number of notebooks showing how to use WebDataset for image classification and LLM training: train-resnet50-wds -- simple, single GPU training from Imagenet train-resnet50-multiray-wds -- multinode training using webdataset generate-text-dataset -- initial dataset generation tesseract-wds -- shard-to-shard transformations, here for OCR running over large dataset Installation and Documentation $ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets Examples: (NB: some of these are for older versions of WebDataset, but the differences should be small) loading videos splitting raw videos into clips for training converting the Falling Things dataset Dependencies The WebDataset library only requires PyTorch, NumPy, and a small library called braceexpand . WebDataset loads a few additional libraries dynamically only when they are actually needed and only in the decoder: PIL/Pillow for image decoding torchvision , torchvideo , torchaudio for image/video/audio decoding msgpack for MessagePack decoding the curl command line tool for accessing HTTP servers the Google/Amazon/Azure command line tools for accessing cloud storage buckets Loading of one of these libraries is triggered by configuring a decoder that attempts to decode content in the given format and encountering a file in that format during decoding. (Eventually, the torch... dependencies will be refactored into those libraries.) Data Decoding Data decoding is a special kind of transformations of samples. You could simply write a decoding function like this: def my_sample_decoder(sample): result = dict(__key__=sample[\"__key__\"]) for key, value in sample.items(): if key == \"png\" or key.endswith(\".png\"): result[key] = mageio.imread(io.BytesIO(value)) elif ...: ... return result dataset = wds.Processor(wds.map, my_sample_decoder)(dataset) This gets tedious, though, and it also unnecessarily hardcodes the sample's keys into the processing pipeline. To help with this, there is a helper class that simplifies this kind of code. The primary use of Decoder is for decoding compressed image, video, and audio formats, as well as unzipping .gz files. Here is an example of automatically decoding .png images with imread and using the default torch_video and torch_audio decoders for video and audio: def my_png_decoder(key, value): if not key.endswith(\".png\"): return None assert isinstance(value, bytes) return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(my_png_decoder, wds.torch_video, wds.torch_audio)(dataset) You can use whatever criteria you like for deciding how to decode values in samples. When used with standard WebDataset format files, the keys are the full extensions of the file names inside a .tar file. For consistency, it's recommended that you primarily rely on the extensions (e.g., .png , .mp4 ) to decide which decoders to use. There is a special helper function that simplifies this: def my_decoder(value): return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(wds.handle_extension(\".png\", my_decoder))(dataset) \"Smaller\" Datasets and Desktop Computing WebDataset is an ideal solution for training on petascale datasets kept on high performance distributed data stores like AIStore, AWS/S3, and Google Cloud. Compared to data center GPU servers, desktop machines have much slower network connections, but training jobs on desktop machines often also use much smaller datasets. WebDataset also is very useful for such smaller datasets, and it can easily be used for developing and testing on small datasets and then scaling up to large datasets by simply using more shards. Here are different usage scenarios: desktop deep learning, smaller datasets copy all shards to local disk manually use automatic shard caching prototyping, development, testing of jobs for large scale training copy a small subset of shards to local disk use automatic shard caching with a small subrange of shards cloud training against cloud buckets use WebDataset directly with remote URLs on premises training with high performance store (e.g., AIStore) and fast networks use WebDataset directly with remote URLs on premises training with slower object stores and/or slower networks use automatic shard caching Location Independence, Caching, Etc. WebDataset makes it easy to use a single specification for your datasets and run your code without change in different environments. Loadable Dataset Specifications If you write your input pipelines such that they are defined by a dataset specification in some language, you can most easily retarget your training pipelines to different datasets. You can do this either by dynamically loading the Python code that constructs the pipeline or by using a YAML/JSON dataset specification. A YAML dataset specification looks like this: dataset: - shards: gs://nvdata-ocropus-tess/ia1-{000000..000033}.tar scaleprob: 0.3 - shards: gs://nvdata-ocropus-tess/cdipsub-{000000..000022}.tar scale: [1.0, 3.0] - shards: gs://nvdata-ocropus-tess/gsub-{000000..000167}.tar scale: [0.4, 1.0] - shards: gs://nvdata-ocropus-tess/bin-gsub-{000000..000167}.tar extensions: nrm.jpg scale: [0.3, 1.0] - shards: gs://nvdata-ocropus/rendered.tar scaleprob: 1.0 Note that datasets can be composed from different shard collections, mixed in different proportions. The dataset specification reader will be integrated in the next minor version update. AIStore Proxy If you want to use an AISTore server as a cache, you can tell any WebDataset pipeline to replace direct accesses to your URLs to proxied accesses via the AIStore server. To do that, you need to set a couple of environment variables. export AIS_ENDPOINT=http://nix:51080 export USE_AIS_FOR=\"gs\" Now, any accesses to Google Cloud Storage ( gs:// urls) will be routed through the AIS server. URL Rewriting You can rewrite URLs using regular expressions via an environment variable; the syntax is WDS_REWRITE=regex=regex;regex=regex . For example, to replace gs:// accesses with local file accesses, use export WDS_REWRITE=\"gs://=/shared/data/\" To access Google cloud data via ssh, you might use something like: export WDS_REWRITE=\"gs://=pipe:ssh proxyhost gsutil cat \" Use the Caching Mechanism If you use the built-in caching mechanism, you can simply download shards to a local directory and specify that directory as the cache directory. The shards in that directory will override the shards that are being downloaded. Shards in the cache are mapped based on the pathname and file name of your shard names. Direct Copying of Shards Let's take the OpenImages dataset as an example; it's half a terabyte large. For development and testing, you may not want to download the entire dataset, but you may also not want to use the dataset remotely. With WebDataset, you can just download a small number of shards and use them during development. !curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar > /tmp/openimages-train-000000.tar dataset = wds.DataPipeline( wds.SimpleShardList(\"/tmp/openimages-train-000000.tar\"), wds.tarfile_to_samples(), ) repr(next(iter(dataset)))[:200] \"{'__key__': 'e39871fd9fd74f55', '__url__': '/tmp/openimages-train-000000.tar', 'jpg': b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x01\\\\x01:\\\\x01:\\\\x00\\\\x00\\\\xff\\\\xdb\\\\x00C\\\\x00\\\\x06\\\\x04\\\\x05\\\\x06\\\\x05\\\\x04\\\\x06\\\\x06\\\\\" Note that the WebDataset class works the same way on local files as it does on remote files. Furthermore, unlike other kinds of dataset formats and archive formats, downloaded datasets are immediately useful and don't need to be unpacked. Creating a WebDataset Using tar Since WebDatasets are just regular tar files, you can usually create them by just using the tar command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with $ tar --sort=name -cf dataset.tar dataset/ If your dataset has some other directory layout, you may need a different file name in the archive from the name on disk. You can use the --transform argument to GNU tar to transform file names. You can also use the -T argument to read the files from a text file and embed other options in that text file. The tarp create Command The tarp command is a little utility for manipulating tar archives. Its create subcommand makes it particularly simple to construct tar archives from files. The tarp create command takes a recipe for building a tar archive that contains lines of the form: archive-name-1 source-name-1 archive-name-2 source-name-2 ... The source name can either be a file, \"text:something\", or \"pipe:something\". Programmatically in Python You can also create a WebDataset with library functions in this library: webdataset.TarWriter takes dictionaries containing key value pairs and writes them to disk webdataset.ShardWriter takes dictionaries containing key value pairs and writes them to disk as a series of shards Here is a quick way of converting an existing dataset into a WebDataset; this will store all tensors as Python pickles: sink = wds.TarWriter(\"dest.tar\") dataset = open_my_dataset() for index, (input, output) in dataset: sink.write({ \"__key__\": \"sample%06d\" % index, \"input.pyd\": input, \"output.pyd\": output, }) sink.close() Storing data as Python pickles allows most common Python datatypes to be stored, it is lossless, and the format is fast to decode. However, it is uncompressed and cannot be read by non-Python programs. It's often better to choose other storage formats, e.g., taking advantage of common image compression formats. If you know that the input is an image and the output is an integer class, you can also write something like this: sink = wds.TarWriter(\"dest.tar\") dataset = open_my_dataset() for index, (input, output) in dataset: assert input.ndim == 3 and input.shape[2] == 3 assert input.dtype = np.float32 and np.amin(input) >= 0 and np.amax(input) <= 1 assert type(output) == int sink.write({ \"__key__\": \"sample%06d\" % index, \"input.jpg\": input, \"output.cls\": output, }) sink.close() The assert statements in that loop are not necessary, but they document and illustrate the expectations for this particular dataset. Generally, the \".jpg\" encoder can actually encode a wide variety of array types as images. The \".cls\" encoder always requires an integer for encoding. Here is how you can use TarWriter for writing a dataset without using an encoder: sink = wds.TarWriter(\"dest.tar\", encoder=False) for basename in basenames: with open(f\"{basename}.png\", \"rb\") as stream): image = stream.read() cls = lookup_cls(basename) sample = { \"__key__\": basename, \"input.png\": image, \"target.cls\": cls } sink.write(sample) sink.close() Since no encoder is used, if you want to be able to read this data with the default decoder, image must contain a byte string corresponding to a PNG image (as indicated by the \".png\" extension on its dictionary key), and cls must contain an integer encoded in ASCII (as indicated by the \".cls\" extension on its dictionary key). Writing Filters and Offline Augmentation Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels. from torchvision import transforms from itertools import islice def extract_class(data): # mock implementation return 0 def preproc(image): image = transforms.ToTensor()(image) # more preprocessing here return image def augment_wds(input, output, maxcount=999999999): src = wds.DataPipeline( wds.SimpleShardList(input), wds.tarfile_to_samples(), wds.decode(\"pil\"), wds.to_tuple(\"__key__\", \"jpg;png\", \"json\"), wds.map_tuple(None, preproc, None), ) with wds.TarWriter(output) as dst: for key, image, data in islice(src, 0, maxcount): print(key) image = image.numpy().transpose(1, 2, 0) image -= np.amin(image) image /= np.amax(image) sample = { \"__key__\": key, \"png\": image, \"cls\": extract_class(data) } dst.write(sample) Now run the augmentation pipeline: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" augment_wds(url, \"_temp.tar\", maxcount=5) e39871fd9fd74f55 f18b91585c4d3f3e ede6e66b2fb59aab ed600d57fcee4f94 ff47e649b23f446d To verify that things worked correctly, let's look at the output file: %%bash tar tf _temp.tar e39871fd9fd74f55.cls e39871fd9fd74f55.png f18b91585c4d3f3e.cls f18b91585c4d3f3e.png ede6e66b2fb59aab.cls ede6e66b2fb59aab.png ed600d57fcee4f94.cls ed600d57fcee4f94.png ff47e649b23f446d.cls ff47e649b23f446d.png If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system. For example, using Dask, you could process all 554 shards in parallel using code like this: shards = braceexpand.braceexpand(\"{000000..000554}\") inputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards] outputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards] results = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)] dask.compute(*results) Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker. For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes Job template, or using a workflow engine like Argo. Whether you prefer WebDataset or Dataset is a matter of style. Syntax for URL Sources The SimpleShardList and ResampledShards take either a string or a list of URLs as an argument. If it is given a string, the string is expanded using the braceexpand library. So, the following are equivalent: ShardList(\"dataset-{000..001}.tar\") ShardList([\"dataset-000.tar\", \"dataset-001.tar\"]) The url strings in a shard list are handled by default by the webdataset.url_opener filter. It recognizes three simple kinds of strings: \"-\", \"/path/to/file\", and \"pipe:command\": the string \"-\", referring to stdin a UNIX path, opened as a regular file a URL-like string with the schema \"pipe:\"; such URLs are opened with subprocess.Popen . For example: pipe:curl -s -L http://server/file accesses a file via HTTP pipe:gsutil cat gs://bucket/file accesses a file on GCS pipe:az cp --container bucket --name file --file /dev/stdout accesses a file on Azure pipe:ssh host cat file accesses a file via ssh It might seem at first glance to be \"more efficient\" to use built-in Python libraries for accessing object stores rather than subprocesses, but efficient object store access from Python really requires spawning a separate process anyway, so this approach to accessing object stores is not only convenient, it also is as efficient as we can make it in Python. Length Properties WebDataset instances are subclasses of IterableDataset . These instances are not supposed to have a __len__ method, and some code actually tests for that. If you want to have a length property on your dataset, use the with_length(n) method with whatever length you would like to set. If you want to change the size of the epoch, i.e., if you want to force the iterator to quit after a given number of samples or batches, use the with_epoch method. You can combine both methods; use with_length last. Tar Header Overhead Tar imposes a 512 byte overhead for each file stored in the archive. For most applications, this is not an issue because images and other content tends to be much larger. If you have datasets that contain large amounts of small files (e.g., text-only training, etc.), this overhead may become significant. In that case, you have several options: store some or all of your sample in JSON, MsgPack, or CBOR format gzip-compress your tar file (use .tgz instead of .tar); WebDatset will automatically decompress pre-batch the data (not recommended) Both of the first options are very simple. To store your entire sample in MsgPack format, do something like this: # Writing ... construct sample ... sample = dict(mp=sample) writer.write(sample) # Reading dataset = ... initial construction ... dataset = dataset.map(sample: sample[\"mp\"]) ... use sample as usual ... Related Libraries and Software The AIStore server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs. The tarproc utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and xargs -like functionality. The tensorcom library provides fast three-tiered I/O; it can be inserted between AIStore and WebDataset to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available. You can find the full PyTorch ImageNet sample code converted to WebDataset at tmbdev/pytorch-imagenet-wds","title":"Wds notes"},{"location":"wds-notes/#the-webdataset-format","text":"WebDataset format files are tar files, with two conventions: within each tar file, files that belong together and make up a training sample share the same basename when stripped of all filename extensions the shards of a tar file are numbered like something-000000.tar to something-012345.tar , usually specified using brace notation something-{000000..012345}.tar WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. WebDataset can also read concatenated MsgPack and CBORs sources. The WebDataset representation allows writing purely sequential I/O pipelines for large scale deep learning. This is important for achieving high I/O rates from local storage (3x-10x for local drives compared to random access) and for using object stores and cloud storage for training. The WebDataset format represents images, movies, audio, etc. in their native file formats, making the creation of WebDataset format data as easy as just creating a tar archive. Because of the way data is aligned, WebDataset works well with block deduplication as well and aligns data on predictable boundaries. Standard tools can be used for accessing and processing WebDataset-format files. bucket = \"https://storage.googleapis.com/webdataset/testdata/\" dataset = \"publaynet-train-{000000..000009}.tar\" url = bucket + dataset !curl -s {url} | tar tf - | sed 10q PMC4991227_00003.json PMC4991227_00003.png PMC4537884_00002.json PMC4537884_00002.png PMC4323233_00003.json PMC4323233_00003.png PMC5429906_00004.json PMC5429906_00004.png PMC5592712_00002.json PMC5592712_00002.png tar: stdout: write error Note that in these .tar files, we have pairs of .json and .png files; each such pair makes up a training sample.","title":"The WebDataset Format"},{"location":"wds-notes/#webdataset-libraries","text":"There are several libraries supporting the WebDataset format: webdataset for Python3 (includes the wids library), this repository Webdataset.jl a Julia implementation tarp , a Golang implementation and command line tool Ray Data sources and sinks The webdataset library can be used with PyTorch, Tensorflow, and Jax.","title":"WebDataset Libraries"},{"location":"wds-notes/#the-webdataset-library","text":"The webdataset library is an implementation of PyTorch IterableDataset (or a mock implementation thereof if you aren't using PyTorch). It implements as form of stream processing. Some of its features are: large scale parallel data access through sharding high performance disk I/O due to purely sequential reads latency insensitive due to big fat pipes no local storage required instant startup for training jobs only requires reading from file descriptors/network streams, no special APIs its API encourages high performance I/O pipelines scalable from tiny desktop datasets to petascale datasets provides local caching if desired requires no dataset metadata; any collection of shards can be read and used instantly The main limitations people run into are related to the fact that IterableDataset is less commonly used in PyTorch and some existing code may not support it as well, and that achieving an exactly balanced number of training samples across many compute nodes for a fixed epoch size is tricky; for multinode training, webdataset is usually used with shard resampling. There are two interfaces, the concise \"fluid\" interface and a longer \"pipeline\" interface. We'll show examples using the fluid interface, which is usually what you want. import webdataset as wds pil_dataset = wds.WebDataset(url).shuffle(1000).decode(\"pil\").to_tuple(\"png\", \"json\") The resulting datasets are standard PyTorch IterableDataset instances. isinstance(pil_dataset, torch.utils.data.IterableDataset) True for image, json in pil_dataset: break plt.imshow(image) <matplotlib.image.AxesImage at 0x7f6e34732dd0> We can add onto the existing pipeline for augmentation and data preparation. import torchvision.transforms as transforms from PIL import Image preproc = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), lambda x: 1-x, ]) def preprocess(sample): image, json = sample try: label = json[\"annotations\"][0][\"category_id\"] except: label = 0 return preproc(image), label dataset = pil_dataset.map(preprocess) for image, label in dataset: break plt.imshow(image.numpy().transpose(1, 2, 0)) <matplotlib.image.AxesImage at 0x7f6e347a1c90> WebDataset is just an instance of a standard IterableDataset . It's a single-threaded way of iterating over a dataset. Since image decompression and data augmentation can be compute intensive, PyTorch usually uses the DataLoader class to parallelize data loading and preprocessing. WebDataset is fully compatible with the standard DataLoader . The wds.WebDataset fluid interface is just a convenient shorthand for writing down pipelines. The underlying pipeline is an instance of the wds.DataPipeline class, and you can construct data pipelines explicitly, similar to the way you use nn.Sequential inside models. dataset = wds.DataPipeline( wds.SimpleShardList(url), # at this point we have an iterator over all the shards wds.shuffle(100), # add wds.split_by_node here if you are using multiple nodes wds.split_by_worker, # at this point, we have an iterator over the shards assigned to each worker wds.tarfile_to_samples(), # this shuffles the samples in memory wds.shuffle(1000), # this decodes the images and json wds.decode(\"pil\"), wds.to_tuple(\"png\", \"json\"), wds.map(preprocess), wds.shuffle(1000), wds.batched(16) ) batch = next(iter(dataset)) batch[0].shape, batch[1].shape (torch.Size([16, 3, 224, 224]), (16,)) Here are a number of notebooks showing how to use WebDataset for image classification and LLM training: train-resnet50-wds -- simple, single GPU training from Imagenet train-resnet50-multiray-wds -- multinode training using webdataset generate-text-dataset -- initial dataset generation tesseract-wds -- shard-to-shard transformations, here for OCR running over large dataset","title":"The webdataset Library"},{"location":"wds-notes/#installation-and-documentation","text":"$ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets Examples: (NB: some of these are for older versions of WebDataset, but the differences should be small) loading videos splitting raw videos into clips for training converting the Falling Things dataset","title":"Installation and Documentation"},{"location":"wds-notes/#dependencies","text":"The WebDataset library only requires PyTorch, NumPy, and a small library called braceexpand . WebDataset loads a few additional libraries dynamically only when they are actually needed and only in the decoder: PIL/Pillow for image decoding torchvision , torchvideo , torchaudio for image/video/audio decoding msgpack for MessagePack decoding the curl command line tool for accessing HTTP servers the Google/Amazon/Azure command line tools for accessing cloud storage buckets Loading of one of these libraries is triggered by configuring a decoder that attempts to decode content in the given format and encountering a file in that format during decoding. (Eventually, the torch... dependencies will be refactored into those libraries.)","title":"Dependencies"},{"location":"wds-notes/#data-decoding","text":"Data decoding is a special kind of transformations of samples. You could simply write a decoding function like this: def my_sample_decoder(sample): result = dict(__key__=sample[\"__key__\"]) for key, value in sample.items(): if key == \"png\" or key.endswith(\".png\"): result[key] = mageio.imread(io.BytesIO(value)) elif ...: ... return result dataset = wds.Processor(wds.map, my_sample_decoder)(dataset) This gets tedious, though, and it also unnecessarily hardcodes the sample's keys into the processing pipeline. To help with this, there is a helper class that simplifies this kind of code. The primary use of Decoder is for decoding compressed image, video, and audio formats, as well as unzipping .gz files. Here is an example of automatically decoding .png images with imread and using the default torch_video and torch_audio decoders for video and audio: def my_png_decoder(key, value): if not key.endswith(\".png\"): return None assert isinstance(value, bytes) return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(my_png_decoder, wds.torch_video, wds.torch_audio)(dataset) You can use whatever criteria you like for deciding how to decode values in samples. When used with standard WebDataset format files, the keys are the full extensions of the file names inside a .tar file. For consistency, it's recommended that you primarily rely on the extensions (e.g., .png , .mp4 ) to decide which decoders to use. There is a special helper function that simplifies this: def my_decoder(value): return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(wds.handle_extension(\".png\", my_decoder))(dataset)","title":"Data Decoding"},{"location":"wds-notes/#smaller-datasets-and-desktop-computing","text":"WebDataset is an ideal solution for training on petascale datasets kept on high performance distributed data stores like AIStore, AWS/S3, and Google Cloud. Compared to data center GPU servers, desktop machines have much slower network connections, but training jobs on desktop machines often also use much smaller datasets. WebDataset also is very useful for such smaller datasets, and it can easily be used for developing and testing on small datasets and then scaling up to large datasets by simply using more shards. Here are different usage scenarios: desktop deep learning, smaller datasets copy all shards to local disk manually use automatic shard caching prototyping, development, testing of jobs for large scale training copy a small subset of shards to local disk use automatic shard caching with a small subrange of shards cloud training against cloud buckets use WebDataset directly with remote URLs on premises training with high performance store (e.g., AIStore) and fast networks use WebDataset directly with remote URLs on premises training with slower object stores and/or slower networks use automatic shard caching","title":"\"Smaller\" Datasets and Desktop Computing"},{"location":"wds-notes/#location-independence-caching-etc","text":"WebDataset makes it easy to use a single specification for your datasets and run your code without change in different environments.","title":"Location Independence, Caching, Etc."},{"location":"wds-notes/#loadable-dataset-specifications","text":"If you write your input pipelines such that they are defined by a dataset specification in some language, you can most easily retarget your training pipelines to different datasets. You can do this either by dynamically loading the Python code that constructs the pipeline or by using a YAML/JSON dataset specification. A YAML dataset specification looks like this: dataset: - shards: gs://nvdata-ocropus-tess/ia1-{000000..000033}.tar scaleprob: 0.3 - shards: gs://nvdata-ocropus-tess/cdipsub-{000000..000022}.tar scale: [1.0, 3.0] - shards: gs://nvdata-ocropus-tess/gsub-{000000..000167}.tar scale: [0.4, 1.0] - shards: gs://nvdata-ocropus-tess/bin-gsub-{000000..000167}.tar extensions: nrm.jpg scale: [0.3, 1.0] - shards: gs://nvdata-ocropus/rendered.tar scaleprob: 1.0 Note that datasets can be composed from different shard collections, mixed in different proportions. The dataset specification reader will be integrated in the next minor version update.","title":"Loadable Dataset Specifications"},{"location":"wds-notes/#aistore-proxy","text":"If you want to use an AISTore server as a cache, you can tell any WebDataset pipeline to replace direct accesses to your URLs to proxied accesses via the AIStore server. To do that, you need to set a couple of environment variables. export AIS_ENDPOINT=http://nix:51080 export USE_AIS_FOR=\"gs\" Now, any accesses to Google Cloud Storage ( gs:// urls) will be routed through the AIS server.","title":"AIStore Proxy"},{"location":"wds-notes/#url-rewriting","text":"You can rewrite URLs using regular expressions via an environment variable; the syntax is WDS_REWRITE=regex=regex;regex=regex . For example, to replace gs:// accesses with local file accesses, use export WDS_REWRITE=\"gs://=/shared/data/\" To access Google cloud data via ssh, you might use something like: export WDS_REWRITE=\"gs://=pipe:ssh proxyhost gsutil cat \"","title":"URL Rewriting"},{"location":"wds-notes/#use-the-caching-mechanism","text":"If you use the built-in caching mechanism, you can simply download shards to a local directory and specify that directory as the cache directory. The shards in that directory will override the shards that are being downloaded. Shards in the cache are mapped based on the pathname and file name of your shard names.","title":"Use the Caching Mechanism"},{"location":"wds-notes/#direct-copying-of-shards","text":"Let's take the OpenImages dataset as an example; it's half a terabyte large. For development and testing, you may not want to download the entire dataset, but you may also not want to use the dataset remotely. With WebDataset, you can just download a small number of shards and use them during development. !curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar > /tmp/openimages-train-000000.tar dataset = wds.DataPipeline( wds.SimpleShardList(\"/tmp/openimages-train-000000.tar\"), wds.tarfile_to_samples(), ) repr(next(iter(dataset)))[:200] \"{'__key__': 'e39871fd9fd74f55', '__url__': '/tmp/openimages-train-000000.tar', 'jpg': b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x01\\\\x01:\\\\x01:\\\\x00\\\\x00\\\\xff\\\\xdb\\\\x00C\\\\x00\\\\x06\\\\x04\\\\x05\\\\x06\\\\x05\\\\x04\\\\x06\\\\x06\\\\\" Note that the WebDataset class works the same way on local files as it does on remote files. Furthermore, unlike other kinds of dataset formats and archive formats, downloaded datasets are immediately useful and don't need to be unpacked.","title":"Direct Copying of Shards"},{"location":"wds-notes/#creating-a-webdataset","text":"","title":"Creating a WebDataset"},{"location":"wds-notes/#using-tar","text":"Since WebDatasets are just regular tar files, you can usually create them by just using the tar command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with $ tar --sort=name -cf dataset.tar dataset/ If your dataset has some other directory layout, you may need a different file name in the archive from the name on disk. You can use the --transform argument to GNU tar to transform file names. You can also use the -T argument to read the files from a text file and embed other options in that text file.","title":"Using tar"},{"location":"wds-notes/#the-tarp-create-command","text":"The tarp command is a little utility for manipulating tar archives. Its create subcommand makes it particularly simple to construct tar archives from files. The tarp create command takes a recipe for building a tar archive that contains lines of the form: archive-name-1 source-name-1 archive-name-2 source-name-2 ... The source name can either be a file, \"text:something\", or \"pipe:something\".","title":"The tarp create Command"},{"location":"wds-notes/#programmatically-in-python","text":"You can also create a WebDataset with library functions in this library: webdataset.TarWriter takes dictionaries containing key value pairs and writes them to disk webdataset.ShardWriter takes dictionaries containing key value pairs and writes them to disk as a series of shards Here is a quick way of converting an existing dataset into a WebDataset; this will store all tensors as Python pickles: sink = wds.TarWriter(\"dest.tar\") dataset = open_my_dataset() for index, (input, output) in dataset: sink.write({ \"__key__\": \"sample%06d\" % index, \"input.pyd\": input, \"output.pyd\": output, }) sink.close() Storing data as Python pickles allows most common Python datatypes to be stored, it is lossless, and the format is fast to decode. However, it is uncompressed and cannot be read by non-Python programs. It's often better to choose other storage formats, e.g., taking advantage of common image compression formats. If you know that the input is an image and the output is an integer class, you can also write something like this: sink = wds.TarWriter(\"dest.tar\") dataset = open_my_dataset() for index, (input, output) in dataset: assert input.ndim == 3 and input.shape[2] == 3 assert input.dtype = np.float32 and np.amin(input) >= 0 and np.amax(input) <= 1 assert type(output) == int sink.write({ \"__key__\": \"sample%06d\" % index, \"input.jpg\": input, \"output.cls\": output, }) sink.close() The assert statements in that loop are not necessary, but they document and illustrate the expectations for this particular dataset. Generally, the \".jpg\" encoder can actually encode a wide variety of array types as images. The \".cls\" encoder always requires an integer for encoding. Here is how you can use TarWriter for writing a dataset without using an encoder: sink = wds.TarWriter(\"dest.tar\", encoder=False) for basename in basenames: with open(f\"{basename}.png\", \"rb\") as stream): image = stream.read() cls = lookup_cls(basename) sample = { \"__key__\": basename, \"input.png\": image, \"target.cls\": cls } sink.write(sample) sink.close() Since no encoder is used, if you want to be able to read this data with the default decoder, image must contain a byte string corresponding to a PNG image (as indicated by the \".png\" extension on its dictionary key), and cls must contain an integer encoded in ASCII (as indicated by the \".cls\" extension on its dictionary key).","title":"Programmatically in Python"},{"location":"wds-notes/#writing-filters-and-offline-augmentation","text":"Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels. from torchvision import transforms from itertools import islice def extract_class(data): # mock implementation return 0 def preproc(image): image = transforms.ToTensor()(image) # more preprocessing here return image def augment_wds(input, output, maxcount=999999999): src = wds.DataPipeline( wds.SimpleShardList(input), wds.tarfile_to_samples(), wds.decode(\"pil\"), wds.to_tuple(\"__key__\", \"jpg;png\", \"json\"), wds.map_tuple(None, preproc, None), ) with wds.TarWriter(output) as dst: for key, image, data in islice(src, 0, maxcount): print(key) image = image.numpy().transpose(1, 2, 0) image -= np.amin(image) image /= np.amax(image) sample = { \"__key__\": key, \"png\": image, \"cls\": extract_class(data) } dst.write(sample) Now run the augmentation pipeline: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" augment_wds(url, \"_temp.tar\", maxcount=5) e39871fd9fd74f55 f18b91585c4d3f3e ede6e66b2fb59aab ed600d57fcee4f94 ff47e649b23f446d To verify that things worked correctly, let's look at the output file: %%bash tar tf _temp.tar e39871fd9fd74f55.cls e39871fd9fd74f55.png f18b91585c4d3f3e.cls f18b91585c4d3f3e.png ede6e66b2fb59aab.cls ede6e66b2fb59aab.png ed600d57fcee4f94.cls ed600d57fcee4f94.png ff47e649b23f446d.cls ff47e649b23f446d.png If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system. For example, using Dask, you could process all 554 shards in parallel using code like this: shards = braceexpand.braceexpand(\"{000000..000554}\") inputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards] outputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards] results = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)] dask.compute(*results) Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker. For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes Job template, or using a workflow engine like Argo. Whether you prefer WebDataset or Dataset is a matter of style.","title":"Writing Filters and Offline Augmentation"},{"location":"wds-notes/#syntax-for-url-sources","text":"The SimpleShardList and ResampledShards take either a string or a list of URLs as an argument. If it is given a string, the string is expanded using the braceexpand library. So, the following are equivalent: ShardList(\"dataset-{000..001}.tar\") ShardList([\"dataset-000.tar\", \"dataset-001.tar\"]) The url strings in a shard list are handled by default by the webdataset.url_opener filter. It recognizes three simple kinds of strings: \"-\", \"/path/to/file\", and \"pipe:command\": the string \"-\", referring to stdin a UNIX path, opened as a regular file a URL-like string with the schema \"pipe:\"; such URLs are opened with subprocess.Popen . For example: pipe:curl -s -L http://server/file accesses a file via HTTP pipe:gsutil cat gs://bucket/file accesses a file on GCS pipe:az cp --container bucket --name file --file /dev/stdout accesses a file on Azure pipe:ssh host cat file accesses a file via ssh It might seem at first glance to be \"more efficient\" to use built-in Python libraries for accessing object stores rather than subprocesses, but efficient object store access from Python really requires spawning a separate process anyway, so this approach to accessing object stores is not only convenient, it also is as efficient as we can make it in Python.","title":"Syntax for URL Sources"},{"location":"wds-notes/#length-properties","text":"WebDataset instances are subclasses of IterableDataset . These instances are not supposed to have a __len__ method, and some code actually tests for that. If you want to have a length property on your dataset, use the with_length(n) method with whatever length you would like to set. If you want to change the size of the epoch, i.e., if you want to force the iterator to quit after a given number of samples or batches, use the with_epoch method. You can combine both methods; use with_length last.","title":"Length Properties"},{"location":"wds-notes/#tar-header-overhead","text":"Tar imposes a 512 byte overhead for each file stored in the archive. For most applications, this is not an issue because images and other content tends to be much larger. If you have datasets that contain large amounts of small files (e.g., text-only training, etc.), this overhead may become significant. In that case, you have several options: store some or all of your sample in JSON, MsgPack, or CBOR format gzip-compress your tar file (use .tgz instead of .tar); WebDatset will automatically decompress pre-batch the data (not recommended) Both of the first options are very simple. To store your entire sample in MsgPack format, do something like this: # Writing ... construct sample ... sample = dict(mp=sample) writer.write(sample) # Reading dataset = ... initial construction ... dataset = dataset.map(sample: sample[\"mp\"]) ... use sample as usual ...","title":"Tar Header Overhead"},{"location":"wds-notes/#related-libraries-and-software","text":"The AIStore server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs. The tarproc utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and xargs -like functionality. The tensorcom library provides fast three-tiered I/O; it can be inserted between AIStore and WebDataset to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available. You can find the full PyTorch ImageNet sample code converted to WebDataset at tmbdev/pytorch-imagenet-wds","title":"Related Libraries and Software"},{"location":"webdataset/","text":"WebDataset API Fluid Interfaces The FluidInterface class provides a way to create fluent interfaces for chaining operations on datasets. Most operations are contained in the FluidInterface mixin class. with_epoch sets the epoch size (number of samples per epoch), effectively an itertools.islice over the dataset. webdataset.WebDataset Bases: DataPipeline , FluidInterface Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Parameters: urls \u2013 The source URLs or specifications for the dataset. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. mode \u2013 The mode of operation. Defaults to None. resampled \u2013 Whether to use resampled mode. Defaults to False. repeat \u2013 Whether to repeat the dataset. Defaults to False. shardshuffle \u2013 The number of shards to shuffle, or None. Defaults to None. cache_size \u2013 The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir \u2013 The directory to use for caching. Defaults to None. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle \u2013 Whether to use deterministic shuffling. Defaults to False. nodesplitter \u2013 Function to split data by node. Defaults to single_node_only. workersplitter \u2013 Function to split data by worker. Defaults to split_by_worker. select_files \u2013 Function to select files from tar archives. Defaults to None. rename_files \u2013 Function to rename files from tar archives. Defaults to None. empty_check \u2013 Whether to check for empty datasets. Defaults to True. verbose \u2013 Whether to print verbose output. Defaults to False. seed \u2013 Random seed for shuffling. Defaults to None. Raises: ValueError \u2013 If the cache directory does not exist or if the URL type is not supported. Source code in webdataset/compat.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 class WebDataset ( DataPipeline , FluidInterface ): \"\"\"Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Args: urls: The source URLs or specifications for the dataset. handler: Function to handle exceptions. Defaults to reraise_exception. mode: The mode of operation. Defaults to None. resampled: Whether to use resampled mode. Defaults to False. repeat: Whether to repeat the dataset. Defaults to False. shardshuffle: The number of shards to shuffle, or None. Defaults to None. cache_size: The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir: The directory to use for caching. Defaults to None. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle: Whether to use deterministic shuffling. Defaults to False. nodesplitter: Function to split data by node. Defaults to single_node_only. workersplitter: Function to split data by worker. Defaults to split_by_worker. select_files: Function to select files from tar archives. Defaults to None. rename_files: Function to rename files from tar archives. Defaults to None. empty_check: Whether to check for empty datasets. Defaults to True. verbose: Whether to print verbose output. Defaults to False. seed: Random seed for shuffling. Defaults to None. Raises: ValueError: If the cache directory does not exist or if the URL type is not supported. \"\"\" def __init__ ( self , urls , handler = reraise_exception , mode = None , resampled = False , repeat = False , shardshuffle = None , cache_size =- 1 , cache_dir = None , url_to_name = cache . pipe_cleaner , detshuffle = False , nodesplitter = shardlists . single_node_only , workersplitter = shardlists . split_by_worker , select_files = None , rename_files = None , empty_check = True , verbose = False , seed = None , ): super () . __init__ () if resampled : mode = \"resampled\" if mode == \"resampled\" and shardshuffle not in ( False , None ): warnings . warn ( \"WebDataset(shardshuffle=...) is ignored for resampled datasets\" ) elif shardshuffle is None : warnings . warn ( \"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\" ) if shardshuffle is True : warnings . warn ( \"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\" ) shardshuffle = 100 args = SimpleNamespace ( ** locals ()) self . seed = seed or os . environ . get ( \"WDS_SEED\" , random . randint ( 0 , 1000000 )) self . update_cache_info ( args ) # first, we add a generator for the urls to used # this generates a stream of dict(url=...) self . create_url_iterator ( args ) # split by node (for distributed processing) if nodesplitter is not None : self . append ( nodesplitter ) # split by worker (for DataLoader) if workersplitter : self . append ( shardlists . split_by_worker ) # add a shard shuffler if args . shardshuffle is not None : if args . detshuffle : self . append ( filters . detshuffle ( args . shardshuffle , seed = args . seed )) else : self . append ( filters . shuffle ( args . shardshuffle , seed = args . seed )) # next, we select a URL opener, either with or without caching # this generates a stream of dict(url=..., stream=...) if cache_dir is None or cache_size == 0 : opener = cache . StreamingOpen ( handler = handler ) else : opener = cache . FileCache ( cache_dir = cache_dir , cache_size = cache_size , handler = handler ) self . append ( opener ) # now we need to open each stream and read the tar files contained in it # this generates a stream of dict(fname=..., data=...) objects expander = pipelinefilter ( tar_file_expander ) self . append ( expander ( handler = handler , select_files = select_files , rename_files = rename_files ) ) # finally, the files need to be groups into samples # this generates a stream of dict(__key__=..., ...=...) objects grouper = pipelinefilter ( group_by_keys ) self . append ( grouper ( handler = handler )) # check for empty datasets if empty_check : self . append ( check_empty ) def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" ) def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardlist ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" ) def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close () __enter__ () Enter the runtime context for the WebDataset. Returns: self \u2013 The WebDataset instance. Source code in webdataset/compat.py 503 504 505 506 507 508 509 def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self __exit__ ( * args ) Exit the runtime context for the WebDataset. Parameters: *args \u2013 Exception type, value, and traceback if an exception occurred. Source code in webdataset/compat.py 511 512 513 514 515 516 517 def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close () create_url_iterator ( args ) Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the URL type is not supported or implemented. Source code in webdataset/compat.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardlist ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" ) update_cache_info ( args ) Update cache information based on arguments and environment variables. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the specified cache directory does not exist. Source code in webdataset/compat.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" ) webdataset.WebLoader Bases: DataPipeline , FluidInterface A wrapper for DataLoader that adds a fluid interface. Source code in webdataset/compat.py 528 529 530 531 class WebLoader ( DataPipeline , FluidInterface ): \"\"\"A wrapper for DataLoader that adds a fluid interface.\"\"\" def __init__ ( self , * args , ** kw ): super () . __init__ ( DataLoader ( * args , ** kw )) webdataset.FluidInterface Source code in webdataset/compat.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 class FluidInterface : def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) ) def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ()) def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None )) def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ()) def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile )) def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw )) def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler )) def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler ) def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw )) def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw )) def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw )) def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler )) def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args )) def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw )) def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p )) def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw )) def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw )) def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw )) def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ()) def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw )) batched ( batchsize , collation_fn = filters . default_collation_fn , partial = True ) Create batches of the given size. This method forwards to the filters.batched function. Parameters: batchsize ( int ) \u2013 Target batch size. collation_fn ( callable , default: default_collation_fn ) \u2013 Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial ( bool , default: True ) \u2013 Whether to return partial batches. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with batched filter. Source code in webdataset/compat.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) ) decode ( * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception ) Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Parameters: *args \u2013 Decoding functions or strings representing image handlers. pre ( callable , default: None ) \u2013 Pre-processing function. post ( callable , default: None ) \u2013 Post-processing function. only ( list , default: None ) \u2013 List of keys to decode. partial ( bool , default: False ) \u2013 Whether to allow partial decoding. Defaults to False. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with decode filter. Source code in webdataset/compat.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler ) extract_keys ( * args , ** kw ) Extract specific keys from samples. This method forwards to the filters.extract_keys function. Parameters: *args \u2013 Keys or patterns to extract. **kw \u2013 Additional keyword arguments for filters.extract_keys. Returns: FluidInterface \u2013 Updated pipeline with extract_keys filter. Source code in webdataset/compat.py 256 257 258 259 260 261 262 263 264 265 266 267 268 def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw )) listed ( batchsize , partial = True ) Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Parameters: batchsize ( int ) \u2013 Target list size. partial ( bool , default: True ) \u2013 Whether to return partial lists. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with listed filter. Source code in webdataset/compat.py 47 48 49 50 51 52 53 54 55 56 57 58 59 def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None )) lmdb_cached ( * args , ** kw ) Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Parameters: *args \u2013 Positional arguments for filters.LMDBCached. **kw \u2013 Keyword arguments for filters.LMDBCached. Returns: FluidInterface \u2013 Updated pipeline with LMDB caching. Source code in webdataset/compat.py 294 295 296 297 298 299 300 301 302 303 304 305 306 def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw )) log_keys ( logfile = None ) Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Parameters: logfile ( str , default: None ) \u2013 Path to the log file. If None, logging is disabled. Returns: FluidInterface \u2013 Updated pipeline with log_keys filter. Source code in webdataset/compat.py 71 72 73 74 75 76 77 78 79 80 81 82 def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile )) map ( f , handler = reraise_exception ) Apply a function to each sample in the stream. This method forwards to the filters.map function. Parameters: f ( callable ) \u2013 Function to apply to each sample. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map filter. Source code in webdataset/compat.py 101 102 103 104 105 106 107 108 109 110 111 112 113 def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler )) map_dict ( handler = reraise_exception , ** kw ) Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Parameters: handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. **kw \u2013 Mapping of keys to functions to apply. Returns: FluidInterface \u2013 Updated pipeline with map_dict filter. Source code in webdataset/compat.py 147 148 149 150 151 152 153 154 155 156 157 158 159 def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw )) map_tuple ( * args , handler = reraise_exception ) Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Parameters: *args \u2013 Functions to apply to each element of the tuple. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map_tuple filter. Source code in webdataset/compat.py 189 190 191 192 193 194 195 196 197 198 199 200 201 def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler )) mcached () Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface \u2013 Updated pipeline with memory caching. Source code in webdataset/compat.py 284 285 286 287 288 289 290 291 292 def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ()) rename ( ** kw ) Rename samples based on keyword arguments. This method forwards to the filters.rename function. Parameters: **kw \u2013 Mapping of old names to new names. Returns: FluidInterface \u2013 Updated pipeline with rename filter. Source code in webdataset/compat.py 216 217 218 219 220 221 222 223 224 225 226 227 def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw )) rename_keys ( * args , ** kw ) Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Parameters: *args \u2013 Positional arguments for filters.rename_keys. **kw \u2013 Keyword arguments for filters.rename_keys. Returns: FluidInterface \u2013 Updated pipeline with rename_keys filter. Source code in webdataset/compat.py 242 243 244 245 246 247 248 249 250 251 252 253 254 def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw )) rsample ( p = 0.5 ) Randomly subsample a stream of data. This method forwards to the filters.rsample function. Parameters: p ( float , default: 0.5 ) \u2013 Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface \u2013 Updated pipeline with rsample filter. Source code in webdataset/compat.py 229 230 231 232 233 234 235 236 237 238 239 240 def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p )) select ( predicate , ** kw ) Select samples based on a predicate. This method forwards to the filters.select function. Parameters: predicate ( callable ) \u2013 Function that returns True for samples to keep. **kw \u2013 Additional keyword arguments for filters.select. Returns: FluidInterface \u2013 Updated pipeline with select filter. Source code in webdataset/compat.py 161 162 163 164 165 166 167 168 169 170 171 172 173 def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw )) shuffle ( size , ** kw ) Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Parameters: size ( int ) \u2013 Buffer size for shuffling. **kw \u2013 Additional keyword arguments for filters.shuffle. Returns: FluidInterface \u2013 Updated pipeline with shuffle filter, or self if size < 1. Source code in webdataset/compat.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw )) slice ( * args ) Slice the data stream. This method forwards to the filters.slice function. Parameters: *args \u2013 Arguments for slicing (start, stop, step). Returns: FluidInterface \u2013 Updated pipeline with slice filter. Source code in webdataset/compat.py 203 204 205 206 207 208 209 210 211 212 213 214 def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args )) to_tuple ( * args , ** kw ) Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Parameters: *args \u2013 Keys to extract from the dict. **kw \u2013 Additional keyword arguments for filters.to_tuple. Returns: FluidInterface \u2013 Updated pipeline with to_tuple filter. Source code in webdataset/compat.py 175 176 177 178 179 180 181 182 183 184 185 186 187 def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw )) unbatched () Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface \u2013 Updated pipeline with unbatched filter. Source code in webdataset/compat.py 37 38 39 40 41 42 43 44 45 def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ()) unlisted () Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface \u2013 Updated pipeline with unlisted filter. Source code in webdataset/compat.py 61 62 63 64 65 66 67 68 69 def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ()) xdecode ( * args , ** kw ) Decode data based on file extensions. This method forwards to the filters.xdecode function. Parameters: *args \u2013 Positional arguments for filters.xdecode. **kw \u2013 Keyword arguments for filters.xdecode. Returns: FluidInterface \u2013 Updated pipeline with xdecode filter. Source code in webdataset/compat.py 270 271 272 273 274 275 276 277 278 279 280 281 282 def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw )) webdataset.with_epoch Bases: IterableDataset Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Parameters: dataset \u2013 The source IterableDataset. length ( int ) \u2013 Declared length of the dataset. Source code in webdataset/extradatasets.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class with_epoch ( IterableDataset ): \"\"\"Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Args: dataset: The source IterableDataset. length (int): Declared length of the dataset. \"\"\" def __init__ ( self , dataset , length ): super () . __init__ () self . length = length self . source = None def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None __getstate__ () Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict \u2013 A dictionary representing the pickled state of the dataset. Source code in webdataset/extradatasets.py 91 92 93 94 95 96 97 98 99 100 101 def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result invoke ( dataset ) Return an iterator over the dataset. This iterator returns as many samples as given by the length parameter. Parameters: dataset \u2013 The source dataset to iterate over. Yields: Sample \u2013 The next sample from the dataset. Source code in webdataset/extradatasets.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None Writing WebDatasets webdataset.ShardWriter Like TarWriter but splits into multiple shards. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. Defaults to 100000. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. Defaults to 3e9. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. Defaults to None. start_shard ( int , default: 0 ) \u2013 Starting shard number. Defaults to 0. verbose ( int , default: 1 ) \u2013 Verbosity level. Defaults to 1. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. Defaults to None. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 class ShardWriter : \"\"\"Like TarWriter but splits into multiple shards. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. Defaults to 100000. maxsize: Maximum size of each shard. Defaults to 3e9. post: Optional callable to be executed after each shard is written. Defaults to None. start_shard: Starting shard number. Defaults to 0. verbose: Verbosity level. Defaults to 1. opener: Optional callable to open output files. Defaults to None. **kw: Other options passed to TarWriter. \"\"\" def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream () def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close () __enter__ () Enter context. Returns: self \u2013 The ShardWriter object. Source code in webdataset/writer.py 593 594 595 596 597 598 599 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self __exit__ ( * args , ** kw ) Exit context. Source code in webdataset/writer.py 601 602 603 def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close () __init__ ( pattern , maxcount = 100000 , maxsize = 3000000000.0 , post = None , start_shard = 0 , verbose = 1 , opener = None , ** kw ) Create a ShardWriter. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. start_shard ( int , default: 0 ) \u2013 Starting shard number. verbose ( int , default: 1 ) \u2013 Verbosity level. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream () close () Close the stream. Source code in webdataset/writer.py 585 586 587 588 589 590 591 def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size finish () Finish all writing (use close instead). Source code in webdataset/writer.py 576 577 578 579 580 581 582 583 def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None next_stream () Close the current stream and move to the next. Source code in webdataset/writer.py 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0 write ( obj ) Write a sample. Parameters: obj \u2013 Sample to be written. Source code in webdataset/writer.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size webdataset.TarWriter A class for writing dictionaries to tar files. Parameters: fileobj \u2013 File name for tar file (.tgz/.tar) or open file descriptor. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Sample encoding. Defaults to True. compress ( Optional [ bool ] , default: None ) \u2013 Compression flag. Defaults to None. user ( str , default: 'bigdata' ) \u2013 User for tar files. Defaults to \"bigdata\". group ( str , default: 'bigdata' ) \u2013 Group for tar files. Defaults to \"bigdata\". mode ( int , default: 292 ) \u2013 Mode for tar files. Defaults to 0o0444. keep_meta ( bool , default: False ) \u2013 Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime ( Optional [ float ] , default: None ) \u2013 Modification time. Defaults to None. format ( Any , default: None ) \u2013 Tar format. Defaults to None. Returns: \u2013 TarWriter object. Raises: ValueError \u2013 If the encoder doesn't yield bytes for a key. True will use an encoder that behaves similar to the automatic decoder for Dataset . False disables encoding and expects byte strings (except for metadata, which must be strings). The encoder argument can also be a callable , or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: a/b.png and a/b.output.png . tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) Source code in webdataset/writer.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 class TarWriter : \"\"\"A class for writing dictionaries to tar files. Args: fileobj: File name for tar file (.tgz/.tar) or open file descriptor. encoder: Sample encoding. Defaults to True. compress: Compression flag. Defaults to None. user: User for tar files. Defaults to \"bigdata\". group: Group for tar files. Defaults to \"bigdata\". mode: Mode for tar files. Defaults to 0o0444. keep_meta: Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime: Modification time. Defaults to None. format: Tar format. Defaults to None. Returns: TarWriter object. Raises: ValueError: If the encoder doesn't yield bytes for a key. `True` will use an encoder that behaves similar to the automatic decoder for `Dataset`. `False` disables encoding and expects byte strings (except for metadata, which must be strings). The `encoder` argument can also be a `callable`, or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: `a/b.png` and `a/b.output.png`. tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) \"\"\" def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ bool ] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime if isinstance ( fileobj , str ): if compress is False : tarmode = \"w|\" elif compress is True : tarmode = \"w|gz\" else : tarmode = \"w|gz\" if fileobj . endswith ( \"gz\" ) else \"w|\" fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : tarmode = \"w|gz\" if compress is True else \"w|\" self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close () def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total __enter__ () Enter context. Returns: self \u2013 The TarWriter object. Source code in webdataset/writer.py 421 422 423 424 425 426 427 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self __exit__ ( exc_type , exc_val , exc_tb ) Exit context. Source code in webdataset/writer.py 429 430 431 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close () __init__ ( fileobj , user = 'bigdata' , group = 'bigdata' , mode = 292 , compress = None , encoder = True , keep_meta = False , mtime = None , format = None ) Create a tar writer. Parameters: fileobj \u2013 Stream to write data to. user ( str , default: 'bigdata' ) \u2013 User for tar files. group ( str , default: 'bigdata' ) \u2013 Group for tar files. mode ( int , default: 292 ) \u2013 Mode for tar files. compress ( Optional [ bool ] , default: None ) \u2013 Desired compression. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Encoder function. keep_meta ( bool , default: False ) \u2013 Keep metadata (entries starting with \"_\"). mtime ( Optional [ float ] , default: None ) \u2013 Modification time (set this to some fixed value to get reproducible tar files). format ( Any , default: None ) \u2013 Tar format. Source code in webdataset/writer.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ bool ] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime if isinstance ( fileobj , str ): if compress is False : tarmode = \"w|\" elif compress is True : tarmode = \"w|gz\" else : tarmode = \"w|gz\" if fileobj . endswith ( \"gz\" ) else \"w|\" fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : tarmode = \"w|gz\" if compress is True else \"w|\" self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress close () Close the tar file. Source code in webdataset/writer.py 433 434 435 436 437 438 def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None write ( obj ) Write a dictionary to the tar file. Parameters: obj \u2013 Dictionary of objects to be stored. Returns: int \u2013 Size of the entry. Raises: ValueError \u2013 If the object doesn't contain a key or if a key doesn't map to bytes after encoding. Source code in webdataset/writer.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total Low Level I/O webdataset . gopen . gopen ( url , mode = 'rb' , bufsize = 8192 , ** kw ) Open the URL using various schemes and protocols. This function provides a unified interface for opening resources specified by URLs, supporting multiple schemes and protocols. It uses the gopen_schemes dispatch table to handle different URL schemes. Built-in support is provided for the following schemes: - pipe: for opening named pipes - file: for local file system access - http, https: for web resources - sftp, ftps: for secure file transfer - scp: for secure copy protocol When no scheme is specified in the URL, it is treated as a local file path. Environment Variables: - GOPEN_VERBOSE: Set to a non-zero value to enable verbose logging of file operations. Format: GOPEN_VERBOSE=1 - USE_AIS_FOR: Specifies which cloud storage services should use AIS (and its cache) for access. Format: USE_AIS_FOR=aws:gs:s3 - GOPEN_BUFFER: Sets the buffer size for file operations (in bytes). Format: GOPEN_BUFFER=8192 Parameters: url ( str ) \u2013 The source URL or file path to open. mode ( str , default: 'rb' ) \u2013 The mode for opening the resource. Only \"rb\" (read binary) and \"wb\" (write binary) are supported. bufsize ( int , default: 8192 ) \u2013 The buffer size for file operations. Default is 8192 bytes. **kw \u2013 Additional keyword arguments to pass to the underlying open function. Returns: \u2013 file-like object: An opened file-like object for the specified resource. Raises: ValueError \u2013 If an unsupported mode is specified. Note: - For stdin/stdout operations, use \"-\" as the URL. - The function applies URL rewriting based on the GOPEN_REWRITE environment variable before processing. Source code in webdataset/gopen.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def gopen ( url , mode = \"rb\" , bufsize = 8192 , ** kw ): \"\"\"Open the URL using various schemes and protocols. This function provides a unified interface for opening resources specified by URLs, supporting multiple schemes and protocols. It uses the `gopen_schemes` dispatch table to handle different URL schemes. Built-in support is provided for the following schemes: - pipe: for opening named pipes - file: for local file system access - http, https: for web resources - sftp, ftps: for secure file transfer - scp: for secure copy protocol When no scheme is specified in the URL, it is treated as a local file path. Environment Variables: - GOPEN_VERBOSE: Set to a non-zero value to enable verbose logging of file operations. Format: GOPEN_VERBOSE=1 - USE_AIS_FOR: Specifies which cloud storage services should use AIS (and its cache) for access. Format: USE_AIS_FOR=aws:gs:s3 - GOPEN_BUFFER: Sets the buffer size for file operations (in bytes). Format: GOPEN_BUFFER=8192 Args: url (str): The source URL or file path to open. mode (str): The mode for opening the resource. Only \"rb\" (read binary) and \"wb\" (write binary) are supported. bufsize (int): The buffer size for file operations. Default is 8192 bytes. **kw: Additional keyword arguments to pass to the underlying open function. Returns: file-like object: An opened file-like object for the specified resource. Raises: ValueError: If an unsupported mode is specified. Other exceptions may be raised depending on the specific handler used for the URL scheme. Note: - For stdin/stdout operations, use \"-\" as the URL. - The function applies URL rewriting based on the GOPEN_REWRITE environment variable before processing. \"\"\" global fallback_gopen verbose = int ( os . environ . get ( \"GOPEN_VERBOSE\" , 0 )) if verbose : print ( \"GOPEN\" , url , info , file = sys . stderr ) assert mode in [ \"rb\" , \"wb\" ], mode if url == \"-\" : if mode == \"rb\" : return sys . stdin . buffer elif mode == \"wb\" : return sys . stdout . buffer else : raise ValueError ( f \"unknown mode { mode } \" ) url = rewrite_url ( url ) pr = urlparse ( url ) if pr . scheme == \"\" : bufsize = int ( os . environ . get ( \"GOPEN_BUFFER\" , - 1 )) return open ( url , mode , buffering = bufsize ) if pr . scheme == \"file\" : bufsize = int ( os . environ . get ( \"GOPEN_BUFFER\" , - 1 )) return open ( pr . path , mode , buffering = bufsize ) handler = gopen_schemes [ \"__default__\" ] handler = gopen_schemes . get ( pr . scheme , handler ) return handler ( url , mode , bufsize , ** kw ) Error Handling webdataset . ignore_and_continue ( exn ) Ignore the exception and continue processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 34 35 36 37 38 39 40 41 42 43 def ignore_and_continue ( exn ): \"\"\"Ignore the exception and continue processing. Args: exn: The exception to be ignored. Returns: bool: Always returns True to indicate continuation. \"\"\" return True webdataset . ignore_and_stop ( exn ) Ignore the exception and stop further processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 60 61 62 63 64 65 66 67 68 69 def ignore_and_stop ( exn ): \"\"\"Ignore the exception and stop further processing. Args: exn: The exception to be ignored. Returns: bool: Always returns False to indicate stopping. \"\"\" return False webdataset . reraise_exception ( exn ) Re-raise the given exception. Parameters: exn \u2013 The exception to be re-raised. Source code in webdataset/handlers.py 22 23 24 25 26 27 28 29 30 31 def reraise_exception ( exn ): \"\"\"Re-raise the given exception. Args: exn: The exception to be re-raised. Raises: The input exception. \"\"\" raise exn webdataset . warn_and_continue ( exn ) Issue a warning for the exception and continue processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 46 47 48 49 50 51 52 53 54 55 56 57 def warn_and_continue ( exn ): \"\"\"Issue a warning for the exception and continue processing. Args: exn: The exception to be warned about. Returns: bool: Always returns True to indicate continuation. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return True webdataset . warn_and_stop ( exn ) Issue a warning for the exception and stop further processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 72 73 74 75 76 77 78 79 80 81 82 83 def warn_and_stop ( exn ): \"\"\"Issue a warning for the exception and stop further processing. Args: exn: The exception to be warned about. Returns: bool: Always returns False to indicate stopping. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return False","title":"WebDataset"},{"location":"webdataset/#webdataset-api","text":"","title":"WebDataset API"},{"location":"webdataset/#fluid-interfaces","text":"The FluidInterface class provides a way to create fluent interfaces for chaining operations on datasets. Most operations are contained in the FluidInterface mixin class. with_epoch sets the epoch size (number of samples per epoch), effectively an itertools.islice over the dataset.","title":"Fluid Interfaces"},{"location":"webdataset/#webdataset.WebDataset","text":"Bases: DataPipeline , FluidInterface Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Parameters: urls \u2013 The source URLs or specifications for the dataset. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. mode \u2013 The mode of operation. Defaults to None. resampled \u2013 Whether to use resampled mode. Defaults to False. repeat \u2013 Whether to repeat the dataset. Defaults to False. shardshuffle \u2013 The number of shards to shuffle, or None. Defaults to None. cache_size \u2013 The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir \u2013 The directory to use for caching. Defaults to None. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle \u2013 Whether to use deterministic shuffling. Defaults to False. nodesplitter \u2013 Function to split data by node. Defaults to single_node_only. workersplitter \u2013 Function to split data by worker. Defaults to split_by_worker. select_files \u2013 Function to select files from tar archives. Defaults to None. rename_files \u2013 Function to rename files from tar archives. Defaults to None. empty_check \u2013 Whether to check for empty datasets. Defaults to True. verbose \u2013 Whether to print verbose output. Defaults to False. seed \u2013 Random seed for shuffling. Defaults to None. Raises: ValueError \u2013 If the cache directory does not exist or if the URL type is not supported. Source code in webdataset/compat.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 class WebDataset ( DataPipeline , FluidInterface ): \"\"\"Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Args: urls: The source URLs or specifications for the dataset. handler: Function to handle exceptions. Defaults to reraise_exception. mode: The mode of operation. Defaults to None. resampled: Whether to use resampled mode. Defaults to False. repeat: Whether to repeat the dataset. Defaults to False. shardshuffle: The number of shards to shuffle, or None. Defaults to None. cache_size: The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir: The directory to use for caching. Defaults to None. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle: Whether to use deterministic shuffling. Defaults to False. nodesplitter: Function to split data by node. Defaults to single_node_only. workersplitter: Function to split data by worker. Defaults to split_by_worker. select_files: Function to select files from tar archives. Defaults to None. rename_files: Function to rename files from tar archives. Defaults to None. empty_check: Whether to check for empty datasets. Defaults to True. verbose: Whether to print verbose output. Defaults to False. seed: Random seed for shuffling. Defaults to None. Raises: ValueError: If the cache directory does not exist or if the URL type is not supported. \"\"\" def __init__ ( self , urls , handler = reraise_exception , mode = None , resampled = False , repeat = False , shardshuffle = None , cache_size =- 1 , cache_dir = None , url_to_name = cache . pipe_cleaner , detshuffle = False , nodesplitter = shardlists . single_node_only , workersplitter = shardlists . split_by_worker , select_files = None , rename_files = None , empty_check = True , verbose = False , seed = None , ): super () . __init__ () if resampled : mode = \"resampled\" if mode == \"resampled\" and shardshuffle not in ( False , None ): warnings . warn ( \"WebDataset(shardshuffle=...) is ignored for resampled datasets\" ) elif shardshuffle is None : warnings . warn ( \"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\" ) if shardshuffle is True : warnings . warn ( \"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\" ) shardshuffle = 100 args = SimpleNamespace ( ** locals ()) self . seed = seed or os . environ . get ( \"WDS_SEED\" , random . randint ( 0 , 1000000 )) self . update_cache_info ( args ) # first, we add a generator for the urls to used # this generates a stream of dict(url=...) self . create_url_iterator ( args ) # split by node (for distributed processing) if nodesplitter is not None : self . append ( nodesplitter ) # split by worker (for DataLoader) if workersplitter : self . append ( shardlists . split_by_worker ) # add a shard shuffler if args . shardshuffle is not None : if args . detshuffle : self . append ( filters . detshuffle ( args . shardshuffle , seed = args . seed )) else : self . append ( filters . shuffle ( args . shardshuffle , seed = args . seed )) # next, we select a URL opener, either with or without caching # this generates a stream of dict(url=..., stream=...) if cache_dir is None or cache_size == 0 : opener = cache . StreamingOpen ( handler = handler ) else : opener = cache . FileCache ( cache_dir = cache_dir , cache_size = cache_size , handler = handler ) self . append ( opener ) # now we need to open each stream and read the tar files contained in it # this generates a stream of dict(fname=..., data=...) objects expander = pipelinefilter ( tar_file_expander ) self . append ( expander ( handler = handler , select_files = select_files , rename_files = rename_files ) ) # finally, the files need to be groups into samples # this generates a stream of dict(__key__=..., ...=...) objects grouper = pipelinefilter ( group_by_keys ) self . append ( grouper ( handler = handler )) # check for empty datasets if empty_check : self . append ( check_empty ) def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" ) def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardlist ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" ) def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close ()","title":"WebDataset"},{"location":"webdataset/#webdataset.WebDataset.__enter__","text":"Enter the runtime context for the WebDataset. Returns: self \u2013 The WebDataset instance. Source code in webdataset/compat.py 503 504 505 506 507 508 509 def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self","title":"__enter__"},{"location":"webdataset/#webdataset.WebDataset.__exit__","text":"Exit the runtime context for the WebDataset. Parameters: *args \u2013 Exception type, value, and traceback if an exception occurred. Source code in webdataset/compat.py 511 512 513 514 515 516 517 def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close ()","title":"__exit__"},{"location":"webdataset/#webdataset.WebDataset.create_url_iterator","text":"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the URL type is not supported or implemented. Source code in webdataset/compat.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardlist ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" )","title":"create_url_iterator"},{"location":"webdataset/#webdataset.WebDataset.update_cache_info","text":"Update cache information based on arguments and environment variables. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the specified cache directory does not exist. Source code in webdataset/compat.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" )","title":"update_cache_info"},{"location":"webdataset/#webdataset.WebLoader","text":"Bases: DataPipeline , FluidInterface A wrapper for DataLoader that adds a fluid interface. Source code in webdataset/compat.py 528 529 530 531 class WebLoader ( DataPipeline , FluidInterface ): \"\"\"A wrapper for DataLoader that adds a fluid interface.\"\"\" def __init__ ( self , * args , ** kw ): super () . __init__ ( DataLoader ( * args , ** kw ))","title":"WebLoader"},{"location":"webdataset/#webdataset.FluidInterface","text":"Source code in webdataset/compat.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 class FluidInterface : def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) ) def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ()) def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None )) def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ()) def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile )) def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw )) def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler )) def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler ) def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw )) def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw )) def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw )) def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler )) def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args )) def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw )) def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p )) def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw )) def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw )) def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw )) def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ()) def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw ))","title":"FluidInterface"},{"location":"webdataset/#webdataset.FluidInterface.batched","text":"Create batches of the given size. This method forwards to the filters.batched function. Parameters: batchsize ( int ) \u2013 Target batch size. collation_fn ( callable , default: default_collation_fn ) \u2013 Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial ( bool , default: True ) \u2013 Whether to return partial batches. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with batched filter. Source code in webdataset/compat.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) )","title":"batched"},{"location":"webdataset/#webdataset.FluidInterface.decode","text":"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Parameters: *args \u2013 Decoding functions or strings representing image handlers. pre ( callable , default: None ) \u2013 Pre-processing function. post ( callable , default: None ) \u2013 Post-processing function. only ( list , default: None ) \u2013 List of keys to decode. partial ( bool , default: False ) \u2013 Whether to allow partial decoding. Defaults to False. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with decode filter. Source code in webdataset/compat.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler )","title":"decode"},{"location":"webdataset/#webdataset.FluidInterface.extract_keys","text":"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Parameters: *args \u2013 Keys or patterns to extract. **kw \u2013 Additional keyword arguments for filters.extract_keys. Returns: FluidInterface \u2013 Updated pipeline with extract_keys filter. Source code in webdataset/compat.py 256 257 258 259 260 261 262 263 264 265 266 267 268 def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw ))","title":"extract_keys"},{"location":"webdataset/#webdataset.FluidInterface.listed","text":"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Parameters: batchsize ( int ) \u2013 Target list size. partial ( bool , default: True ) \u2013 Whether to return partial lists. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with listed filter. Source code in webdataset/compat.py 47 48 49 50 51 52 53 54 55 56 57 58 59 def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None ))","title":"listed"},{"location":"webdataset/#webdataset.FluidInterface.lmdb_cached","text":"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Parameters: *args \u2013 Positional arguments for filters.LMDBCached. **kw \u2013 Keyword arguments for filters.LMDBCached. Returns: FluidInterface \u2013 Updated pipeline with LMDB caching. Source code in webdataset/compat.py 294 295 296 297 298 299 300 301 302 303 304 305 306 def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw ))","title":"lmdb_cached"},{"location":"webdataset/#webdataset.FluidInterface.log_keys","text":"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Parameters: logfile ( str , default: None ) \u2013 Path to the log file. If None, logging is disabled. Returns: FluidInterface \u2013 Updated pipeline with log_keys filter. Source code in webdataset/compat.py 71 72 73 74 75 76 77 78 79 80 81 82 def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile ))","title":"log_keys"},{"location":"webdataset/#webdataset.FluidInterface.map","text":"Apply a function to each sample in the stream. This method forwards to the filters.map function. Parameters: f ( callable ) \u2013 Function to apply to each sample. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map filter. Source code in webdataset/compat.py 101 102 103 104 105 106 107 108 109 110 111 112 113 def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler ))","title":"map"},{"location":"webdataset/#webdataset.FluidInterface.map_dict","text":"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Parameters: handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. **kw \u2013 Mapping of keys to functions to apply. Returns: FluidInterface \u2013 Updated pipeline with map_dict filter. Source code in webdataset/compat.py 147 148 149 150 151 152 153 154 155 156 157 158 159 def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw ))","title":"map_dict"},{"location":"webdataset/#webdataset.FluidInterface.map_tuple","text":"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Parameters: *args \u2013 Functions to apply to each element of the tuple. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map_tuple filter. Source code in webdataset/compat.py 189 190 191 192 193 194 195 196 197 198 199 200 201 def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler ))","title":"map_tuple"},{"location":"webdataset/#webdataset.FluidInterface.mcached","text":"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface \u2013 Updated pipeline with memory caching. Source code in webdataset/compat.py 284 285 286 287 288 289 290 291 292 def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ())","title":"mcached"},{"location":"webdataset/#webdataset.FluidInterface.rename","text":"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Parameters: **kw \u2013 Mapping of old names to new names. Returns: FluidInterface \u2013 Updated pipeline with rename filter. Source code in webdataset/compat.py 216 217 218 219 220 221 222 223 224 225 226 227 def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw ))","title":"rename"},{"location":"webdataset/#webdataset.FluidInterface.rename_keys","text":"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Parameters: *args \u2013 Positional arguments for filters.rename_keys. **kw \u2013 Keyword arguments for filters.rename_keys. Returns: FluidInterface \u2013 Updated pipeline with rename_keys filter. Source code in webdataset/compat.py 242 243 244 245 246 247 248 249 250 251 252 253 254 def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw ))","title":"rename_keys"},{"location":"webdataset/#webdataset.FluidInterface.rsample","text":"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Parameters: p ( float , default: 0.5 ) \u2013 Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface \u2013 Updated pipeline with rsample filter. Source code in webdataset/compat.py 229 230 231 232 233 234 235 236 237 238 239 240 def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p ))","title":"rsample"},{"location":"webdataset/#webdataset.FluidInterface.select","text":"Select samples based on a predicate. This method forwards to the filters.select function. Parameters: predicate ( callable ) \u2013 Function that returns True for samples to keep. **kw \u2013 Additional keyword arguments for filters.select. Returns: FluidInterface \u2013 Updated pipeline with select filter. Source code in webdataset/compat.py 161 162 163 164 165 166 167 168 169 170 171 172 173 def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw ))","title":"select"},{"location":"webdataset/#webdataset.FluidInterface.shuffle","text":"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Parameters: size ( int ) \u2013 Buffer size for shuffling. **kw \u2013 Additional keyword arguments for filters.shuffle. Returns: FluidInterface \u2013 Updated pipeline with shuffle filter, or self if size < 1. Source code in webdataset/compat.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw ))","title":"shuffle"},{"location":"webdataset/#webdataset.FluidInterface.slice","text":"Slice the data stream. This method forwards to the filters.slice function. Parameters: *args \u2013 Arguments for slicing (start, stop, step). Returns: FluidInterface \u2013 Updated pipeline with slice filter. Source code in webdataset/compat.py 203 204 205 206 207 208 209 210 211 212 213 214 def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args ))","title":"slice"},{"location":"webdataset/#webdataset.FluidInterface.to_tuple","text":"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Parameters: *args \u2013 Keys to extract from the dict. **kw \u2013 Additional keyword arguments for filters.to_tuple. Returns: FluidInterface \u2013 Updated pipeline with to_tuple filter. Source code in webdataset/compat.py 175 176 177 178 179 180 181 182 183 184 185 186 187 def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw ))","title":"to_tuple"},{"location":"webdataset/#webdataset.FluidInterface.unbatched","text":"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface \u2013 Updated pipeline with unbatched filter. Source code in webdataset/compat.py 37 38 39 40 41 42 43 44 45 def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ())","title":"unbatched"},{"location":"webdataset/#webdataset.FluidInterface.unlisted","text":"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface \u2013 Updated pipeline with unlisted filter. Source code in webdataset/compat.py 61 62 63 64 65 66 67 68 69 def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ())","title":"unlisted"},{"location":"webdataset/#webdataset.FluidInterface.xdecode","text":"Decode data based on file extensions. This method forwards to the filters.xdecode function. Parameters: *args \u2013 Positional arguments for filters.xdecode. **kw \u2013 Keyword arguments for filters.xdecode. Returns: FluidInterface \u2013 Updated pipeline with xdecode filter. Source code in webdataset/compat.py 270 271 272 273 274 275 276 277 278 279 280 281 282 def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw ))","title":"xdecode"},{"location":"webdataset/#webdataset.with_epoch","text":"Bases: IterableDataset Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Parameters: dataset \u2013 The source IterableDataset. length ( int ) \u2013 Declared length of the dataset. Source code in webdataset/extradatasets.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class with_epoch ( IterableDataset ): \"\"\"Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Args: dataset: The source IterableDataset. length (int): Declared length of the dataset. \"\"\" def __init__ ( self , dataset , length ): super () . __init__ () self . length = length self . source = None def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None","title":"with_epoch"},{"location":"webdataset/#webdataset.with_epoch.__getstate__","text":"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict \u2013 A dictionary representing the pickled state of the dataset. Source code in webdataset/extradatasets.py 91 92 93 94 95 96 97 98 99 100 101 def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result","title":"__getstate__"},{"location":"webdataset/#webdataset.with_epoch.invoke","text":"Return an iterator over the dataset. This iterator returns as many samples as given by the length parameter. Parameters: dataset \u2013 The source dataset to iterate over. Yields: Sample \u2013 The next sample from the dataset. Source code in webdataset/extradatasets.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None","title":"invoke"},{"location":"webdataset/#writing-webdatasets","text":"","title":"Writing WebDatasets"},{"location":"webdataset/#webdataset.ShardWriter","text":"Like TarWriter but splits into multiple shards. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. Defaults to 100000. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. Defaults to 3e9. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. Defaults to None. start_shard ( int , default: 0 ) \u2013 Starting shard number. Defaults to 0. verbose ( int , default: 1 ) \u2013 Verbosity level. Defaults to 1. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. Defaults to None. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 class ShardWriter : \"\"\"Like TarWriter but splits into multiple shards. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. Defaults to 100000. maxsize: Maximum size of each shard. Defaults to 3e9. post: Optional callable to be executed after each shard is written. Defaults to None. start_shard: Starting shard number. Defaults to 0. verbose: Verbosity level. Defaults to 1. opener: Optional callable to open output files. Defaults to None. **kw: Other options passed to TarWriter. \"\"\" def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream () def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close ()","title":"ShardWriter"},{"location":"webdataset/#webdataset.ShardWriter.__enter__","text":"Enter context. Returns: self \u2013 The ShardWriter object. Source code in webdataset/writer.py 593 594 595 596 597 598 599 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self","title":"__enter__"},{"location":"webdataset/#webdataset.ShardWriter.__exit__","text":"Exit context. Source code in webdataset/writer.py 601 602 603 def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close ()","title":"__exit__"},{"location":"webdataset/#webdataset.ShardWriter.__init__","text":"Create a ShardWriter. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. start_shard ( int , default: 0 ) \u2013 Starting shard number. verbose ( int , default: 1 ) \u2013 Verbosity level. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream ()","title":"__init__"},{"location":"webdataset/#webdataset.ShardWriter.close","text":"Close the stream. Source code in webdataset/writer.py 585 586 587 588 589 590 591 def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size","title":"close"},{"location":"webdataset/#webdataset.ShardWriter.finish","text":"Finish all writing (use close instead). Source code in webdataset/writer.py 576 577 578 579 580 581 582 583 def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None","title":"finish"},{"location":"webdataset/#webdataset.ShardWriter.next_stream","text":"Close the current stream and move to the next. Source code in webdataset/writer.py 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0","title":"next_stream"},{"location":"webdataset/#webdataset.ShardWriter.write","text":"Write a sample. Parameters: obj \u2013 Sample to be written. Source code in webdataset/writer.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size","title":"write"},{"location":"webdataset/#webdataset.TarWriter","text":"A class for writing dictionaries to tar files. Parameters: fileobj \u2013 File name for tar file (.tgz/.tar) or open file descriptor. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Sample encoding. Defaults to True. compress ( Optional [ bool ] , default: None ) \u2013 Compression flag. Defaults to None. user ( str , default: 'bigdata' ) \u2013 User for tar files. Defaults to \"bigdata\". group ( str , default: 'bigdata' ) \u2013 Group for tar files. Defaults to \"bigdata\". mode ( int , default: 292 ) \u2013 Mode for tar files. Defaults to 0o0444. keep_meta ( bool , default: False ) \u2013 Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime ( Optional [ float ] , default: None ) \u2013 Modification time. Defaults to None. format ( Any , default: None ) \u2013 Tar format. Defaults to None. Returns: \u2013 TarWriter object. Raises: ValueError \u2013 If the encoder doesn't yield bytes for a key. True will use an encoder that behaves similar to the automatic decoder for Dataset . False disables encoding and expects byte strings (except for metadata, which must be strings). The encoder argument can also be a callable , or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: a/b.png and a/b.output.png . tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) Source code in webdataset/writer.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 class TarWriter : \"\"\"A class for writing dictionaries to tar files. Args: fileobj: File name for tar file (.tgz/.tar) or open file descriptor. encoder: Sample encoding. Defaults to True. compress: Compression flag. Defaults to None. user: User for tar files. Defaults to \"bigdata\". group: Group for tar files. Defaults to \"bigdata\". mode: Mode for tar files. Defaults to 0o0444. keep_meta: Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime: Modification time. Defaults to None. format: Tar format. Defaults to None. Returns: TarWriter object. Raises: ValueError: If the encoder doesn't yield bytes for a key. `True` will use an encoder that behaves similar to the automatic decoder for `Dataset`. `False` disables encoding and expects byte strings (except for metadata, which must be strings). The `encoder` argument can also be a `callable`, or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: `a/b.png` and `a/b.output.png`. tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) \"\"\" def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ bool ] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime if isinstance ( fileobj , str ): if compress is False : tarmode = \"w|\" elif compress is True : tarmode = \"w|gz\" else : tarmode = \"w|gz\" if fileobj . endswith ( \"gz\" ) else \"w|\" fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : tarmode = \"w|gz\" if compress is True else \"w|\" self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close () def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total","title":"TarWriter"},{"location":"webdataset/#webdataset.TarWriter.__enter__","text":"Enter context. Returns: self \u2013 The TarWriter object. Source code in webdataset/writer.py 421 422 423 424 425 426 427 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self","title":"__enter__"},{"location":"webdataset/#webdataset.TarWriter.__exit__","text":"Exit context. Source code in webdataset/writer.py 429 430 431 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close ()","title":"__exit__"},{"location":"webdataset/#webdataset.TarWriter.__init__","text":"Create a tar writer. Parameters: fileobj \u2013 Stream to write data to. user ( str , default: 'bigdata' ) \u2013 User for tar files. group ( str , default: 'bigdata' ) \u2013 Group for tar files. mode ( int , default: 292 ) \u2013 Mode for tar files. compress ( Optional [ bool ] , default: None ) \u2013 Desired compression. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Encoder function. keep_meta ( bool , default: False ) \u2013 Keep metadata (entries starting with \"_\"). mtime ( Optional [ float ] , default: None ) \u2013 Modification time (set this to some fixed value to get reproducible tar files). format ( Any , default: None ) \u2013 Tar format. Source code in webdataset/writer.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ bool ] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime if isinstance ( fileobj , str ): if compress is False : tarmode = \"w|\" elif compress is True : tarmode = \"w|gz\" else : tarmode = \"w|gz\" if fileobj . endswith ( \"gz\" ) else \"w|\" fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : tarmode = \"w|gz\" if compress is True else \"w|\" self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress","title":"__init__"},{"location":"webdataset/#webdataset.TarWriter.close","text":"Close the tar file. Source code in webdataset/writer.py 433 434 435 436 437 438 def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None","title":"close"},{"location":"webdataset/#webdataset.TarWriter.write","text":"Write a dictionary to the tar file. Parameters: obj \u2013 Dictionary of objects to be stored. Returns: int \u2013 Size of the entry. Raises: ValueError \u2013 If the object doesn't contain a key or if a key doesn't map to bytes after encoding. Source code in webdataset/writer.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total","title":"write"},{"location":"webdataset/#low-level-io","text":"","title":"Low Level I/O"},{"location":"webdataset/#webdataset.gopen.gopen","text":"Open the URL using various schemes and protocols. This function provides a unified interface for opening resources specified by URLs, supporting multiple schemes and protocols. It uses the gopen_schemes dispatch table to handle different URL schemes. Built-in support is provided for the following schemes: - pipe: for opening named pipes - file: for local file system access - http, https: for web resources - sftp, ftps: for secure file transfer - scp: for secure copy protocol When no scheme is specified in the URL, it is treated as a local file path. Environment Variables: - GOPEN_VERBOSE: Set to a non-zero value to enable verbose logging of file operations. Format: GOPEN_VERBOSE=1 - USE_AIS_FOR: Specifies which cloud storage services should use AIS (and its cache) for access. Format: USE_AIS_FOR=aws:gs:s3 - GOPEN_BUFFER: Sets the buffer size for file operations (in bytes). Format: GOPEN_BUFFER=8192 Parameters: url ( str ) \u2013 The source URL or file path to open. mode ( str , default: 'rb' ) \u2013 The mode for opening the resource. Only \"rb\" (read binary) and \"wb\" (write binary) are supported. bufsize ( int , default: 8192 ) \u2013 The buffer size for file operations. Default is 8192 bytes. **kw \u2013 Additional keyword arguments to pass to the underlying open function. Returns: \u2013 file-like object: An opened file-like object for the specified resource. Raises: ValueError \u2013 If an unsupported mode is specified. Note: - For stdin/stdout operations, use \"-\" as the URL. - The function applies URL rewriting based on the GOPEN_REWRITE environment variable before processing. Source code in webdataset/gopen.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def gopen ( url , mode = \"rb\" , bufsize = 8192 , ** kw ): \"\"\"Open the URL using various schemes and protocols. This function provides a unified interface for opening resources specified by URLs, supporting multiple schemes and protocols. It uses the `gopen_schemes` dispatch table to handle different URL schemes. Built-in support is provided for the following schemes: - pipe: for opening named pipes - file: for local file system access - http, https: for web resources - sftp, ftps: for secure file transfer - scp: for secure copy protocol When no scheme is specified in the URL, it is treated as a local file path. Environment Variables: - GOPEN_VERBOSE: Set to a non-zero value to enable verbose logging of file operations. Format: GOPEN_VERBOSE=1 - USE_AIS_FOR: Specifies which cloud storage services should use AIS (and its cache) for access. Format: USE_AIS_FOR=aws:gs:s3 - GOPEN_BUFFER: Sets the buffer size for file operations (in bytes). Format: GOPEN_BUFFER=8192 Args: url (str): The source URL or file path to open. mode (str): The mode for opening the resource. Only \"rb\" (read binary) and \"wb\" (write binary) are supported. bufsize (int): The buffer size for file operations. Default is 8192 bytes. **kw: Additional keyword arguments to pass to the underlying open function. Returns: file-like object: An opened file-like object for the specified resource. Raises: ValueError: If an unsupported mode is specified. Other exceptions may be raised depending on the specific handler used for the URL scheme. Note: - For stdin/stdout operations, use \"-\" as the URL. - The function applies URL rewriting based on the GOPEN_REWRITE environment variable before processing. \"\"\" global fallback_gopen verbose = int ( os . environ . get ( \"GOPEN_VERBOSE\" , 0 )) if verbose : print ( \"GOPEN\" , url , info , file = sys . stderr ) assert mode in [ \"rb\" , \"wb\" ], mode if url == \"-\" : if mode == \"rb\" : return sys . stdin . buffer elif mode == \"wb\" : return sys . stdout . buffer else : raise ValueError ( f \"unknown mode { mode } \" ) url = rewrite_url ( url ) pr = urlparse ( url ) if pr . scheme == \"\" : bufsize = int ( os . environ . get ( \"GOPEN_BUFFER\" , - 1 )) return open ( url , mode , buffering = bufsize ) if pr . scheme == \"file\" : bufsize = int ( os . environ . get ( \"GOPEN_BUFFER\" , - 1 )) return open ( pr . path , mode , buffering = bufsize ) handler = gopen_schemes [ \"__default__\" ] handler = gopen_schemes . get ( pr . scheme , handler ) return handler ( url , mode , bufsize , ** kw )","title":"gopen"},{"location":"webdataset/#error-handling","text":"","title":"Error Handling"},{"location":"webdataset/#webdataset.ignore_and_continue","text":"Ignore the exception and continue processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 34 35 36 37 38 39 40 41 42 43 def ignore_and_continue ( exn ): \"\"\"Ignore the exception and continue processing. Args: exn: The exception to be ignored. Returns: bool: Always returns True to indicate continuation. \"\"\" return True","title":"ignore_and_continue"},{"location":"webdataset/#webdataset.ignore_and_stop","text":"Ignore the exception and stop further processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 60 61 62 63 64 65 66 67 68 69 def ignore_and_stop ( exn ): \"\"\"Ignore the exception and stop further processing. Args: exn: The exception to be ignored. Returns: bool: Always returns False to indicate stopping. \"\"\" return False","title":"ignore_and_stop"},{"location":"webdataset/#webdataset.reraise_exception","text":"Re-raise the given exception. Parameters: exn \u2013 The exception to be re-raised. Source code in webdataset/handlers.py 22 23 24 25 26 27 28 29 30 31 def reraise_exception ( exn ): \"\"\"Re-raise the given exception. Args: exn: The exception to be re-raised. Raises: The input exception. \"\"\" raise exn","title":"reraise_exception"},{"location":"webdataset/#webdataset.warn_and_continue","text":"Issue a warning for the exception and continue processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 46 47 48 49 50 51 52 53 54 55 56 57 def warn_and_continue ( exn ): \"\"\"Issue a warning for the exception and continue processing. Args: exn: The exception to be warned about. Returns: bool: Always returns True to indicate continuation. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return True","title":"warn_and_continue"},{"location":"webdataset/#webdataset.warn_and_stop","text":"Issue a warning for the exception and stop further processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 72 73 74 75 76 77 78 79 80 81 82 83 def warn_and_stop ( exn ): \"\"\"Issue a warning for the exception and stop further processing. Args: exn: The exception to be warned about. Returns: bool: Always returns False to indicate stopping. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return False","title":"warn_and_stop"},{"location":"webdataset2/","text":"WebDataset WebDataset is a PyTorch Dataset (IterableDataset) implementation providing efficient access to large datasets stored in POSIX tar archives. It is designed for use with distributed training and for streaming data directly from web servers into training pipelines. webdataset.autodecode Automatically decode webdataset samples. pytorch_weights_only = os . environ . get ( 'WDS_PYTORCH_WEIGHTS_ONLY' , '0' ) == '1' module-attribute Extensions passed on to the image decoder. Continue Special class for continuing decoding. This is mostly used for decompression, as in: def decompressor(key, data): if key.endswith(\".gz\"): return Continue(key[:-3], decompress(data)) return None Source code in webdataset/autodecode.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 class Continue : \"\"\"Special class for continuing decoding. This is mostly used for decompression, as in: def decompressor(key, data): if key.endswith(\".gz\"): return Continue(key[:-3], decompress(data)) return None \"\"\" def __init__ ( self , key , data ): \"\"\"__init__. :param key: :param data: \"\"\" self . key , self . data = key , data __init__ ( key , data ) init . :param key: :param data: Source code in webdataset/autodecode.py 460 461 462 463 464 465 466 def __init__ ( self , key , data ): \"\"\"__init__. :param key: :param data: \"\"\" self . key , self . data = key , data Decoder Decode samples using a list of handlers. For each key/data item, this iterates through the list of handlers until some handler returns something other than None. Source code in webdataset/autodecode.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 class Decoder : \"\"\"Decode samples using a list of handlers. For each key/data item, this iterates through the list of handlers until some handler returns something other than None. \"\"\" def __init__ ( self , handlers , pre = None , post = None , only = None , partial = False ): \"\"\"Create a Decoder. :param handlers: main list of handlers :param pre: handlers called before the main list (.gz handler by default) :param post: handlers called after the main list (default handlers by default) :param only: a list of extensions; when give, only ignores files with those extensions :param partial: allow partial decoding (i.e., don't decode fields that aren't of type bytes) \"\"\" assert isinstance ( handlers , list ), f \"handlers = { handlers } must be a list\" if isinstance ( only , str ): only = only . split () self . only = only if only is None else set ( only ) if pre is None : pre = default_pre_handlers if post is None : post = default_post_handlers assert all ( callable ( h ) for h in handlers ), f \"one of { handlers } not callable\" assert all ( callable ( h ) for h in pre ), f \"one of { pre } not callable\" assert all ( callable ( h ) for h in post ), f \"one of { post } not callable\" self . handlers = pre + handlers + post self . partial = partial def decode1 ( self , key , data ): \"\"\"Decode a single field of a sample. :param key: file name extension :param data: binary data \"\"\" key = \".\" + key for f in self . handlers : result = f ( key , data ) if isinstance ( result , Continue ): key , data = result . key , result . data continue if result is not None : return result return data def decode ( self , sample ): \"\"\"Decode an entire sample. :param sample: the sample, a dictionary of key value pairs \"\"\" result = {} assert isinstance ( sample , dict ), sample for k , v in list ( sample . items ()): if k [: 2 ] == \"__\" : if isinstance ( v , bytes ): try : v = v . decode ( \"utf-8\" ) except Exception : print ( f \"Can't decode v of k = { k } as utf-8: v = { v } \" ) result [ k ] = v continue if self . only is not None and k not in self . only : result [ k ] = v continue assert v is not None if self . partial : if isinstance ( v , bytes ): result [ k ] = self . decode1 ( k , v ) else : result [ k ] = v else : assert isinstance ( v , bytes ), f \"k,v = { k } , { v } \" result [ k ] = self . decode1 ( k , v ) return result def __call__ ( self , sample ): \"\"\"Decode an entire sample. :param sample: the sample \"\"\" assert isinstance ( sample , dict ), ( len ( sample ), sample ) return self . decode ( sample ) __call__ ( sample ) Decode an entire sample. :param sample: the sample Source code in webdataset/autodecode.py 570 571 572 573 574 575 576 def __call__ ( self , sample ): \"\"\"Decode an entire sample. :param sample: the sample \"\"\" assert isinstance ( sample , dict ), ( len ( sample ), sample ) return self . decode ( sample ) __init__ ( handlers , pre = None , post = None , only = None , partial = False ) Create a Decoder. :param handlers: main list of handlers :param pre: handlers called before the main list (.gz handler by default) :param post: handlers called after the main list (default handlers by default) :param only: a list of extensions; when give, only ignores files with those extensions :param partial: allow partial decoding (i.e., don't decode fields that aren't of type bytes) Source code in webdataset/autodecode.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 def __init__ ( self , handlers , pre = None , post = None , only = None , partial = False ): \"\"\"Create a Decoder. :param handlers: main list of handlers :param pre: handlers called before the main list (.gz handler by default) :param post: handlers called after the main list (default handlers by default) :param only: a list of extensions; when give, only ignores files with those extensions :param partial: allow partial decoding (i.e., don't decode fields that aren't of type bytes) \"\"\" assert isinstance ( handlers , list ), f \"handlers = { handlers } must be a list\" if isinstance ( only , str ): only = only . split () self . only = only if only is None else set ( only ) if pre is None : pre = default_pre_handlers if post is None : post = default_post_handlers assert all ( callable ( h ) for h in handlers ), f \"one of { handlers } not callable\" assert all ( callable ( h ) for h in pre ), f \"one of { pre } not callable\" assert all ( callable ( h ) for h in post ), f \"one of { post } not callable\" self . handlers = pre + handlers + post self . partial = partial decode ( sample ) Decode an entire sample. :param sample: the sample, a dictionary of key value pairs Source code in webdataset/autodecode.py 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 def decode ( self , sample ): \"\"\"Decode an entire sample. :param sample: the sample, a dictionary of key value pairs \"\"\" result = {} assert isinstance ( sample , dict ), sample for k , v in list ( sample . items ()): if k [: 2 ] == \"__\" : if isinstance ( v , bytes ): try : v = v . decode ( \"utf-8\" ) except Exception : print ( f \"Can't decode v of k = { k } as utf-8: v = { v } \" ) result [ k ] = v continue if self . only is not None and k not in self . only : result [ k ] = v continue assert v is not None if self . partial : if isinstance ( v , bytes ): result [ k ] = self . decode1 ( k , v ) else : result [ k ] = v else : assert isinstance ( v , bytes ), f \"k,v = { k } , { v } \" result [ k ] = self . decode1 ( k , v ) return result decode1 ( key , data ) Decode a single field of a sample. :param key: file name extension :param data: binary data Source code in webdataset/autodecode.py 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 def decode1 ( self , key , data ): \"\"\"Decode a single field of a sample. :param key: file name extension :param data: binary data \"\"\" key = \".\" + key for f in self . handlers : result = f ( key , data ) if isinstance ( result , Continue ): key , data = result . key , result . data continue if result is not None : return result return data ImageHandler Decode image data using the given imagespec . The imagespec specifies whether the image is decoded to numpy/torch/pi, decoded to uint8/float, and decoded to l/rgb/rgba: l8: numpy uint8 l rgb8: numpy uint8 rgb rgba8: numpy uint8 rgba l: numpy float l rgb: numpy float rgb rgba: numpy float rgba torchl8: torch uint8 l torchrgb8: torch uint8 rgb torchrgba8: torch uint8 rgba torchl: torch float l torchrgb: torch float rgb torch: torch float rgb torchrgba: torch float rgba pill: pil None l pil: pil None rgb pilrgb: pil None rgb pilrgba: pil None rgba Source code in webdataset/autodecode.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 class ImageHandler : \"\"\"Decode image data using the given `imagespec`. The `imagespec` specifies whether the image is decoded to numpy/torch/pi, decoded to uint8/float, and decoded to l/rgb/rgba: - l8: numpy uint8 l - rgb8: numpy uint8 rgb - rgba8: numpy uint8 rgba - l: numpy float l - rgb: numpy float rgb - rgba: numpy float rgba - torchl8: torch uint8 l - torchrgb8: torch uint8 rgb - torchrgba8: torch uint8 rgba - torchl: torch float l - torchrgb: torch float rgb - torch: torch float rgb - torchrgba: torch float rgba - pill: pil None l - pil: pil None rgb - pilrgb: pil None rgb - pilrgba: pil None rgba \"\"\" def __init__ ( self , imagespec , extensions = IMAGE_EXTENSIONS ): \"\"\"Create an image handler. :param imagespec: short string indicating the type of decoding :param extensions: list of extensions the image handler is invoked for \"\"\" if imagespec not in list ( imagespecs . keys ()): raise ValueError ( \"Unknown imagespec: %s \" % imagespec ) self . imagespec = imagespec . lower () self . extensions = extensions def __call__ ( self , key , data ): \"\"\"Perform image decoding. :param key: file name extension :param data: binary data \"\"\" import PIL.Image extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension . lower () not in self . extensions : return None imagespec = self . imagespec atype , etype , mode = imagespecs [ imagespec ] with io . BytesIO ( data ) as stream : img = PIL . Image . open ( stream ) img . load () img = img . convert ( mode . upper ()) if atype == \"pil\" : if mode == \"l\" : img = img . convert ( \"L\" ) return img elif mode == \"rgb\" : img = img . convert ( \"RGB\" ) return img elif mode == \"rgba\" : img = img . convert ( \"RGBA\" ) return img else : raise ValueError ( \"Unknown mode: %s \" % mode ) result = np . asarray ( img ) if etype == \"float\" : result = result . astype ( np . float32 ) / 255.0 assert result . ndim in [ 2 , 3 ], result . shape assert mode in [ \"l\" , \"rgb\" , \"rgba\" ], mode if mode == \"l\" : if result . ndim == 3 : result = np . mean ( result [:, :, : 3 ], axis = 2 ) elif mode == \"rgb\" : if result . ndim == 2 : result = np . repeat ( result [:, :, np . newaxis ], 3 , axis = 2 ) elif result . shape [ 2 ] == 4 : result = result [:, :, : 3 ] elif mode == \"rgba\" : if result . ndim == 2 : result = np . repeat ( result [:, :, np . newaxis ], 4 , axis = 2 ) result [:, :, 3 ] = 255 elif result . shape [ 2 ] == 3 : result = np . concatenate ( [ result , 255 * np . ones ( result . shape [: 2 ])], axis = 2 ) assert atype in [ \"numpy\" , \"torch\" ], atype if atype == \"numpy\" : return result elif atype == \"torch\" : import torch if result . ndim == 3 : return torch . from_numpy ( result . transpose ( 2 , 0 , 1 ) . copy ()) else : return torch . from_numpy ( result . copy ()) return None __call__ ( key , data ) Perform image decoding. :param key: file name extension :param data: binary data Source code in webdataset/autodecode.py 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def __call__ ( self , key , data ): \"\"\"Perform image decoding. :param key: file name extension :param data: binary data \"\"\" import PIL.Image extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension . lower () not in self . extensions : return None imagespec = self . imagespec atype , etype , mode = imagespecs [ imagespec ] with io . BytesIO ( data ) as stream : img = PIL . Image . open ( stream ) img . load () img = img . convert ( mode . upper ()) if atype == \"pil\" : if mode == \"l\" : img = img . convert ( \"L\" ) return img elif mode == \"rgb\" : img = img . convert ( \"RGB\" ) return img elif mode == \"rgba\" : img = img . convert ( \"RGBA\" ) return img else : raise ValueError ( \"Unknown mode: %s \" % mode ) result = np . asarray ( img ) if etype == \"float\" : result = result . astype ( np . float32 ) / 255.0 assert result . ndim in [ 2 , 3 ], result . shape assert mode in [ \"l\" , \"rgb\" , \"rgba\" ], mode if mode == \"l\" : if result . ndim == 3 : result = np . mean ( result [:, :, : 3 ], axis = 2 ) elif mode == \"rgb\" : if result . ndim == 2 : result = np . repeat ( result [:, :, np . newaxis ], 3 , axis = 2 ) elif result . shape [ 2 ] == 4 : result = result [:, :, : 3 ] elif mode == \"rgba\" : if result . ndim == 2 : result = np . repeat ( result [:, :, np . newaxis ], 4 , axis = 2 ) result [:, :, 3 ] = 255 elif result . shape [ 2 ] == 3 : result = np . concatenate ( [ result , 255 * np . ones ( result . shape [: 2 ])], axis = 2 ) assert atype in [ \"numpy\" , \"torch\" ], atype if atype == \"numpy\" : return result elif atype == \"torch\" : import torch if result . ndim == 3 : return torch . from_numpy ( result . transpose ( 2 , 0 , 1 ) . copy ()) else : return torch . from_numpy ( result . copy ()) return None __init__ ( imagespec , extensions = IMAGE_EXTENSIONS ) Create an image handler. :param imagespec: short string indicating the type of decoding :param extensions: list of extensions the image handler is invoked for Source code in webdataset/autodecode.py 303 304 305 306 307 308 309 310 311 312 def __init__ ( self , imagespec , extensions = IMAGE_EXTENSIONS ): \"\"\"Create an image handler. :param imagespec: short string indicating the type of decoding :param extensions: list of extensions the image handler is invoked for \"\"\" if imagespec not in list ( imagespecs . keys ()): raise ValueError ( \"Unknown imagespec: %s \" % imagespec ) self . imagespec = imagespec . lower () self . extensions = extensions basichandlers ( key , data ) Handle basic file decoding. This function is usually part of the post= decoders. This handles the following forms of decoding: txt -> unicode string cls cls2 class count index inx id -> int json jsn -> JSON decoding pyd pickle -> pickle decoding pth -> torch.loads ten tenbin -> fast tensor loading mp messagepack msg -> messagepack decoding npy -> Python NPY decoding :param key: file name extension :param data: binary data to be decoded Source code in webdataset/autodecode.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def basichandlers ( key , data ): \"\"\"Handle basic file decoding. This function is usually part of the post= decoders. This handles the following forms of decoding: - txt -> unicode string - cls cls2 class count index inx id -> int - json jsn -> JSON decoding - pyd pickle -> pickle decoding - pth -> torch.loads - ten tenbin -> fast tensor loading - mp messagepack msg -> messagepack decoding - npy -> Python NPY decoding :param key: file name extension :param data: binary data to be decoded \"\"\" extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension in decoders : return decoders [ extension ]( data ) return None call_extension_handler ( key , data , f , extensions ) Call the function f with the given data if the key matches the extensions. :param key: actual key found in the sample :param data: binary data :param f: decoder function :param extensions: list of matching extensions Source code in webdataset/autodecode.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def call_extension_handler ( key , data , f , extensions ): \"\"\"Call the function f with the given data if the key matches the extensions. :param key: actual key found in the sample :param data: binary data :param f: decoder function :param extensions: list of matching extensions \"\"\" extension = key . lower () . split ( \".\" ) for target in extensions : target = target . split ( \".\" ) if len ( target ) > len ( extension ): continue if extension [ - len ( target ) :] == target : return f ( data ) return None cbor_loads ( data ) Load data from cbor format. Imports cbor only if necessary. Source code in webdataset/autodecode.py 155 156 157 158 159 def cbor_loads ( data ): \"\"\"Load data from cbor format. Imports cbor only if necessary.\"\"\" import cbor return cbor . loads ( data ) gzfilter ( key , data ) Decode .gz files. This decodes compressed files and the continues decoding. :param key: file name extension :param data: binary data Source code in webdataset/autodecode.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 def gzfilter ( key , data ): \"\"\"Decode .gz files. This decodes compressed files and the continues decoding. :param key: file name extension :param data: binary data \"\"\" import gzip if not key . endswith ( \".gz\" ): return None decompressed = gzip . open ( io . BytesIO ( data )) . read () return Continue ( key [: - 3 ], decompressed ) handle_extension ( extensions , f ) Return a decoder function for the list of extensions. Extensions can be a space separated list of extensions. Extensions can contain dots, in which case the corresponding number of extension components must be present in the key given to f. Comparisons are case insensitive. Examples: handle_extension(\"jpg jpeg\", my_decode_jpg) # invoked for any file.jpg handle_extension(\"seg.jpg\", special_case_jpg) # invoked only for file.seg.jpg Source code in webdataset/autodecode.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def handle_extension ( extensions , f ): \"\"\"Return a decoder function for the list of extensions. Extensions can be a space separated list of extensions. Extensions can contain dots, in which case the corresponding number of extension components must be present in the key given to f. Comparisons are case insensitive. Examples: handle_extension(\"jpg jpeg\", my_decode_jpg) # invoked for any file.jpg handle_extension(\"seg.jpg\", special_case_jpg) # invoked only for file.seg.jpg \"\"\" extensions = extensions . lower () . split () return partial ( call_extension_handler , f = f , extensions = extensions ) imagehandler ( imagespec , extensions = IMAGE_EXTENSIONS ) Create an image handler. This is just a lower case alias for ImageHander. :param imagespec: textual image spec :param extensions: list of extensions the handler should be applied for Source code in webdataset/autodecode.py 385 386 387 388 389 390 391 392 393 def imagehandler ( imagespec , extensions = IMAGE_EXTENSIONS ): \"\"\"Create an image handler. This is just a lower case alias for ImageHander. :param imagespec: textual image spec :param extensions: list of extensions the handler should be applied for \"\"\" return ImageHandler ( imagespec , extensions ) msgpack_loads ( data ) Load data from msgpack format. Imports msgpack only if necessary. Source code in webdataset/autodecode.py 133 134 135 136 137 def msgpack_loads ( data ): \"\"\"Load data from msgpack format. Imports msgpack only if necessary.\"\"\" import msgpack return msgpack . unpackb ( data ) npy_loads ( data ) Load data from npy format. Imports numpy only if necessary. Source code in webdataset/autodecode.py 140 141 142 143 144 145 def npy_loads ( data ): \"\"\"Load data from npy format. Imports numpy only if necessary.\"\"\" import numpy.lib.format stream = io . BytesIO ( data ) return numpy . lib . format . read_array ( stream ) npz_loads ( data ) Load data from npz format. Imports numpy only if necessary. Source code in webdataset/autodecode.py 147 148 149 150 151 152 def npz_loads ( data ): \"\"\"Load data from npz format. Imports numpy only if necessary.\"\"\" import numpy.lib.format stream = io . BytesIO ( data ) return dict ( np . load ( stream )) tenbin_loads ( data ) Load data from tenbin format. Imports tenbin only if necessary. Source code in webdataset/autodecode.py 126 127 128 129 130 def tenbin_loads ( data ): \"\"\"Load data from tenbin format. Imports tenbin only if necessary.\"\"\" from . import tenbin return tenbin . decode_buffer ( data ) torch_audio ( key , data ) Decode audio using the torchaudio library. :param key: file name extension :param data: data to be decoded Source code in webdataset/autodecode.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def torch_audio ( key , data ): \"\"\"Decode audio using the torchaudio library. :param key: file name extension :param data: data to be decoded \"\"\" extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension not in [ \"flac\" , \"mp3\" , \"sox\" , \"wav\" , \"m4a\" , \"ogg\" , \"wma\" ]: return None import torchaudio with tempfile . TemporaryDirectory () as dirname : fname = os . path . join ( dirname , f \"file. { extension } \" ) with open ( fname , \"wb\" ) as stream : stream . write ( data ) return torchaudio . load ( fname ) torch_loads ( data ) Function: torch_loads Description: This function loads data using torch.loads. It first imports torch only if necessary. Then it decodes the input data using torch.load. Parameters: - data (bytes): The data to be decoded. Returns: It returns the decoded input data. Example data = b'...' output = torch_loads(data) Source code in webdataset/autodecode.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def torch_loads ( data : bytes ): \"\"\"Function: torch_loads Description: This function loads data using torch.loads. It first imports torch only if necessary. Then it decodes the input data using torch.load. Parameters: - data (bytes): The data to be decoded. Returns: It returns the decoded input data. Example: data = b'...' output = torch_loads(data) \"\"\" import io import torch stream = io . BytesIO ( data ) return torch . load ( stream , weights_only = pytorch_weights_only ) torch_video ( key , data ) Decode video using the torchvideo library. :param key: file name extension :param data: data to be decoded Source code in webdataset/autodecode.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 def torch_video ( key , data ): \"\"\"Decode video using the torchvideo library. :param key: file name extension :param data: data to be decoded \"\"\" extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension not in \"mp4 ogv mjpeg avi mov h264 mpg webm wmv\" . split (): return None import torchvision.io with tempfile . TemporaryDirectory () as dirname : fname = os . path . join ( dirname , f \"file. { extension } \" ) with open ( fname , \"wb\" ) as stream : stream . write ( data ) return torchvision . io . read_video ( fname , pts_unit = \"sec\" ) webdataset.cache Code related to caching files downloaded from storage servers, object servers, and web servers. FileCache Cache files from URLs. This class provides functionality to download and cache files from URLs, with options for validation, error handling, and cache management. Parameters: cache_dir ( Optional [ str ] , default: None ) \u2013 The directory to use for caching. Defaults to None. url_to_name ( Callable [[ str ], str ] , default: url_to_cache_name ) \u2013 Function to convert URLs to cache names. verbose ( bool , default: False ) \u2013 Whether to print verbose output. Defaults to False. validator ( Callable [[ str ], bool ] , default: check_tar_format ) \u2013 Function to validate downloaded files. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Function to handle exceptions. cache_size ( int , default: -1 ) \u2013 Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_cleanup_interval ( int , default: 30 ) \u2013 Interval between cache cleanup operations in seconds. Source code in webdataset/cache.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class FileCache : \"\"\"Cache files from URLs. This class provides functionality to download and cache files from URLs, with options for validation, error handling, and cache management. Args: cache_dir (Optional[str]): The directory to use for caching. Defaults to None. url_to_name (Callable[[str], str]): Function to convert URLs to cache names. verbose (bool): Whether to print verbose output. Defaults to False. validator (Callable[[str], bool]): Function to validate downloaded files. handler (Callable[[Exception], bool]): Function to handle exceptions. cache_size (int): Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_cleanup_interval (int): Interval between cache cleanup operations in seconds. \"\"\" def __init__ ( self , cache_dir : Optional [ str ] = None , * , url_to_name : Callable [[ str ], str ] = url_to_cache_name , verbose : bool = False , validator : Callable [[ str ], bool ] = check_tar_format , handler : Callable [[ Exception ], bool ] = reraise_exception , cache_size : int = - 1 , cache_cleanup_interval : int = 30 , ): self . url_to_name = url_to_name self . validator = validator self . handler = handler if cache_dir is None : self . cache_dir = default_cache_dir else : self . cache_dir = cache_dir self . verbose = verbose if cache_size > 0 : self . cleaner = LRUCleanup ( self . cache_dir , cache_size , verbose = self . verbose , interval = cache_cleanup_interval , ) else : self . cleaner = None def get_file ( self , url : str ) -> str : \"\"\"Download a file from a given URL and return the path to the downloaded file. Args: url (str): The URL of the file to download. Returns: str: The path to the downloaded file. Raises: ValueError: If the downloaded file fails validation. \"\"\" assert isinstance ( url , str ) if islocal ( url ): return urlparse ( url ) . path cache_name = self . url_to_name ( url ) assert \"/\" not in cache_name , f \"bad cache name { cache_name } for { url } \" destdir = os . path . join ( self . cache_dir , os . path . dirname ( cache_name )) os . makedirs ( destdir , exist_ok = True ) dest = os . path . join ( self . cache_dir , cache_name ) if not os . path . exists ( dest ): if self . verbose : print ( \"# downloading %s to %s \" % ( url , dest ), file = sys . stderr ) if self . cleaner is not None : self . cleaner . cleanup () download ( url , dest , verbose = self . verbose ) if self . validator : if not self . validator ( dest ): ftype = get_filetype ( dest ) with open ( dest , \"rb\" ) as f : data = f . read ( 200 ) os . remove ( dest ) raise ValueError ( \" %s ( %s ) is not a tar archive, but a %s , contains %s \" % ( dest , url , ftype , repr ( data )) ) return dest def __call__ ( self , urls : Iterable [ str ]) -> Iterable [ io . IOBase ]: \"\"\"Download files from a list of URLs and yield file streams. Args: urls (Iterable[str]): An iterable of URLs to download files from. Yields: dict: A dictionary containing the URL, file stream, and local path of each downloaded file. Raises: Exception: If there's an error downloading or opening a file. \"\"\" for url in urls : if isinstance ( url , dict ): url = url [ \"url\" ] for _ in range ( 10 ): try : dest = self . get_file ( url ) stream = open ( dest , \"rb\" ) except Exception as e : if self . handler ( e ): continue else : break yield dict ( url = url , stream = stream , local_path = dest ) break __call__ ( urls ) Download files from a list of URLs and yield file streams. Parameters: urls ( Iterable [ str ] ) \u2013 An iterable of URLs to download files from. Yields: dict ( Iterable [ IOBase ] ) \u2013 A dictionary containing the URL, file stream, and local path of each downloaded file. Raises: Exception \u2013 If there's an error downloading or opening a file. Source code in webdataset/cache.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def __call__ ( self , urls : Iterable [ str ]) -> Iterable [ io . IOBase ]: \"\"\"Download files from a list of URLs and yield file streams. Args: urls (Iterable[str]): An iterable of URLs to download files from. Yields: dict: A dictionary containing the URL, file stream, and local path of each downloaded file. Raises: Exception: If there's an error downloading or opening a file. \"\"\" for url in urls : if isinstance ( url , dict ): url = url [ \"url\" ] for _ in range ( 10 ): try : dest = self . get_file ( url ) stream = open ( dest , \"rb\" ) except Exception as e : if self . handler ( e ): continue else : break yield dict ( url = url , stream = stream , local_path = dest ) break get_file ( url ) Download a file from a given URL and return the path to the downloaded file. Parameters: url ( str ) \u2013 The URL of the file to download. Returns: str ( str ) \u2013 The path to the downloaded file. Raises: ValueError \u2013 If the downloaded file fails validation. Source code in webdataset/cache.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def get_file ( self , url : str ) -> str : \"\"\"Download a file from a given URL and return the path to the downloaded file. Args: url (str): The URL of the file to download. Returns: str: The path to the downloaded file. Raises: ValueError: If the downloaded file fails validation. \"\"\" assert isinstance ( url , str ) if islocal ( url ): return urlparse ( url ) . path cache_name = self . url_to_name ( url ) assert \"/\" not in cache_name , f \"bad cache name { cache_name } for { url } \" destdir = os . path . join ( self . cache_dir , os . path . dirname ( cache_name )) os . makedirs ( destdir , exist_ok = True ) dest = os . path . join ( self . cache_dir , cache_name ) if not os . path . exists ( dest ): if self . verbose : print ( \"# downloading %s to %s \" % ( url , dest ), file = sys . stderr ) if self . cleaner is not None : self . cleaner . cleanup () download ( url , dest , verbose = self . verbose ) if self . validator : if not self . validator ( dest ): ftype = get_filetype ( dest ) with open ( dest , \"rb\" ) as f : data = f . read ( 200 ) os . remove ( dest ) raise ValueError ( \" %s ( %s ) is not a tar archive, but a %s , contains %s \" % ( dest , url , ftype , repr ( data )) ) return dest LRUCleanup Perform LRU cleanup on a cache directory. Source code in webdataset/cache.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class LRUCleanup : \"\"\"Perform LRU cleanup on a cache directory.\"\"\" def __init__ ( self , cache_dir = None , cache_size = int ( 1e12 ), keyfn = os . path . getctime , verbose = False , interval = 30 , ): \"\"\"Initialize the LRU cleanup object.\"\"\" self . cache_dir = cache_dir self . cache_size = cache_size self . keyfn = keyfn self . verbose = verbose self . interval = interval self . last_run = 0 def set_cache_dir ( self , cache_dir ): \"\"\"Set the cache directory.\"\"\" self . cache_dir = cache_dir def cleanup ( self ): \"\"\"Performs cleanup of the file cache in cache_dir using an LRU strategy, keeping the total size of all remaining files below cache_size. \"\"\" if not os . path . exists ( self . cache_dir ): return if self . interval is not None and time . time () - self . last_run < self . interval : return try : total_size = 0 for dirpath , dirnames , filenames in os . walk ( self . cache_dir ): for filename in filenames : total_size += os . path . getsize ( os . path . join ( dirpath , filename )) if total_size <= self . cache_size : return # sort files by last access time files = [] for dirpath , dirnames , filenames in os . walk ( self . cache_dir ): for filename in filenames : files . append ( os . path . join ( dirpath , filename )) files . sort ( key = self . keyfn , reverse = True ) # delete files until we're under the cache size while len ( files ) > 0 and total_size > self . cache_size : fname = files . pop () total_size -= os . path . getsize ( fname ) if self . verbose : print ( \"# deleting %s \" % fname , file = sys . stderr ) os . remove ( fname ) except ( OSError , FileNotFoundError ): # files may be deleted by other processes between walking the directory and getting their size/deleting them pass self . last_run = time . time () __init__ ( cache_dir = None , cache_size = int ( 1000000000000.0 ), keyfn = os . path . getctime , verbose = False , interval = 30 ) Initialize the LRU cleanup object. Source code in webdataset/cache.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , cache_dir = None , cache_size = int ( 1e12 ), keyfn = os . path . getctime , verbose = False , interval = 30 , ): \"\"\"Initialize the LRU cleanup object.\"\"\" self . cache_dir = cache_dir self . cache_size = cache_size self . keyfn = keyfn self . verbose = verbose self . interval = interval self . last_run = 0 cleanup () Performs cleanup of the file cache in cache_dir using an LRU strategy, keeping the total size of all remaining files below cache_size. Source code in webdataset/cache.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def cleanup ( self ): \"\"\"Performs cleanup of the file cache in cache_dir using an LRU strategy, keeping the total size of all remaining files below cache_size. \"\"\" if not os . path . exists ( self . cache_dir ): return if self . interval is not None and time . time () - self . last_run < self . interval : return try : total_size = 0 for dirpath , dirnames , filenames in os . walk ( self . cache_dir ): for filename in filenames : total_size += os . path . getsize ( os . path . join ( dirpath , filename )) if total_size <= self . cache_size : return # sort files by last access time files = [] for dirpath , dirnames , filenames in os . walk ( self . cache_dir ): for filename in filenames : files . append ( os . path . join ( dirpath , filename )) files . sort ( key = self . keyfn , reverse = True ) # delete files until we're under the cache size while len ( files ) > 0 and total_size > self . cache_size : fname = files . pop () total_size -= os . path . getsize ( fname ) if self . verbose : print ( \"# deleting %s \" % fname , file = sys . stderr ) os . remove ( fname ) except ( OSError , FileNotFoundError ): # files may be deleted by other processes between walking the directory and getting their size/deleting them pass self . last_run = time . time () set_cache_dir ( cache_dir ) Set the cache directory. Source code in webdataset/cache.py 109 110 111 def set_cache_dir ( self , cache_dir ): \"\"\"Set the cache directory.\"\"\" self . cache_dir = cache_dir StreamingOpen Open a stream from a URL. Source code in webdataset/cache.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class StreamingOpen : \"\"\"Open a stream from a URL.\"\"\" def __init__ ( self , verbose = False , handler = reraise_exception ): \"\"\"Initialize the streaming open object.\"\"\" self . verbose = verbose self . handler = handler def __call__ ( self , urls ): \"\"\"Open a stream from a URL.\"\"\" for url in urls : if isinstance ( url , dict ): url = url [ \"url\" ] parsed = urlparse ( url ) try : if parsed . scheme in [ \"\" , \"file\" ]: stream = open ( parsed . path , \"rb\" ) yield dict ( url = url , stream = stream , local_path = parsed . path ) else : stream = gopen . gopen ( url ) yield dict ( url = url , stream = stream ) except Exception as exn : if self . handler ( exn ): continue else : break __call__ ( urls ) Open a stream from a URL. Source code in webdataset/cache.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def __call__ ( self , urls ): \"\"\"Open a stream from a URL.\"\"\" for url in urls : if isinstance ( url , dict ): url = url [ \"url\" ] parsed = urlparse ( url ) try : if parsed . scheme in [ \"\" , \"file\" ]: stream = open ( parsed . path , \"rb\" ) yield dict ( url = url , stream = stream , local_path = parsed . path ) else : stream = gopen . gopen ( url ) yield dict ( url = url , stream = stream ) except Exception as exn : if self . handler ( exn ): continue else : break __init__ ( verbose = False , handler = reraise_exception ) Initialize the streaming open object. Source code in webdataset/cache.py 163 164 165 166 def __init__ ( self , verbose = False , handler = reraise_exception ): \"\"\"Initialize the streaming open object.\"\"\" self . verbose = verbose self . handler = handler cached_tarfile_samples ( src , handler = reraise_exception , cache_size =- 1 , cache_dir = None , verbose = False , url_to_name = pipe_cleaner , always = False , select_files = None , rename_files = None ) Process and yield samples from cached tar files. This function is obsolete. Parameters: src \u2013 An iterable source of URLs or dictionaries containing URLs. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. cache_size ( int , default: -1 ) \u2013 Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_dir ( Optional [ str ] , default: None ) \u2013 The directory to use for caching. Defaults to None. verbose ( bool , default: False ) \u2013 Whether to print verbose output. Defaults to False. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. always ( bool , default: False ) \u2013 Whether to always download files, even if they exist locally. Defaults to False. select_files \u2013 Function to select specific files from the tar archive. Defaults to None. rename_files \u2013 Function to rename files from the tar archive. Defaults to None. Returns: \u2013 An iterable of samples extracted from the cached tar files. Source code in webdataset/cache.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 @obsolete def cached_tarfile_samples ( src , handler = reraise_exception , cache_size =- 1 , cache_dir = None , verbose = False , url_to_name = pipe_cleaner , always = False , select_files = None , rename_files = None , ): \"\"\"Process and yield samples from cached tar files. This function is obsolete. Args: src: An iterable source of URLs or dictionaries containing URLs. handler: Function to handle exceptions. Defaults to reraise_exception. cache_size (int): Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_dir (Optional[str]): The directory to use for caching. Defaults to None. verbose (bool): Whether to print verbose output. Defaults to False. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. always (bool): Whether to always download files, even if they exist locally. Defaults to False. select_files: Function to select specific files from the tar archive. Defaults to None. rename_files: Function to rename files from the tar archive. Defaults to None. Returns: An iterable of samples extracted from the cached tar files. \"\"\" verbose = verbose or int ( os . environ . get ( \"GOPEN_VERBOSE\" , 0 )) streams = cached_url_opener ( src , handler = handler , cache_size = cache_size , cache_dir = cache_dir , verbose = verbose , url_to_name = url_to_name , always = always , ) files = tar_file_expander ( streams , handler = handler , select_files = select_files , rename_files = rename_files ) samples = group_by_keys ( files , handler = handler ) return samples cached_url_opener ( data , handler = reraise_exception , cache_size =- 1 , cache_dir = None , url_to_name = pipe_cleaner , validator = check_tar_format , verbose = False , always = False ) Open streams for a sequence of URLs, with caching. Given a stream of URL names (packaged in dict(url=url) ), yield opened streams. Parameters: data \u2013 An iterable of dictionaries containing URLs. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. cache_size ( int , default: -1 ) \u2013 Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_dir ( Optional [ str ] , default: None ) \u2013 The directory to use for caching. Defaults to None. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. validator \u2013 Function to validate downloaded files. Defaults to check_tar_format. verbose ( bool , default: False ) \u2013 Whether to print verbose output. Defaults to False. always ( bool , default: False ) \u2013 Whether to always download files, even if they exist locally. Defaults to False. Yields: dict \u2013 A dictionary containing the original sample data and an opened file stream. Raises: ValueError \u2013 If a downloaded file fails validation. Exception \u2013 For any other errors during download or file opening. Source code in webdataset/cache.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 @obsolete def cached_url_opener ( data , handler = reraise_exception , cache_size =- 1 , cache_dir = None , url_to_name = pipe_cleaner , validator = check_tar_format , verbose = False , always = False , ): \"\"\"Open streams for a sequence of URLs, with caching. Given a stream of URL names (packaged in `dict(url=url)`), yield opened streams. Args: data: An iterable of dictionaries containing URLs. handler: Function to handle exceptions. Defaults to reraise_exception. cache_size (int): Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_dir (Optional[str]): The directory to use for caching. Defaults to None. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. validator: Function to validate downloaded files. Defaults to check_tar_format. verbose (bool): Whether to print verbose output. Defaults to False. always (bool): Whether to always download files, even if they exist locally. Defaults to False. Yields: dict: A dictionary containing the original sample data and an opened file stream. Raises: ValueError: If a downloaded file fails validation. Exception: For any other errors during download or file opening. \"\"\" verbose = verbose or verbose_cache for sample in data : assert isinstance ( sample , dict ), sample assert \"url\" in sample url = sample [ \"url\" ] attempts = 5 try : if not always and os . path . exists ( url ): dest = url else : dest = get_file_cached ( url , cache_size = cache_size , cache_dir = cache_dir , url_to_name = url_to_name , verbose = verbose , ) if verbose : print ( \"# opening %s \" % dest , file = sys . stderr ) assert os . path . exists ( dest ) if not validator ( dest ): ftype = get_filetype ( dest ) with open ( dest , \"rb\" ) as f : data = f . read ( 200 ) os . remove ( dest ) raise ValueError ( \" %s ( %s ) is not a tar archive, but a %s , contains %s \" % ( dest , url , ftype , repr ( data )) ) try : stream = open ( dest , \"rb\" ) sample . update ( stream = stream ) yield sample except FileNotFoundError as exn : # dealing with race conditions in lru_cleanup attempts -= 1 if attempts > 0 : time . sleep ( random . random () * 10 ) continue raise exn except Exception as exn : exn . args = exn . args + ( url ,) if handler ( exn ): continue else : break check_tar_format ( fname ) Check whether a file is a tar archive. Source code in webdataset/cache.py 43 44 45 46 47 def check_tar_format ( fname : str ): \"\"\"Check whether a file is a tar archive.\"\"\" assert os . path . exists ( fname ), fname ftype = get_filetype ( fname ) return \"tar archive\" in ftype or \"gzip compressed\" in ftype download ( url , dest , chunk_size = 1024 ** 2 , verbose = False ) Download a file from url to dest . Source code in webdataset/cache.py 147 148 149 150 151 152 153 154 155 156 157 def download ( url , dest , chunk_size = 1024 ** 2 , verbose = False ): \"\"\"Download a file from `url` to `dest`.\"\"\" temp = dest + f \".temp { os . getpid () } \" with gopen . gopen ( url ) as stream : with open ( temp , \"wb\" ) as f : while True : data = stream . read ( chunk_size ) if not data : break f . write ( data ) os . rename ( temp , dest ) get_filetype ( fname ) Get the file type of a file. Source code in webdataset/cache.py 34 35 36 37 38 39 40 def get_filetype ( fname : str ): \"\"\"Get the file type of a file.\"\"\" assert os . path . exists ( fname ), fname assert os . system ( \"file . > /dev/null\" ) == 0 , \"UNIX/Linux file command not available\" with os . popen ( \"file ' %s '\" % fname ) as f : ftype = f . read () return ftype islocal ( url ) Check whether a URL is a local file. Source code in webdataset/cache.py 28 29 30 31 def islocal ( url ): \"\"\"Check whether a URL is a local file.\"\"\" parsed = urlparse ( url ) return parsed . scheme in [ \"\" , \"file\" ] pipe_cleaner ( spec ) Guess the actual URL from a \"pipe:\" specification. Source code in webdataset/cache.py 50 51 52 53 54 55 56 57 58 59 @obsolete def pipe_cleaner ( spec ): \"\"\"Guess the actual URL from a \"pipe:\" specification.\"\"\" if spec . startswith ( \"pipe:\" ): spec = spec [ 5 :] words = spec . split ( \" \" ) for word in words : if re . match ( r \"^(https?|hdfs|gs|ais|s3):\" , word ): return word return spec url_to_cache_name ( url , ndir = 0 ) Guess the cache name from a URL. Source code in webdataset/cache.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def url_to_cache_name ( url , ndir = 0 ): \"\"\"Guess the cache name from a URL.\"\"\" assert isinstance ( url , str ) parsed = urlparse ( url ) if parsed . scheme in [ None , \"\" , \"file\" , \"http\" , \"https\" , \"ftp\" , \"ftps\" , \"gs\" , \"s3\" , \"ais\" , ]: path = parsed . path path = path . lstrip ( \"/\" ) # always relative list_of_directories = path . split ( \"/\" ) return \"/\" . join ( list_of_directories [ - 1 - ndir :]) else : # don't try to guess, just urlencode the whole thing with \"/\" and \":\" # quoted using the urllib.quote function quoted = urllib . parse . quote ( url , safe = \"_+ {} *,-\" ) quoted = quoted [ - 128 :] return quoted webdataset.compat FluidInterface Source code in webdataset/compat.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 class FluidInterface : def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) ) def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ()) def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None )) def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ()) def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile )) def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw )) def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler )) def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler ) def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw )) def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw )) def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw )) def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler )) def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args )) def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw )) def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p )) def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw )) def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw )) def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw )) def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ()) def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw )) batched ( batchsize , collation_fn = filters . default_collation_fn , partial = True ) Create batches of the given size. This method forwards to the filters.batched function. Parameters: batchsize ( int ) \u2013 Target batch size. collation_fn ( callable , default: default_collation_fn ) \u2013 Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial ( bool , default: True ) \u2013 Whether to return partial batches. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with batched filter. Source code in webdataset/compat.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) ) decode ( * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception ) Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Parameters: *args \u2013 Decoding functions or strings representing image handlers. pre ( callable , default: None ) \u2013 Pre-processing function. post ( callable , default: None ) \u2013 Post-processing function. only ( list , default: None ) \u2013 List of keys to decode. partial ( bool , default: False ) \u2013 Whether to allow partial decoding. Defaults to False. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with decode filter. Source code in webdataset/compat.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler ) extract_keys ( * args , ** kw ) Extract specific keys from samples. This method forwards to the filters.extract_keys function. Parameters: *args \u2013 Keys or patterns to extract. **kw \u2013 Additional keyword arguments for filters.extract_keys. Returns: FluidInterface \u2013 Updated pipeline with extract_keys filter. Source code in webdataset/compat.py 256 257 258 259 260 261 262 263 264 265 266 267 268 def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw )) listed ( batchsize , partial = True ) Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Parameters: batchsize ( int ) \u2013 Target list size. partial ( bool , default: True ) \u2013 Whether to return partial lists. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with listed filter. Source code in webdataset/compat.py 47 48 49 50 51 52 53 54 55 56 57 58 59 def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None )) lmdb_cached ( * args , ** kw ) Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Parameters: *args \u2013 Positional arguments for filters.LMDBCached. **kw \u2013 Keyword arguments for filters.LMDBCached. Returns: FluidInterface \u2013 Updated pipeline with LMDB caching. Source code in webdataset/compat.py 294 295 296 297 298 299 300 301 302 303 304 305 306 def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw )) log_keys ( logfile = None ) Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Parameters: logfile ( str , default: None ) \u2013 Path to the log file. If None, logging is disabled. Returns: FluidInterface \u2013 Updated pipeline with log_keys filter. Source code in webdataset/compat.py 71 72 73 74 75 76 77 78 79 80 81 82 def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile )) map ( f , handler = reraise_exception ) Apply a function to each sample in the stream. This method forwards to the filters.map function. Parameters: f ( callable ) \u2013 Function to apply to each sample. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map filter. Source code in webdataset/compat.py 101 102 103 104 105 106 107 108 109 110 111 112 113 def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler )) map_dict ( handler = reraise_exception , ** kw ) Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Parameters: handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. **kw \u2013 Mapping of keys to functions to apply. Returns: FluidInterface \u2013 Updated pipeline with map_dict filter. Source code in webdataset/compat.py 147 148 149 150 151 152 153 154 155 156 157 158 159 def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw )) map_tuple ( * args , handler = reraise_exception ) Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Parameters: *args \u2013 Functions to apply to each element of the tuple. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map_tuple filter. Source code in webdataset/compat.py 189 190 191 192 193 194 195 196 197 198 199 200 201 def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler )) mcached () Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface \u2013 Updated pipeline with memory caching. Source code in webdataset/compat.py 284 285 286 287 288 289 290 291 292 def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ()) rename ( ** kw ) Rename samples based on keyword arguments. This method forwards to the filters.rename function. Parameters: **kw \u2013 Mapping of old names to new names. Returns: FluidInterface \u2013 Updated pipeline with rename filter. Source code in webdataset/compat.py 216 217 218 219 220 221 222 223 224 225 226 227 def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw )) rename_keys ( * args , ** kw ) Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Parameters: *args \u2013 Positional arguments for filters.rename_keys. **kw \u2013 Keyword arguments for filters.rename_keys. Returns: FluidInterface \u2013 Updated pipeline with rename_keys filter. Source code in webdataset/compat.py 242 243 244 245 246 247 248 249 250 251 252 253 254 def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw )) rsample ( p = 0.5 ) Randomly subsample a stream of data. This method forwards to the filters.rsample function. Parameters: p ( float , default: 0.5 ) \u2013 Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface \u2013 Updated pipeline with rsample filter. Source code in webdataset/compat.py 229 230 231 232 233 234 235 236 237 238 239 240 def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p )) select ( predicate , ** kw ) Select samples based on a predicate. This method forwards to the filters.select function. Parameters: predicate ( callable ) \u2013 Function that returns True for samples to keep. **kw \u2013 Additional keyword arguments for filters.select. Returns: FluidInterface \u2013 Updated pipeline with select filter. Source code in webdataset/compat.py 161 162 163 164 165 166 167 168 169 170 171 172 173 def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw )) shuffle ( size , ** kw ) Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Parameters: size ( int ) \u2013 Buffer size for shuffling. **kw \u2013 Additional keyword arguments for filters.shuffle. Returns: FluidInterface \u2013 Updated pipeline with shuffle filter, or self if size < 1. Source code in webdataset/compat.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw )) slice ( * args ) Slice the data stream. This method forwards to the filters.slice function. Parameters: *args \u2013 Arguments for slicing (start, stop, step). Returns: FluidInterface \u2013 Updated pipeline with slice filter. Source code in webdataset/compat.py 203 204 205 206 207 208 209 210 211 212 213 214 def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args )) to_tuple ( * args , ** kw ) Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Parameters: *args \u2013 Keys to extract from the dict. **kw \u2013 Additional keyword arguments for filters.to_tuple. Returns: FluidInterface \u2013 Updated pipeline with to_tuple filter. Source code in webdataset/compat.py 175 176 177 178 179 180 181 182 183 184 185 186 187 def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw )) unbatched () Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface \u2013 Updated pipeline with unbatched filter. Source code in webdataset/compat.py 37 38 39 40 41 42 43 44 45 def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ()) unlisted () Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface \u2013 Updated pipeline with unlisted filter. Source code in webdataset/compat.py 61 62 63 64 65 66 67 68 69 def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ()) xdecode ( * args , ** kw ) Decode data based on file extensions. This method forwards to the filters.xdecode function. Parameters: *args \u2013 Positional arguments for filters.xdecode. **kw \u2013 Keyword arguments for filters.xdecode. Returns: FluidInterface \u2013 Updated pipeline with xdecode filter. Source code in webdataset/compat.py 270 271 272 273 274 275 276 277 278 279 280 281 282 def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw )) FluidWrapper Bases: DataPipeline , FluidInterface Small fluid-interface wrapper for DataPipeline. Source code in webdataset/compat.py 520 521 522 523 524 525 class FluidWrapper ( DataPipeline , FluidInterface ): \"\"\"Small fluid-interface wrapper for DataPipeline.\"\"\" def __init__ ( self , initial ): super () . __init__ () self . append ( initial ) WebDataset Bases: DataPipeline , FluidInterface Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Parameters: urls \u2013 The source URLs or specifications for the dataset. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. mode \u2013 The mode of operation. Defaults to None. resampled \u2013 Whether to use resampled mode. Defaults to False. repeat \u2013 Whether to repeat the dataset. Defaults to False. shardshuffle \u2013 The number of shards to shuffle, or None. Defaults to None. cache_size \u2013 The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir \u2013 The directory to use for caching. Defaults to None. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle \u2013 Whether to use deterministic shuffling. Defaults to False. nodesplitter \u2013 Function to split data by node. Defaults to single_node_only. workersplitter \u2013 Function to split data by worker. Defaults to split_by_worker. select_files \u2013 Function to select files from tar archives. Defaults to None. rename_files \u2013 Function to rename files from tar archives. Defaults to None. empty_check \u2013 Whether to check for empty datasets. Defaults to True. verbose \u2013 Whether to print verbose output. Defaults to False. seed \u2013 Random seed for shuffling. Defaults to None. Raises: ValueError \u2013 If the cache directory does not exist or if the URL type is not supported. Source code in webdataset/compat.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 class WebDataset ( DataPipeline , FluidInterface ): \"\"\"Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Args: urls: The source URLs or specifications for the dataset. handler: Function to handle exceptions. Defaults to reraise_exception. mode: The mode of operation. Defaults to None. resampled: Whether to use resampled mode. Defaults to False. repeat: Whether to repeat the dataset. Defaults to False. shardshuffle: The number of shards to shuffle, or None. Defaults to None. cache_size: The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir: The directory to use for caching. Defaults to None. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle: Whether to use deterministic shuffling. Defaults to False. nodesplitter: Function to split data by node. Defaults to single_node_only. workersplitter: Function to split data by worker. Defaults to split_by_worker. select_files: Function to select files from tar archives. Defaults to None. rename_files: Function to rename files from tar archives. Defaults to None. empty_check: Whether to check for empty datasets. Defaults to True. verbose: Whether to print verbose output. Defaults to False. seed: Random seed for shuffling. Defaults to None. Raises: ValueError: If the cache directory does not exist or if the URL type is not supported. \"\"\" def __init__ ( self , urls , handler = reraise_exception , mode = None , resampled = False , repeat = False , shardshuffle = None , cache_size =- 1 , cache_dir = None , url_to_name = cache . pipe_cleaner , detshuffle = False , nodesplitter = shardlists . single_node_only , workersplitter = shardlists . split_by_worker , select_files = None , rename_files = None , empty_check = True , verbose = False , seed = None , ): super () . __init__ () if resampled : mode = \"resampled\" if mode == \"resampled\" and shardshuffle not in ( False , None ): warnings . warn ( \"WebDataset(shardshuffle=...) is ignored for resampled datasets\" ) elif shardshuffle is None : warnings . warn ( \"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\" ) if shardshuffle is True : warnings . warn ( \"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\" ) shardshuffle = 100 args = SimpleNamespace ( ** locals ()) self . seed = seed or os . environ . get ( \"WDS_SEED\" , random . randint ( 0 , 1000000 )) self . update_cache_info ( args ) # first, we add a generator for the urls to used # this generates a stream of dict(url=...) self . create_url_iterator ( args ) # split by node (for distributed processing) if nodesplitter is not None : self . append ( nodesplitter ) # split by worker (for DataLoader) if workersplitter : self . append ( shardlists . split_by_worker ) # add a shard shuffler if args . shardshuffle is not None : if args . detshuffle : self . append ( filters . detshuffle ( args . shardshuffle , seed = args . seed )) else : self . append ( filters . shuffle ( args . shardshuffle , seed = args . seed )) # next, we select a URL opener, either with or without caching # this generates a stream of dict(url=..., stream=...) if cache_dir is None or cache_size == 0 : opener = cache . StreamingOpen ( handler = handler ) else : opener = cache . FileCache ( cache_dir = cache_dir , cache_size = cache_size , handler = handler ) self . append ( opener ) # now we need to open each stream and read the tar files contained in it # this generates a stream of dict(fname=..., data=...) objects expander = pipelinefilter ( tar_file_expander ) self . append ( expander ( handler = handler , select_files = select_files , rename_files = rename_files ) ) # finally, the files need to be groups into samples # this generates a stream of dict(__key__=..., ...=...) objects grouper = pipelinefilter ( group_by_keys ) self . append ( grouper ( handler = handler )) # check for empty datasets if empty_check : self . append ( check_empty ) def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" ) def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardlist ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" ) def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close () __enter__ () Enter the runtime context for the WebDataset. Returns: self \u2013 The WebDataset instance. Source code in webdataset/compat.py 503 504 505 506 507 508 509 def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self __exit__ ( * args ) Exit the runtime context for the WebDataset. Parameters: *args \u2013 Exception type, value, and traceback if an exception occurred. Source code in webdataset/compat.py 511 512 513 514 515 516 517 def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close () create_url_iterator ( args ) Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the URL type is not supported or implemented. Source code in webdataset/compat.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardlist ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" ) update_cache_info ( args ) Update cache information based on arguments and environment variables. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the specified cache directory does not exist. Source code in webdataset/compat.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" ) WebLoader Bases: DataPipeline , FluidInterface A wrapper for DataLoader that adds a fluid interface. Source code in webdataset/compat.py 528 529 530 531 class WebLoader ( DataPipeline , FluidInterface ): \"\"\"A wrapper for DataLoader that adds a fluid interface.\"\"\" def __init__ ( self , * args , ** kw ): super () . __init__ ( DataLoader ( * args , ** kw )) check_empty ( source ) Check if the dataset is empty and yield samples. Parameters: source \u2013 An iterable source of samples. Yields: \u2013 The samples from the source. Raises: ValueError \u2013 If no samples are found in the dataset. Source code in webdataset/compat.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def check_empty ( source ): \"\"\"Check if the dataset is empty and yield samples. Args: source: An iterable source of samples. Yields: The samples from the source. Raises: ValueError: If no samples are found in the dataset. \"\"\" count = 0 for sample in source : yield sample count += 1 if count == 0 : raise ValueError ( \"No samples found in dataset; perhaps you have fewer shards than workers. \\n \" + \"Turn off using empty_check=False in the WebDataset constructor.\" ) webdataset.downloader RandomShardDownloader Download shards randomly from a source to a directory. This class can be run in two modes: - update_every: Keep filling the directory with shards until it contains nshards shards. - replace_every: Keep filling the directory with shards, removing a shard every polling period. Parameters: shards ( list ) \u2013 List of shard URLs to download from. nshards ( int ) \u2013 Number of shards to maintain in the directory. directory ( str , default: None ) \u2013 Directory to download shards to. pattern ( str , default: '*.{tar,tgz,tar.gz}' ) \u2013 Glob pattern for matching shard files. increment ( int , default: 999999 ) \u2013 Maximum number of shards to add in one update. maxsize ( int , default: 999999999999 ) \u2013 Maximum total size of downloaded shards in bytes. verbose ( bool , default: False ) \u2013 Whether to print verbose output. download ( function , default: None ) \u2013 Custom download function to use. errors ( str , default: 'ignore' ) \u2013 Error handling strategy ('ignore', 'warn', or 'fail'). Raises: AssertionError \u2013 If a shard filename doesn't match the given pattern. Source code in webdataset/downloader.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 class RandomShardDownloader : \"\"\"Download shards randomly from a source to a directory. This class can be run in two modes: - update_every: Keep filling the directory with shards until it contains nshards shards. - replace_every: Keep filling the directory with shards, removing a shard every polling period. Args: shards (list): List of shard URLs to download from. nshards (int): Number of shards to maintain in the directory. directory (str, optional): Directory to download shards to. pattern (str, optional): Glob pattern for matching shard files. increment (int, optional): Maximum number of shards to add in one update. maxsize (int, optional): Maximum total size of downloaded shards in bytes. verbose (bool, optional): Whether to print verbose output. download (function, optional): Custom download function to use. errors (str, optional): Error handling strategy ('ignore', 'warn', or 'fail'). Raises: AssertionError: If a shard filename doesn't match the given pattern. \"\"\" def __init__ ( self , shards , nshards , * , directory = None , pattern = \"*.{tar,tgz,tar.gz}\" , increment = 999999 , maxsize = 999999999999 , verbose = False , download = None , errors = \"ignore\" , # ignore, warn, fail ): \"\"\"Initialize the downloader with the given parameters.\"\"\" self . shards = shards self . directory = directory self . nshards = nshards self . pattern = pattern self . increment = increment self . errors = errors self . maxsize = maxsize self . verbose = verbose if isinstance ( download , str ): download = download_with ( download ) self . download = download or download_file # check that the file name components of the shards match the glob pattern; use fnmatch for shard in shards : assert fnmatch_with_braces ( os . path . basename ( shard ), pattern ), f \"shard { os . path . basename ( shard ) } does not match pattern { pattern } \" def list_files ( self , inactive = False ): \"\"\"List files in the download directory matching the given pattern. Args: inactive (bool, optional): Whether to include only inactive (non-temporary) files. Returns: list: A list of file paths matching the pattern. \"\"\" files = glob_with_braces ( os . path . join ( self . directory , self . pattern )) if not inactive : tempfiles = glob_with_braces ( os . path . join ( self . directory , \"*._*_\" )) files += [ file_of_tempfile ( tempfile ) for tempfile in tempfiles ] return list ( set ( files )) def set_directory ( self , directory ): \"\"\"Set the directory to download shards to. Args: directory (str): The path to the download directory. \"\"\" self . directory = directory def update ( self ): \"\"\"Download shards randomly from the source to the directory. Ensures that there are nshards shards in the directory. If there are fewer, download random shards from the source. Raises: RuntimeError: If unable to download the required number of shards. \"\"\" assert self . directory is not None , \"directory must be set\" files = self . list_files () start = len ( files ) for _ in range ( 10 * self . nshards ): files = self . list_files () total_size = total_file_size ( files ) if ( len ( files ) >= min ( self . nshards , start + self . increment ) or total_size > self . maxsize ): return shard = random . choice ( self . shards ) filename = os . path . basename ( shard ) if filename in files : continue tempdest = os . path . join ( self . directory , filename + f \"._ { os . getpid () } _\" ) if self . verbose : print ( f \"downloading { shard } to { tempdest } \" , file = sys . stderr ) try : self . download ( shard , tempdest ) except Exception as exn : print ( f \"download failed: { exn } \" , file = sys . stderr ) if self . errors == \"ignore\" : continue if self . errors == \"warn\" : print ( f \"ignoring error { exn } \" , file = sys . stderr ) continue raise try : os . rename ( tempdest , os . path . join ( self . directory , filename )) except FileExistsError : # some other process downloaded the same file os . unlink ( tempdest ) raise RuntimeError ( f \"unable to download { self . nshards } shards\" ) def sleep ( self , poll = 10 ): \"\"\"Sleep for a randomized duration based on the poll interval. Args: poll (float, optional): The base polling interval in seconds. \"\"\" delta = poll * random . uniform ( 0.7 , 1.3 ) time . sleep ( delta ) def update_every ( self , poll = 10 ): \"\"\"Repeatedly call update with a given delay. Args: poll (float, optional): The polling interval in seconds. \"\"\" while True : self . update () delta = poll * random . uniform ( 0.7 , 1.3 ) time . sleep ( delta ) def maybe_remove ( self , strategy = \"oldest\" ): \"\"\"Attempt to remove a shard if the number of shards exceeds the limit. Args: strategy (str, optional): The strategy for selecting which file to remove ('oldest' or 'random'). Returns: bool: True if a file was removed, False otherwise. Raises: ValueError: If an unknown strategy is provided. \"\"\" files = self . list_files () if len ( files ) > self . nshards : inactive = self . list_files ( inactive = True ) if len ( inactive ) == 0 : return False if strategy == \"oldest\" : selected = get_oldest_file ( inactive ) elif strategy == \"random\" : selected = random . choice ( inactive ) else : raise ValueError ( f \"unknown strategy { strategy } \" ) try : os . unlink ( selected ) return True except FileNotFoundError : return False return False def replace_every ( self , poll = 60 , strategy = \"oldest\" ): \"\"\"Repeatedly update and remove shards to maintain the desired number. Args: poll (float, optional): The polling interval in seconds. strategy (str, optional): The strategy for selecting which file to remove. \"\"\" while len ( self . list_files ()) >= self . nshards : if self . maybe_remove ( strategy = strategy ): self . update () self . sleep ( poll ) def run_job ( self , poll = 10 , mode = \"update\" , strategy = \"oldest\" ): \"\"\"Run the downloader job in the specified mode. Args: poll (float, optional): The polling interval in seconds. mode (str, optional): The mode to run in ('update' or 'replace'). strategy (str, optional): The strategy for selecting which file to remove in replace mode. Raises: ValueError: If an unknown mode is provided. \"\"\" if mode == \"update\" : self . update_every ( poll ) elif mode == \"replace\" : self . replace_every ( poll , strategy = strategy ) else : raise ValueError ( f \"unknown mode { mode } \" ) __init__ ( shards , nshards , * , directory = None , pattern = '*.{tar,tgz,tar.gz}' , increment = 999999 , maxsize = 999999999999 , verbose = False , download = None , errors = 'ignore' ) Initialize the downloader with the given parameters. Source code in webdataset/downloader.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __init__ ( self , shards , nshards , * , directory = None , pattern = \"*.{tar,tgz,tar.gz}\" , increment = 999999 , maxsize = 999999999999 , verbose = False , download = None , errors = \"ignore\" , # ignore, warn, fail ): \"\"\"Initialize the downloader with the given parameters.\"\"\" self . shards = shards self . directory = directory self . nshards = nshards self . pattern = pattern self . increment = increment self . errors = errors self . maxsize = maxsize self . verbose = verbose if isinstance ( download , str ): download = download_with ( download ) self . download = download or download_file # check that the file name components of the shards match the glob pattern; use fnmatch for shard in shards : assert fnmatch_with_braces ( os . path . basename ( shard ), pattern ), f \"shard { os . path . basename ( shard ) } does not match pattern { pattern } \" list_files ( inactive = False ) List files in the download directory matching the given pattern. Parameters: inactive ( bool , default: False ) \u2013 Whether to include only inactive (non-temporary) files. Returns: list \u2013 A list of file paths matching the pattern. Source code in webdataset/downloader.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def list_files ( self , inactive = False ): \"\"\"List files in the download directory matching the given pattern. Args: inactive (bool, optional): Whether to include only inactive (non-temporary) files. Returns: list: A list of file paths matching the pattern. \"\"\" files = glob_with_braces ( os . path . join ( self . directory , self . pattern )) if not inactive : tempfiles = glob_with_braces ( os . path . join ( self . directory , \"*._*_\" )) files += [ file_of_tempfile ( tempfile ) for tempfile in tempfiles ] return list ( set ( files )) maybe_remove ( strategy = 'oldest' ) Attempt to remove a shard if the number of shards exceeds the limit. Parameters: strategy ( str , default: 'oldest' ) \u2013 The strategy for selecting which file to remove ('oldest' or 'random'). Returns: bool \u2013 True if a file was removed, False otherwise. Raises: ValueError \u2013 If an unknown strategy is provided. Source code in webdataset/downloader.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def maybe_remove ( self , strategy = \"oldest\" ): \"\"\"Attempt to remove a shard if the number of shards exceeds the limit. Args: strategy (str, optional): The strategy for selecting which file to remove ('oldest' or 'random'). Returns: bool: True if a file was removed, False otherwise. Raises: ValueError: If an unknown strategy is provided. \"\"\" files = self . list_files () if len ( files ) > self . nshards : inactive = self . list_files ( inactive = True ) if len ( inactive ) == 0 : return False if strategy == \"oldest\" : selected = get_oldest_file ( inactive ) elif strategy == \"random\" : selected = random . choice ( inactive ) else : raise ValueError ( f \"unknown strategy { strategy } \" ) try : os . unlink ( selected ) return True except FileNotFoundError : return False return False replace_every ( poll = 60 , strategy = 'oldest' ) Repeatedly update and remove shards to maintain the desired number. Parameters: poll ( float , default: 60 ) \u2013 The polling interval in seconds. strategy ( str , default: 'oldest' ) \u2013 The strategy for selecting which file to remove. Source code in webdataset/downloader.py 270 271 272 273 274 275 276 277 278 279 280 def replace_every ( self , poll = 60 , strategy = \"oldest\" ): \"\"\"Repeatedly update and remove shards to maintain the desired number. Args: poll (float, optional): The polling interval in seconds. strategy (str, optional): The strategy for selecting which file to remove. \"\"\" while len ( self . list_files ()) >= self . nshards : if self . maybe_remove ( strategy = strategy ): self . update () self . sleep ( poll ) run_job ( poll = 10 , mode = 'update' , strategy = 'oldest' ) Run the downloader job in the specified mode. Parameters: poll ( float , default: 10 ) \u2013 The polling interval in seconds. mode ( str , default: 'update' ) \u2013 The mode to run in ('update' or 'replace'). strategy ( str , default: 'oldest' ) \u2013 The strategy for selecting which file to remove in replace mode. Raises: ValueError \u2013 If an unknown mode is provided. Source code in webdataset/downloader.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def run_job ( self , poll = 10 , mode = \"update\" , strategy = \"oldest\" ): \"\"\"Run the downloader job in the specified mode. Args: poll (float, optional): The polling interval in seconds. mode (str, optional): The mode to run in ('update' or 'replace'). strategy (str, optional): The strategy for selecting which file to remove in replace mode. Raises: ValueError: If an unknown mode is provided. \"\"\" if mode == \"update\" : self . update_every ( poll ) elif mode == \"replace\" : self . replace_every ( poll , strategy = strategy ) else : raise ValueError ( f \"unknown mode { mode } \" ) set_directory ( directory ) Set the directory to download shards to. Parameters: directory ( str ) \u2013 The path to the download directory. Source code in webdataset/downloader.py 167 168 169 170 171 172 173 def set_directory ( self , directory ): \"\"\"Set the directory to download shards to. Args: directory (str): The path to the download directory. \"\"\" self . directory = directory sleep ( poll = 10 ) Sleep for a randomized duration based on the poll interval. Parameters: poll ( float , default: 10 ) \u2013 The base polling interval in seconds. Source code in webdataset/downloader.py 220 221 222 223 224 225 226 227 def sleep ( self , poll = 10 ): \"\"\"Sleep for a randomized duration based on the poll interval. Args: poll (float, optional): The base polling interval in seconds. \"\"\" delta = poll * random . uniform ( 0.7 , 1.3 ) time . sleep ( delta ) update () Download shards randomly from the source to the directory. Ensures that there are nshards shards in the directory. If there are fewer, download random shards from the source. Raises: RuntimeError \u2013 If unable to download the required number of shards. Source code in webdataset/downloader.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def update ( self ): \"\"\"Download shards randomly from the source to the directory. Ensures that there are nshards shards in the directory. If there are fewer, download random shards from the source. Raises: RuntimeError: If unable to download the required number of shards. \"\"\" assert self . directory is not None , \"directory must be set\" files = self . list_files () start = len ( files ) for _ in range ( 10 * self . nshards ): files = self . list_files () total_size = total_file_size ( files ) if ( len ( files ) >= min ( self . nshards , start + self . increment ) or total_size > self . maxsize ): return shard = random . choice ( self . shards ) filename = os . path . basename ( shard ) if filename in files : continue tempdest = os . path . join ( self . directory , filename + f \"._ { os . getpid () } _\" ) if self . verbose : print ( f \"downloading { shard } to { tempdest } \" , file = sys . stderr ) try : self . download ( shard , tempdest ) except Exception as exn : print ( f \"download failed: { exn } \" , file = sys . stderr ) if self . errors == \"ignore\" : continue if self . errors == \"warn\" : print ( f \"ignoring error { exn } \" , file = sys . stderr ) continue raise try : os . rename ( tempdest , os . path . join ( self . directory , filename )) except FileExistsError : # some other process downloaded the same file os . unlink ( tempdest ) raise RuntimeError ( f \"unable to download { self . nshards } shards\" ) update_every ( poll = 10 ) Repeatedly call update with a given delay. Parameters: poll ( float , default: 10 ) \u2013 The polling interval in seconds. Source code in webdataset/downloader.py 229 230 231 232 233 234 235 236 237 238 def update_every ( self , poll = 10 ): \"\"\"Repeatedly call update with a given delay. Args: poll (float, optional): The polling interval in seconds. \"\"\" while True : self . update () delta = poll * random . uniform ( 0.7 , 1.3 ) time . sleep ( delta ) download_file ( url , filename ) Download a file from a URL. Parameters: url ( str ) \u2013 The URL of the file to download. filename ( str ) \u2013 The local path where the file will be saved. Returns: \u2013 None Source code in webdataset/downloader.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def download_file ( url , filename ): \"\"\"Download a file from a URL. Args: url (str): The URL of the file to download. filename (str): The local path where the file will be saved. Returns: None \"\"\" with gopen . gopen ( url , \"rb\" ) as stream : with open ( filename , \"wb\" ) as out : while True : chunk = stream . read ( 1024 * 1024 ) if len ( chunk ) == 0 : break out . write ( chunk ) download_with ( command ) Create a download function using a custom command. Parameters: command ( str ) \u2013 The command to use for downloading, containing {url} and {output} placeholders. Returns: function \u2013 A function that takes a URL and filename as arguments and downloads the file. Source code in webdataset/downloader.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def download_with ( command ): \"\"\"Create a download function using a custom command. Args: command (str): The command to use for downloading, containing {url} and {output} placeholders. Returns: function: A function that takes a URL and filename as arguments and downloads the file. \"\"\" def download ( url , filename ): return subprocess . check_call ( command . format ( url = url , output = filename ), shell = True ) return download file_of_tempfile ( tempfile ) Get the original file name from a temporary file name. Parameters: tempfile ( str ) \u2013 The temporary file name. Returns: str \u2013 The original file name without the temporary suffix. Raises: AssertionError \u2013 If the tempfile doesn't end with '_' or doesn't contain a period. Source code in webdataset/downloader.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def file_of_tempfile ( tempfile ): \"\"\"Get the original file name from a temporary file name. Args: tempfile (str): The temporary file name. Returns: str: The original file name without the temporary suffix. Raises: AssertionError: If the tempfile doesn't end with '_' or doesn't contain a period. \"\"\" assert tempfile . endswith ( \"_\" ) and \".\" in tempfile return tempfile . rsplit ( \".\" , 1 )[ 0 ] get_oldest_file ( files ) Find the oldest file in a list of files. Parameters: files ( list ) \u2013 A list of file paths. Returns: str \u2013 The path of the oldest file. Source code in webdataset/downloader.py 87 88 89 90 91 92 93 94 95 96 def get_oldest_file ( files ): \"\"\"Find the oldest file in a list of files. Args: files (list): A list of file paths. Returns: str: The path of the oldest file. \"\"\" return min ( files , key = os . path . getmtime ) random_downloader ( shards , * , directory = None , nshards = 10 , command = None , pattern = '*.{tar,tgz,tar.gz}' , increment = 999999 , maxsize = 999999999999 , njobs = 1 , poll = 10 , mode = 'update' , errors = 'ignore' , verbose = False ) Start multiple jobs to download shards randomly from the given list of shards. Parameters: shards ( List [ str ] ) \u2013 List of shard URLs or patterns to download from. directory ( Optional [ str ] , default: None ) \u2013 Directory to download shards to. nshards ( int , default: 10 ) \u2013 Number of shards to maintain in the directory. command ( Optional [ str ] , default: None ) \u2013 Custom download command to use. pattern ( str , default: '*.{tar,tgz,tar.gz}' ) \u2013 Glob pattern for matching shard files. increment ( int , default: 999999 ) \u2013 Maximum number of shards to add in one update. maxsize ( int , default: 999999999999 ) \u2013 Maximum total size of downloaded shards in bytes. njobs ( int , default: 1 ) \u2013 Number of parallel download jobs to run. poll ( float , default: 10 ) \u2013 Polling interval in seconds. mode ( str , default: 'update' ) \u2013 Mode to run in ('update' or 'replace'). errors ( str , default: 'ignore' ) \u2013 Error handling strategy ('ignore', 'warn', or 'fail'). verbose ( bool , default: False ) \u2013 Whether to print verbose output. Raises: AssertionError \u2013 If the directory is not provided. Source code in webdataset/downloader.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 @app . command () def random_downloader ( shards : List [ str ], * , directory : Optional [ str ] = None , nshards : int = 10 , command : Optional [ str ] = None , pattern : str = \"*.{tar,tgz,tar.gz}\" , increment : int = 999999 , maxsize : int = 999999999999 , njobs : int = 1 , poll : float = 10 , mode : str = \"update\" , errors : str = \"ignore\" , verbose : bool = False , ): \"\"\"Start multiple jobs to download shards randomly from the given list of shards. Args: shards (List[str]): List of shard URLs or patterns to download from. directory (Optional[str]): Directory to download shards to. nshards (int): Number of shards to maintain in the directory. command (Optional[str]): Custom download command to use. pattern (str): Glob pattern for matching shard files. increment (int): Maximum number of shards to add in one update. maxsize (int): Maximum total size of downloaded shards in bytes. njobs (int): Number of parallel download jobs to run. poll (float): Polling interval in seconds. mode (str): Mode to run in ('update' or 'replace'). errors (str): Error handling strategy ('ignore', 'warn', or 'fail'). verbose (bool): Whether to print verbose output. Raises: AssertionError: If the directory is not provided. \"\"\" assert directory is not None shards = [ fname for shard in shards for fname in braceexpand . braceexpand ( shard )] print ( f \"got { len ( shards ) } shards\" , file = sys . stderr ) if njobs > 1 : pool = multiprocessing . Pool ( njobs ) for _ in range ( njobs ): pool . apply_async ( RandomShardDownloader ( shards , nshards , directory = directory , pattern = pattern , increment = increment , maxsize = maxsize , download = command , errors = errors , verbose = verbose , ) . run_job , kwds = dict ( poll = poll , mode = mode ), ) else : RandomShardDownloader ( shards , nshards , directory = directory , pattern = pattern , increment = increment , maxsize = maxsize , download = command , errors = errors , verbose = verbose , ) . run_job ( poll = poll , mode = mode ) total_file_size ( files ) Calculate the total size of a list of files. Parameters: files ( list ) \u2013 A list of file paths. Returns: int \u2013 The total size of all files in bytes. Source code in webdataset/downloader.py 59 60 61 62 63 64 65 66 67 68 def total_file_size ( files ): \"\"\"Calculate the total size of a list of files. Args: files (list): A list of file paths. Returns: int: The total size of all files in bytes. \"\"\" return sum ( os . path . getsize ( f ) for f in files ) webdataset.extradatasets Train PyTorch models directly from POSIX tar archive. Code works locally or over HTTP connections. MockDataset Bases: IterableDataset Create a mock dataset for performance testing and unit testing. Parameters: sample \u2013 The sample to be returned repeatedly. length ( int ) \u2013 The length of the mock dataset. Source code in webdataset/extradatasets.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class MockDataset ( IterableDataset ): \"\"\"Create a mock dataset for performance testing and unit testing. Args: sample: The sample to be returned repeatedly. length (int): The length of the mock dataset. \"\"\" def __init__ ( self , sample , length ): self . sample = sample self . length = length def __iter__ ( self ): \"\"\"Yield samples from the mock dataset. Returns: Iterator: An iterator that yields the same sample repeatedly. \"\"\" for _ in range ( self . length ): yield self . sample __iter__ () Yield samples from the mock dataset. Returns: Iterator \u2013 An iterator that yields the same sample repeatedly. Source code in webdataset/extradatasets.py 31 32 33 34 35 36 37 38 def __iter__ ( self ): \"\"\"Yield samples from the mock dataset. Returns: Iterator: An iterator that yields the same sample repeatedly. \"\"\" for _ in range ( self . length ): yield self . sample repeatedly Bases: IterableDataset , PipelineStage Repeatedly yield samples from a dataset. Parameters: source \u2013 The source dataset to repeat. nepochs ( int , default: None ) \u2013 Maximum number of epochs to repeat. nbatches ( int , default: None ) \u2013 Maximum number of batches to repeat. length ( int , default: None ) \u2013 Length of the repeated dataset. Source code in webdataset/extradatasets.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class repeatedly ( IterableDataset , PipelineStage ): \"\"\"Repeatedly yield samples from a dataset. Args: source: The source dataset to repeat. nepochs (int, optional): Maximum number of epochs to repeat. nbatches (int, optional): Maximum number of batches to repeat. length (int, optional): Length of the repeated dataset. \"\"\" def __init__ ( self , source , nepochs = None , nbatches = None , length = None ): self . source = source self . length = length self . nbatches = nbatches def invoke ( self , source ): \"\"\"Return an iterator that iterates repeatedly over a source. Args: source: The source dataset to repeat. Returns: Iterator: An iterator that repeatedly yields samples from the source. \"\"\" return utils . repeatedly ( source , nepochs = self . nepochs , nbatches = self . nbatches , ) invoke ( source ) Return an iterator that iterates repeatedly over a source. Parameters: source \u2013 The source dataset to repeat. Returns: Iterator \u2013 An iterator that repeatedly yields samples from the source. Source code in webdataset/extradatasets.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def invoke ( self , source ): \"\"\"Return an iterator that iterates repeatedly over a source. Args: source: The source dataset to repeat. Returns: Iterator: An iterator that repeatedly yields samples from the source. \"\"\" return utils . repeatedly ( source , nepochs = self . nepochs , nbatches = self . nbatches , ) with_epoch Bases: IterableDataset Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Parameters: dataset \u2013 The source IterableDataset. length ( int ) \u2013 Declared length of the dataset. Source code in webdataset/extradatasets.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class with_epoch ( IterableDataset ): \"\"\"Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Args: dataset: The source IterableDataset. length (int): Declared length of the dataset. \"\"\" def __init__ ( self , dataset , length ): super () . __init__ () self . length = length self . source = None def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None __getstate__ () Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict \u2013 A dictionary representing the pickled state of the dataset. Source code in webdataset/extradatasets.py 91 92 93 94 95 96 97 98 99 100 101 def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result invoke ( dataset ) Return an iterator over the dataset. This iterator returns as many samples as given by the length parameter. Parameters: dataset \u2013 The source dataset to iterate over. Yields: Sample \u2013 The next sample from the dataset. Source code in webdataset/extradatasets.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None with_length Bases: IterableDataset , PipelineStage Repeatedly yield samples from a dataset with a specified length. Parameters: dataset \u2013 The source dataset. length ( int ) \u2013 The stated length of the dataset. Source code in webdataset/extradatasets.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class with_length ( IterableDataset , PipelineStage ): \"\"\"Repeatedly yield samples from a dataset with a specified length. Args: dataset: The source dataset. length (int): The stated length of the dataset. \"\"\" def __init__ ( self , dataset , length ): super () . __init__ () self . dataset = dataset self . length = length def invoke ( self , dataset ): \"\"\"Return an iterator that iterates over the source dataset. Args: dataset: The source dataset to iterate over. Returns: Iterator: An iterator over the source dataset. \"\"\" return iter ( dataset ) def __len__ ( self ): \"\"\"Return the user specified length. Returns: int: The specified length of the dataset. \"\"\" return self . length __len__ () Return the user specified length. Returns: int \u2013 The specified length of the dataset. Source code in webdataset/extradatasets.py 153 154 155 156 157 158 159 def __len__ ( self ): \"\"\"Return the user specified length. Returns: int: The specified length of the dataset. \"\"\" return self . length invoke ( dataset ) Return an iterator that iterates over the source dataset. Parameters: dataset \u2013 The source dataset to iterate over. Returns: Iterator \u2013 An iterator over the source dataset. Source code in webdataset/extradatasets.py 142 143 144 145 146 147 148 149 150 151 def invoke ( self , dataset ): \"\"\"Return an iterator that iterates over the source dataset. Args: dataset: The source dataset to iterate over. Returns: Iterator: An iterator over the source dataset. \"\"\" return iter ( dataset ) webdataset.filters A collection of iterators for data transformations. These functions are plain iterator functions. You can find curried versions in webdataset.filters, and you can find IterableDataset wrappers in webdataset.processing. Cached Bases: PipelineStage A pipeline stage that caches its output. This stage will cache all samples that pass through it, allowing subsequent iterations to use the cached data instead of recomputing it. Source code in webdataset/filters.py 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 class Cached ( PipelineStage ): \"\"\" A pipeline stage that caches its output. This stage will cache all samples that pass through it, allowing subsequent iterations to use the cached data instead of recomputing it. \"\"\" def __init__ ( self ): \"\"\"Initialize the Cached pipeline stage.\"\"\" super () . __init__ () self . cached = None def run ( self , source ): \"\"\" Run the caching process on the input source. Args: source: Input data source to be cached. Yields: Samples from the source, caching them for future use. \"\"\" if self . cached is None : self . temp = [] for sample in source : self . temp . append ( sample ) yield sample self . cached = self . temp else : yield from self . cached __init__ () Initialize the Cached pipeline stage. Source code in webdataset/filters.py 1064 1065 1066 1067 def __init__ ( self ): \"\"\"Initialize the Cached pipeline stage.\"\"\" super () . __init__ () self . cached = None run ( source ) Run the caching process on the input source. Parameters: source \u2013 Input data source to be cached. Yields: \u2013 Samples from the source, caching them for future use. Source code in webdataset/filters.py 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 def run ( self , source ): \"\"\" Run the caching process on the input source. Args: source: Input data source to be cached. Yields: Samples from the source, caching them for future use. \"\"\" if self . cached is None : self . temp = [] for sample in source : self . temp . append ( sample ) yield sample self . cached = self . temp else : yield from self . cached FilterFunction Bases: object Helper class for currying pipeline stages. This class is used to create a curried function that can be pickled. Attributes: f \u2013 The function to be curried. args \u2013 Positional arguments for the function. kw \u2013 Keyword arguments for the function. Source code in webdataset/filters.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class FilterFunction ( object ): \"\"\" Helper class for currying pipeline stages. This class is used to create a curried function that can be pickled. Attributes: f: The function to be curried. args: Positional arguments for the function. kw: Keyword arguments for the function. \"\"\" def __init__ ( self , f , * args , ** kw ): \"\"\" Create a curried function. Args: f: The function to be curried. *args: Positional arguments for the function. **kw: Keyword arguments for the function. \"\"\" self . f = f self . args = args self . kw = kw def __call__ ( self , data ): \"\"\" Call the curried function with the given argument. Args: data: The data to be processed by the curried function. Returns: The result of calling the curried function with the given data and stored arguments. \"\"\" return self . f ( data , * self . args , ** self . kw ) def __str__ ( self ): \"\"\" Compute a string representation. Returns: str: A string representation of the FilterFunction object. \"\"\" return f \"< { self . f . __name__ } { self . args } { self . kw } >\" def __repr__ ( self ): \"\"\" Compute a string representation. Returns: str: A string representation of the FilterFunction object. \"\"\" return f \"< { self . f . __name__ } { self . args } { self . kw } >\" __call__ ( data ) Call the curried function with the given argument. Parameters: data \u2013 The data to be processed by the curried function. Returns: \u2013 The result of calling the curried function with the given data and stored arguments. Source code in webdataset/filters.py 58 59 60 61 62 63 64 65 66 67 68 def __call__ ( self , data ): \"\"\" Call the curried function with the given argument. Args: data: The data to be processed by the curried function. Returns: The result of calling the curried function with the given data and stored arguments. \"\"\" return self . f ( data , * self . args , ** self . kw ) __init__ ( f , * args , ** kw ) Create a curried function. Parameters: f \u2013 The function to be curried. *args \u2013 Positional arguments for the function. **kw \u2013 Keyword arguments for the function. Source code in webdataset/filters.py 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , f , * args , ** kw ): \"\"\" Create a curried function. Args: f: The function to be curried. *args: Positional arguments for the function. **kw: Keyword arguments for the function. \"\"\" self . f = f self . args = args self . kw = kw __repr__ () Compute a string representation. Returns: str \u2013 A string representation of the FilterFunction object. Source code in webdataset/filters.py 79 80 81 82 83 84 85 86 def __repr__ ( self ): \"\"\" Compute a string representation. Returns: str: A string representation of the FilterFunction object. \"\"\" return f \"< { self . f . __name__ } { self . args } { self . kw } >\" __str__ () Compute a string representation. Returns: str \u2013 A string representation of the FilterFunction object. Source code in webdataset/filters.py 70 71 72 73 74 75 76 77 def __str__ ( self ): \"\"\" Compute a string representation. Returns: str: A string representation of the FilterFunction object. \"\"\" return f \"< { self . f . __name__ } { self . args } { self . kw } >\" LMDBCached Bases: PipelineStage A pipeline stage that caches its output in an LMDB database. This stage will cache all samples that pass through it in an LMDB database, allowing subsequent iterations to use the cached data instead of recomputing it. Source code in webdataset/filters.py 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 class LMDBCached ( PipelineStage ): \"\"\" A pipeline stage that caches its output in an LMDB database. This stage will cache all samples that pass through it in an LMDB database, allowing subsequent iterations to use the cached data instead of recomputing it. \"\"\" def __init__ ( self , fname , map_size = 1e12 , pickler = pickle , chunksize = 500 ): \"\"\" Initialize the LMDBCached pipeline stage. Args: fname (str): Filename for the LMDB database. map_size (int): Maximum size database may grow to. pickler: Module to use for pickling (default is Python's pickle module). chunksize (int): Number of samples to write in each transaction. \"\"\" import lmdb self . db = lmdb . open ( fname , readonly = False , map_size = int ( map_size )) self . pickler = pickler self . chunksize = chunksize def is_complete ( self ): \"\"\" Check if the database is complete. Returns: bool: True if the database is complete, False otherwise. \"\"\" with self . db . begin ( write = False ) as txn : return txn . get ( b \"_\" ) is not None def add_samples ( self , samples ): \"\"\" Add samples to the database. Args: samples: Iterable of (key, sample) pairs to add to the database. \"\"\" with self . db . begin ( write = True ) as txn : for key , sample in samples : txn . put ( key . encode (), self . pickler . dumps ( sample )) def run ( self , source ): \"\"\" Run the caching process on the input source. If the database is complete, yield samples from the database. Otherwise, yield samples from the source and cache them in the database. Args: source: Input data source to be cached. Yields: Samples from the source or the database. \"\"\" if self . is_complete (): with self . db . begin ( write = False ) as txn : for key , value in txn . cursor (): if key == b \"_\" : continue yield self . pickler . loads ( value ) else : buffer = [] for i , sample in enumerate ( source ): key = ( isinstance ( sample , dict ) and sample . get ( \"__key__\" )) or str ( i ) buffer . append (( key , sample )) if len ( buffer ) >= self . chunksize : self . add_samples ( buffer ) buffer = [] yield sample if len ( buffer ) > 0 : self . add_samples ( buffer ) with self . db . begin ( write = True ) as txn : txn . put ( b \"_\" , b \"1\" ) __init__ ( fname , map_size = 1000000000000.0 , pickler = pickle , chunksize = 500 ) Initialize the LMDBCached pipeline stage. Parameters: fname ( str ) \u2013 Filename for the LMDB database. map_size ( int , default: 1000000000000.0 ) \u2013 Maximum size database may grow to. pickler \u2013 Module to use for pickling (default is Python's pickle module). chunksize ( int , default: 500 ) \u2013 Number of samples to write in each transaction. Source code in webdataset/filters.py 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 def __init__ ( self , fname , map_size = 1e12 , pickler = pickle , chunksize = 500 ): \"\"\" Initialize the LMDBCached pipeline stage. Args: fname (str): Filename for the LMDB database. map_size (int): Maximum size database may grow to. pickler: Module to use for pickling (default is Python's pickle module). chunksize (int): Number of samples to write in each transaction. \"\"\" import lmdb self . db = lmdb . open ( fname , readonly = False , map_size = int ( map_size )) self . pickler = pickler self . chunksize = chunksize add_samples ( samples ) Add samples to the database. Parameters: samples \u2013 Iterable of (key, sample) pairs to add to the database. Source code in webdataset/filters.py 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 def add_samples ( self , samples ): \"\"\" Add samples to the database. Args: samples: Iterable of (key, sample) pairs to add to the database. \"\"\" with self . db . begin ( write = True ) as txn : for key , sample in samples : txn . put ( key . encode (), self . pickler . dumps ( sample )) is_complete () Check if the database is complete. Returns: bool \u2013 True if the database is complete, False otherwise. Source code in webdataset/filters.py 1113 1114 1115 1116 1117 1118 1119 1120 1121 def is_complete ( self ): \"\"\" Check if the database is complete. Returns: bool: True if the database is complete, False otherwise. \"\"\" with self . db . begin ( write = False ) as txn : return txn . get ( b \"_\" ) is not None run ( source ) Run the caching process on the input source. If the database is complete, yield samples from the database. Otherwise, yield samples from the source and cache them in the database. Parameters: source \u2013 Input data source to be cached. Yields: \u2013 Samples from the source or the database. Source code in webdataset/filters.py 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 def run ( self , source ): \"\"\" Run the caching process on the input source. If the database is complete, yield samples from the database. Otherwise, yield samples from the source and cache them in the database. Args: source: Input data source to be cached. Yields: Samples from the source or the database. \"\"\" if self . is_complete (): with self . db . begin ( write = False ) as txn : for key , value in txn . cursor (): if key == b \"_\" : continue yield self . pickler . loads ( value ) else : buffer = [] for i , sample in enumerate ( source ): key = ( isinstance ( sample , dict ) and sample . get ( \"__key__\" )) or str ( i ) buffer . append (( key , sample )) if len ( buffer ) >= self . chunksize : self . add_samples ( buffer ) buffer = [] yield sample if len ( buffer ) > 0 : self . add_samples ( buffer ) with self . db . begin ( write = True ) as txn : txn . put ( b \"_\" , b \"1\" ) RestCurried Bases: object Helper class for currying pipeline stages. This class is used to create a curried function that can be pickled. Attributes: f \u2013 The function to be curried. Source code in webdataset/filters.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class RestCurried ( object ): \"\"\" Helper class for currying pipeline stages. This class is used to create a curried function that can be pickled. Attributes: f: The function to be curried. \"\"\" def __init__ ( self , f ): \"\"\" Store the function for future currying. Args: f: The function to be curried. \"\"\" self . f = f def __call__ ( self , * args , ** kw ): \"\"\" Curry with the given arguments. Args: *args: Positional arguments for the function. **kw: Keyword arguments for the function. Returns: FilterFunction: A FilterFunction object with the curried function and arguments. \"\"\" return FilterFunction ( self . f , * args , ** kw ) __call__ ( * args , ** kw ) Curry with the given arguments. Parameters: *args \u2013 Positional arguments for the function. **kw \u2013 Keyword arguments for the function. Returns: FilterFunction \u2013 A FilterFunction object with the curried function and arguments. Source code in webdataset/filters.py 108 109 110 111 112 113 114 115 116 117 118 119 def __call__ ( self , * args , ** kw ): \"\"\" Curry with the given arguments. Args: *args: Positional arguments for the function. **kw: Keyword arguments for the function. Returns: FilterFunction: A FilterFunction object with the curried function and arguments. \"\"\" return FilterFunction ( self . f , * args , ** kw ) __init__ ( f ) Store the function for future currying. Parameters: f \u2013 The function to be curried. Source code in webdataset/filters.py 99 100 101 102 103 104 105 106 def __init__ ( self , f ): \"\"\" Store the function for future currying. Args: f: The function to be curried. \"\"\" self . f = f detshuffle Bases: PipelineStage A deterministic shuffling stage for the pipeline. This class provides a reproducible shuffling mechanism based on a seed and epoch. Attributes: bufsize ( int ) \u2013 Size of the buffer for shuffling. initial ( int ) \u2013 Initial number of samples to collect before shuffling. seed ( int ) \u2013 Seed for the random number generator. epoch ( int ) \u2013 Current epoch number. Source code in webdataset/filters.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 class detshuffle ( PipelineStage ): \"\"\" A deterministic shuffling stage for the pipeline. This class provides a reproducible shuffling mechanism based on a seed and epoch. Attributes: bufsize (int): Size of the buffer for shuffling. initial (int): Initial number of samples to collect before shuffling. seed (int): Seed for the random number generator. epoch (int): Current epoch number. \"\"\" def __init__ ( self , bufsize = 1000 , initial = 100 , seed = 0 , epoch =- 1 ): \"\"\" Initialize the detshuffle stage. Args: bufsize (int): Size of the buffer for shuffling. initial (int): Initial number of samples to collect before shuffling. seed (int): Seed for the random number generator. epoch (int): Starting epoch number. \"\"\" self . bufsize = bufsize self . initial = initial self . seed = seed self . epoch = epoch def run ( self , src ): \"\"\" Run the shuffling process on the input source. Args: src: Input data source to be shuffled. Returns: Iterator: Shuffled data iterator. \"\"\" self . epoch += 1 rng = random . Random () rng . seed ( self . seed + self . epoch ) return _shuffle ( src , self . bufsize , self . initial , rng ) __init__ ( bufsize = 1000 , initial = 100 , seed = 0 , epoch =- 1 ) Initialize the detshuffle stage. Parameters: bufsize ( int , default: 1000 ) \u2013 Size of the buffer for shuffling. initial ( int , default: 100 ) \u2013 Initial number of samples to collect before shuffling. seed ( int , default: 0 ) \u2013 Seed for the random number generator. epoch ( int , default: -1 ) \u2013 Starting epoch number. Source code in webdataset/filters.py 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def __init__ ( self , bufsize = 1000 , initial = 100 , seed = 0 , epoch =- 1 ): \"\"\" Initialize the detshuffle stage. Args: bufsize (int): Size of the buffer for shuffling. initial (int): Initial number of samples to collect before shuffling. seed (int): Seed for the random number generator. epoch (int): Starting epoch number. \"\"\" self . bufsize = bufsize self . initial = initial self . seed = seed self . epoch = epoch run ( src ) Run the shuffling process on the input source. Parameters: src \u2013 Input data source to be shuffled. Returns: Iterator \u2013 Shuffled data iterator. Source code in webdataset/filters.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def run ( self , src ): \"\"\" Run the shuffling process on the input source. Args: src: Input data source to be shuffled. Returns: Iterator: Shuffled data iterator. \"\"\" self . epoch += 1 rng = random . Random () rng . seed ( self . seed + self . epoch ) return _shuffle ( src , self . bufsize , self . initial , rng ) compose ( * args ) Compose a sequence of functions (left-to-right). Parameters: *args \u2013 Functions to be composed. Returns: function \u2013 A new function that applies all input functions in sequence. Source code in webdataset/filters.py 177 178 179 180 181 182 183 184 185 186 187 def compose ( * args ): \"\"\" Compose a sequence of functions (left-to-right). Args: *args: Functions to be composed. Returns: function: A new function that applies all input functions in sequence. \"\"\" return reduce ( compose2 , args ) compose2 ( f , g ) Compose two functions, g(f(x)). Parameters: f \u2013 The first function to be composed. g \u2013 The second function to be composed. Returns: function \u2013 A new function that applies f and then g to its input. Source code in webdataset/filters.py 163 164 165 166 167 168 169 170 171 172 173 174 def compose2 ( f , g ): \"\"\" Compose two functions, g(f(x)). Args: f: The first function to be composed. g: The second function to be composed. Returns: function: A new function that applies f and then g to its input. \"\"\" return lambda x : g ( f ( x )) decode_bin ( stream ) Decode binary data from a stream. Parameters: stream \u2013 A file-like object containing binary data. Returns: bytes \u2013 The binary data read from the stream. Source code in webdataset/filters.py 941 942 943 944 945 946 947 948 949 950 951 def decode_bin ( stream ): \"\"\" Decode binary data from a stream. Args: stream: A file-like object containing binary data. Returns: bytes: The binary data read from the stream. \"\"\" return stream . read () decode_pickle ( stream ) Decode pickle data from a stream. Parameters: stream \u2013 A file-like object containing pickle data. Returns: \u2013 The unpickled object. Source code in webdataset/filters.py 968 969 970 971 972 973 974 975 976 977 978 def decode_pickle ( stream ): \"\"\" Decode pickle data from a stream. Args: stream: A file-like object containing pickle data. Returns: The unpickled object. \"\"\" return pickle . load ( stream ) decode_text ( stream ) Decode text data from a stream. Parameters: stream \u2013 A file-like object containing text data. Returns: str \u2013 The decoded text data. Source code in webdataset/filters.py 954 955 956 957 958 959 960 961 962 963 964 965 def decode_text ( stream ): \"\"\" Decode text data from a stream. Args: stream: A file-like object containing text data. Returns: str: The decoded text data. \"\"\" binary = stream . read () return binary . decode ( \"utf-8\" ) default_collation_fn ( samples , combine_tensors = True , combine_scalars = True ) Take a collection of samples (dictionaries) and create a batch. Parameters: samples ( list ) \u2013 List of samples to be batched. combine_tensors ( bool , default: True ) \u2013 Whether to combine tensor-like objects into batches. combine_scalars ( bool , default: True ) \u2013 Whether to combine scalar values into numpy arrays. Returns: list \u2013 A batch of samples. Source code in webdataset/filters.py 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 def default_collation_fn ( samples , combine_tensors = True , combine_scalars = True ): \"\"\" Take a collection of samples (dictionaries) and create a batch. Args: samples (list): List of samples to be batched. combine_tensors (bool): Whether to combine tensor-like objects into batches. combine_scalars (bool): Whether to combine scalar values into numpy arrays. Returns: list: A batch of samples. \"\"\" assert isinstance ( samples [ 0 ], ( list , tuple )), type ( samples [ 0 ]) batched = list ( zip ( * samples )) result = [] for b in batched : if isinstance ( b [ 0 ], ( int , float )): if combine_scalars : b = np . array ( list ( b )) elif isinstance ( b [ 0 ], TorchTensor ): if combine_tensors : import torch b = torch . stack ( list ( b )) elif isinstance ( b [ 0 ], np . ndarray ): if combine_tensors : b = np . array ( list ( b )) else : b = list ( b ) result . append ( b ) return result find_decoder ( decoders , path ) Find the appropriate decoder for a given path. Parameters: decoders \u2013 List of (pattern, decoder_function) pairs. path \u2013 The path to find a decoder for. Returns: callable \u2013 The decoder function for the given path, or None if no match is found. Source code in webdataset/filters.py 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 def find_decoder ( decoders , path ): \"\"\" Find the appropriate decoder for a given path. Args: decoders: List of (pattern, decoder_function) pairs. path: The path to find a decoder for. Returns: callable: The decoder function for the given path, or None if no match is found. \"\"\" fname = re . sub ( r \".*/\" , \"\" , path ) if fname . startswith ( \"__\" ): return lambda x : x for pattern , fun in decoders [:: - 1 ]: if fnmatch ( fname . lower (), pattern ) or fnmatch ( \".\" + fname . lower (), pattern ): return fun return None getfirst ( a , keys , default = None , missing_is_error = True ) Get the first matching key from a dictionary. Keys can be specified as a list, or as a string of keys separated by ';'. Parameters: a ( dict ) \u2013 The dictionary to search. keys ( str or list ) \u2013 The keys to search for. default \u2013 The default value to return if no key is found. missing_is_error ( bool , default: True ) \u2013 If True, raise an error when no key is found. Returns: \u2013 The value of the first matching key found in the dictionary. Raises: ValueError \u2013 If no matching key is found and missing_is_error is True. Source code in webdataset/filters.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def getfirst ( a , keys , default = None , missing_is_error = True ): \"\"\" Get the first matching key from a dictionary. Keys can be specified as a list, or as a string of keys separated by ';'. Args: a (dict): The dictionary to search. keys (str or list): The keys to search for. default: The default value to return if no key is found. missing_is_error (bool): If True, raise an error when no key is found. Returns: The value of the first matching key found in the dictionary. Raises: ValueError: If no matching key is found and missing_is_error is True. \"\"\" if isinstance ( keys , str ): assert \" \" not in keys keys = keys . split ( \";\" ) for k in keys : if k in a : return a [ k ] if missing_is_error : raise ValueError ( f \"didn't find { keys } in { list ( a . keys ()) } \" ) return default identity ( x ) Return the argument unchanged. Parameters: x \u2013 The input value. Returns: \u2013 The input value unchanged. Source code in webdataset/filters.py 150 151 152 153 154 155 156 157 158 159 160 def identity ( x ): \"\"\" Return the argument unchanged. Args: x: The input value. Returns: The input value unchanged. \"\"\" return x parse_field_spec ( fields ) Parse a specification for a list of fields to be extracted. Keys are separated by spaces in the spec. Each key can itself be composed of key alternatives separated by ';'. Parameters: fields ( str or list ) \u2013 The field specification to parse. Returns: list \u2013 A list of parsed field specifications. Source code in webdataset/filters.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def parse_field_spec ( fields ): \"\"\" Parse a specification for a list of fields to be extracted. Keys are separated by spaces in the spec. Each key can itself be composed of key alternatives separated by ';'. Args: fields (str or list): The field specification to parse. Returns: list: A list of parsed field specifications. \"\"\" if isinstance ( fields , str ): fields = fields . split () return [ field . split ( \";\" ) for field in fields ] pick ( buf , rng ) Pick a random item from the buffer and remove it. Parameters: buf ( list ) \u2013 The buffer to pick from. rng \u2013 Random number generator. Returns: \u2013 The randomly picked item. Source code in webdataset/filters.py 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def pick ( buf , rng ): \"\"\" Pick a random item from the buffer and remove it. Args: buf (list): The buffer to pick from. rng: Random number generator. Returns: The randomly picked item. \"\"\" k = rng . randint ( 0 , len ( buf ) - 1 ) sample = buf [ k ] buf [ k ] = buf [ - 1 ] buf . pop () return sample pipeline ( source , * args ) Write an input pipeline; first argument is source, rest are filters. Parameters: source \u2013 The data source for the pipeline. *args \u2013 Filters to be applied to the data. Returns: \u2013 The result of applying all filters to the source data. Source code in webdataset/filters.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def pipeline ( source , * args ): \"\"\" Write an input pipeline; first argument is source, rest are filters. Args: source: The data source for the pipeline. *args: Filters to be applied to the data. Returns: The result of applying all filters to the source data. \"\"\" if len ( args ) == 0 : return source return compose ( * args )( source ) pipelinefilter ( f ) Turn the decorated function into one that is partially applied for all arguments other than the first. Parameters: f \u2013 The function to be decorated. Returns: RestCurried \u2013 A RestCurried object that can be used to create a FilterFunction. Source code in webdataset/filters.py 122 123 124 125 126 127 128 129 130 131 132 133 134 def pipelinefilter ( f ): \"\"\" Turn the decorated function into one that is partially applied for all arguments other than the first. Args: f: The function to be decorated. Returns: RestCurried: A RestCurried object that can be used to create a FilterFunction. \"\"\" result = RestCurried ( f ) functools . update_wrapper ( result , f ) return result reraise_exception ( exn ) Reraise the given exception. Parameters: exn \u2013 The exception to be reraised. Source code in webdataset/filters.py 137 138 139 140 141 142 143 144 145 146 147 def reraise_exception ( exn ): \"\"\" Reraise the given exception. Args: exn: The exception to be reraised. Raises: The input exception. \"\"\" raise exn transform_with ( sample , transformers ) Transform a list of values using a list of functions. If there are fewer transformers than inputs, or if a transformer function is None, then the identity function is used for the corresponding sample fields. Parameters: sample ( list ) \u2013 List of values to transform. transformers ( list ) \u2013 List of functions to apply to the sample. Returns: list \u2013 The transformed sample. Source code in webdataset/filters.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def transform_with ( sample , transformers ): \"\"\" Transform a list of values using a list of functions. If there are fewer transformers than inputs, or if a transformer function is None, then the identity function is used for the corresponding sample fields. Args: sample (list): List of values to transform. transformers (list): List of functions to apply to the sample. Returns: list: The transformed sample. \"\"\" if transformers is None or len ( transformers ) == 0 : return sample result = list ( sample ) assert len ( transformers ) <= len ( sample ) for i in range ( len ( transformers )): # skipcq: PYL-C0200 f = transformers [ i ] if f is not None : result [ i ] = f ( sample [ i ]) return result webdataset.handlers Pluggable exception handlers. These are functions that take an exception as an argument and then return... the exception (in order to re-raise it) True (in order to continue and ignore the exception) False (in order to ignore the exception and stop processing) They are used as handler= arguments in much of the library. ignore_and_continue ( exn ) Ignore the exception and continue processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 34 35 36 37 38 39 40 41 42 43 def ignore_and_continue ( exn ): \"\"\"Ignore the exception and continue processing. Args: exn: The exception to be ignored. Returns: bool: Always returns True to indicate continuation. \"\"\" return True ignore_and_stop ( exn ) Ignore the exception and stop further processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 60 61 62 63 64 65 66 67 68 69 def ignore_and_stop ( exn ): \"\"\"Ignore the exception and stop further processing. Args: exn: The exception to be ignored. Returns: bool: Always returns False to indicate stopping. \"\"\" return False reraise_exception ( exn ) Re-raise the given exception. Parameters: exn \u2013 The exception to be re-raised. Source code in webdataset/handlers.py 22 23 24 25 26 27 28 29 30 31 def reraise_exception ( exn ): \"\"\"Re-raise the given exception. Args: exn: The exception to be re-raised. Raises: The input exception. \"\"\" raise exn warn_and_continue ( exn ) Issue a warning for the exception and continue processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 46 47 48 49 50 51 52 53 54 55 56 57 def warn_and_continue ( exn ): \"\"\"Issue a warning for the exception and continue processing. Args: exn: The exception to be warned about. Returns: bool: Always returns True to indicate continuation. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return True warn_and_stop ( exn ) Issue a warning for the exception and stop further processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 72 73 74 75 76 77 78 79 80 81 82 83 def warn_and_stop ( exn ): \"\"\"Issue a warning for the exception and stop further processing. Args: exn: The exception to be warned about. Returns: bool: Always returns False to indicate stopping. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return False webdataset.pipeline DataPipeline Bases: IterableDataset , PipelineStage A pipeline starting with an IterableDataset and a series of filters. Parameters: *args \u2013 Variable length argument list of pipeline stages. **kwargs \u2013 Arbitrary keyword arguments. Source code in webdataset/pipeline.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class DataPipeline ( IterableDataset , PipelineStage ): \"\"\"A pipeline starting with an IterableDataset and a series of filters. Args: *args: Variable length argument list of pipeline stages. **kwargs: Arbitrary keyword arguments. \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ () self . pipeline = [] self . length = - 1 self . repetitions = 1 self . nsamples = - 1 for arg in args : if arg is None : continue if isinstance ( arg , list ): self . pipeline . extend ( arg ) else : self . pipeline . append ( arg ) def close ( self ): \"\"\"Close the pipeline and release resources.\"\"\" for step in self . pipeline : if hasattr ( step , \"close\" ): step . close () del self . pipeline def invoke ( self , f , * args , ** kwargs ): \"\"\"Apply a pipeline stage, possibly to the output of a previous stage. Args: f: The pipeline stage to invoke. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. Returns: The result of invoking the pipeline stage. Raises: ValueError: If the pipeline stage is not valid. \"\"\" if isinstance ( f , PipelineStage ): return f . run ( * args , ** kwargs ) if isinstance ( f , ( IterableDataset , DataLoader )) and len ( args ) == 0 : return iter ( f ) if isinstance ( f , list ): return iter ( f ) if callable ( f ): result = f ( * args , ** kwargs ) return result raise ValueError ( f \" { f } : not a valid pipeline stage\" ) def iterator1 ( self ): \"\"\"Create an iterator through one epoch in the pipeline. Returns: An iterator for one epoch of the pipeline. \"\"\" source = self . invoke ( self . pipeline [ 0 ]) for step in self . pipeline [ 1 :]: source = self . invoke ( step , source ) return source def iterator ( self ): \"\"\"Create an iterator through the entire dataset, using the given number of repetitions. Yields: Samples from the dataset. \"\"\" for _ in range ( self . repetitions ): count = 0 for sample in self . iterator1 (): yield sample count += 1 if count == 0 : # if the dataset is empty, don't keep looping break def __iter__ ( self ): \"\"\"Create an iterator through the pipeline, repeating and slicing as requested. Returns: An iterator through the pipeline. \"\"\" if self . repetitions != 1 : if self . nsamples > 0 : return islice ( self . iterator (), self . nsamples ) else : return self . iterator () else : return self . iterator () def stage ( self , i ): \"\"\"Return pipeline stage i. Args: i: The index of the pipeline stage to return. Returns: The pipeline stage at index i. \"\"\" return self . pipeline [ i ] def append ( self , f ): \"\"\"Append a pipeline stage (modifies the object). Args: f: The pipeline stage to append. \"\"\" self . pipeline . append ( f ) def compose ( self , * args ): \"\"\"Append pipeline stages to a copy of the pipeline and return the copy. Args: *args: Variable length argument list of pipeline stages to append. Returns: A new DataPipeline object with the appended stages. \"\"\" result = copy . copy ( self ) result . pipeline = copy . copy ( result . pipeline ) for arg in args : result . append ( arg ) return result def with_length ( self , n , silent = False ): \"\"\"Add a __len__ method returning the desired value. This does not change the actual number of samples in an epoch. PyTorch IterableDataset should not have a __len__ method. This is provided only as a workaround for some broken training environments that require a __len__ method. Args: n: The length value to set. silent: If True, suppress the warning message. Returns: The modified DataPipeline object with a __len__ method. \"\"\" if not silent : warnings . warn ( \".with_length() only sets the value of __len__ for compatibility with some training environments. It does not change the number of samples in an epoch.\" ) self . size = n return add_length_method ( self ) def with_epoch ( self , nsamples =- 1 , nbatches =- 1 ): \"\"\"Change the epoch to return the given number of samples/batches. Args: nsamples: The number of samples per epoch. nbatches: The number of batches per epoch. Returns: The modified DataPipeline object. \"\"\" self . repetitions = sys . maxsize self . nsamples = max ( nsamples , nbatches ) return self def repeat ( self , nepochs =- 1 , nbatches =- 1 ): \"\"\"Repeat iterating through the dataset for the given number of epochs up to the given number of samples. Args: nepochs: The number of epochs to repeat. nbatches: The number of batches to limit per repetition. Returns: The modified DataPipeline object. \"\"\" if nepochs > 0 : self . repetitions = nepochs self . nsamples = nbatches else : self . repetitions = sys . maxsize self . nsamples = nbatches return self __iter__ () Create an iterator through the pipeline, repeating and slicing as requested. Returns: \u2013 An iterator through the pipeline. Source code in webdataset/pipeline.py 111 112 113 114 115 116 117 118 119 120 121 122 123 def __iter__ ( self ): \"\"\"Create an iterator through the pipeline, repeating and slicing as requested. Returns: An iterator through the pipeline. \"\"\" if self . repetitions != 1 : if self . nsamples > 0 : return islice ( self . iterator (), self . nsamples ) else : return self . iterator () else : return self . iterator () append ( f ) Append a pipeline stage (modifies the object). Parameters: f \u2013 The pipeline stage to append. Source code in webdataset/pipeline.py 136 137 138 139 140 141 142 def append ( self , f ): \"\"\"Append a pipeline stage (modifies the object). Args: f: The pipeline stage to append. \"\"\" self . pipeline . append ( f ) close () Close the pipeline and release resources. Source code in webdataset/pipeline.py 53 54 55 56 57 58 def close ( self ): \"\"\"Close the pipeline and release resources.\"\"\" for step in self . pipeline : if hasattr ( step , \"close\" ): step . close () del self . pipeline compose ( * args ) Append pipeline stages to a copy of the pipeline and return the copy. Parameters: *args \u2013 Variable length argument list of pipeline stages to append. Returns: \u2013 A new DataPipeline object with the appended stages. Source code in webdataset/pipeline.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def compose ( self , * args ): \"\"\"Append pipeline stages to a copy of the pipeline and return the copy. Args: *args: Variable length argument list of pipeline stages to append. Returns: A new DataPipeline object with the appended stages. \"\"\" result = copy . copy ( self ) result . pipeline = copy . copy ( result . pipeline ) for arg in args : result . append ( arg ) return result invoke ( f , * args , ** kwargs ) Apply a pipeline stage, possibly to the output of a previous stage. Parameters: f \u2013 The pipeline stage to invoke. *args \u2013 Variable length argument list. **kwargs \u2013 Arbitrary keyword arguments. Returns: \u2013 The result of invoking the pipeline stage. Raises: ValueError \u2013 If the pipeline stage is not valid. Source code in webdataset/pipeline.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def invoke ( self , f , * args , ** kwargs ): \"\"\"Apply a pipeline stage, possibly to the output of a previous stage. Args: f: The pipeline stage to invoke. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. Returns: The result of invoking the pipeline stage. Raises: ValueError: If the pipeline stage is not valid. \"\"\" if isinstance ( f , PipelineStage ): return f . run ( * args , ** kwargs ) if isinstance ( f , ( IterableDataset , DataLoader )) and len ( args ) == 0 : return iter ( f ) if isinstance ( f , list ): return iter ( f ) if callable ( f ): result = f ( * args , ** kwargs ) return result raise ValueError ( f \" { f } : not a valid pipeline stage\" ) iterator () Create an iterator through the entire dataset, using the given number of repetitions. Yields: \u2013 Samples from the dataset. Source code in webdataset/pipeline.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def iterator ( self ): \"\"\"Create an iterator through the entire dataset, using the given number of repetitions. Yields: Samples from the dataset. \"\"\" for _ in range ( self . repetitions ): count = 0 for sample in self . iterator1 (): yield sample count += 1 if count == 0 : # if the dataset is empty, don't keep looping break iterator1 () Create an iterator through one epoch in the pipeline. Returns: \u2013 An iterator for one epoch of the pipeline. Source code in webdataset/pipeline.py 85 86 87 88 89 90 91 92 93 94 def iterator1 ( self ): \"\"\"Create an iterator through one epoch in the pipeline. Returns: An iterator for one epoch of the pipeline. \"\"\" source = self . invoke ( self . pipeline [ 0 ]) for step in self . pipeline [ 1 :]: source = self . invoke ( step , source ) return source repeat ( nepochs =- 1 , nbatches =- 1 ) Repeat iterating through the dataset for the given number of epochs up to the given number of samples. Parameters: nepochs \u2013 The number of epochs to repeat. nbatches \u2013 The number of batches to limit per repetition. Returns: \u2013 The modified DataPipeline object. Source code in webdataset/pipeline.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def repeat ( self , nepochs =- 1 , nbatches =- 1 ): \"\"\"Repeat iterating through the dataset for the given number of epochs up to the given number of samples. Args: nepochs: The number of epochs to repeat. nbatches: The number of batches to limit per repetition. Returns: The modified DataPipeline object. \"\"\" if nepochs > 0 : self . repetitions = nepochs self . nsamples = nbatches else : self . repetitions = sys . maxsize self . nsamples = nbatches return self stage ( i ) Return pipeline stage i. Parameters: i \u2013 The index of the pipeline stage to return. Returns: \u2013 The pipeline stage at index i. Source code in webdataset/pipeline.py 125 126 127 128 129 130 131 132 133 134 def stage ( self , i ): \"\"\"Return pipeline stage i. Args: i: The index of the pipeline stage to return. Returns: The pipeline stage at index i. \"\"\" return self . pipeline [ i ] with_epoch ( nsamples =- 1 , nbatches =- 1 ) Change the epoch to return the given number of samples/batches. Parameters: nsamples \u2013 The number of samples per epoch. nbatches \u2013 The number of batches per epoch. Returns: \u2013 The modified DataPipeline object. Source code in webdataset/pipeline.py 179 180 181 182 183 184 185 186 187 188 189 190 191 def with_epoch ( self , nsamples =- 1 , nbatches =- 1 ): \"\"\"Change the epoch to return the given number of samples/batches. Args: nsamples: The number of samples per epoch. nbatches: The number of batches per epoch. Returns: The modified DataPipeline object. \"\"\" self . repetitions = sys . maxsize self . nsamples = max ( nsamples , nbatches ) return self with_length ( n , silent = False ) Add a len method returning the desired value. This does not change the actual number of samples in an epoch. PyTorch IterableDataset should not have a len method. This is provided only as a workaround for some broken training environments that require a len method. Parameters: n \u2013 The length value to set. silent \u2013 If True, suppress the warning message. Returns: \u2013 The modified DataPipeline object with a len method. Source code in webdataset/pipeline.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def with_length ( self , n , silent = False ): \"\"\"Add a __len__ method returning the desired value. This does not change the actual number of samples in an epoch. PyTorch IterableDataset should not have a __len__ method. This is provided only as a workaround for some broken training environments that require a __len__ method. Args: n: The length value to set. silent: If True, suppress the warning message. Returns: The modified DataPipeline object with a __len__ method. \"\"\" if not silent : warnings . warn ( \".with_length() only sets the value of __len__ for compatibility with some training environments. It does not change the number of samples in an epoch.\" ) self . size = n return add_length_method ( self ) add_length_method ( obj ) Add a length method to the given object. Parameters: obj \u2013 The object to which the length method will be added. Returns: \u2013 The modified object with a new length method. Source code in webdataset/pipeline.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def add_length_method ( obj ): \"\"\"Add a length method to the given object. Args: obj: The object to which the length method will be added. Returns: The modified object with a new length method. \"\"\" def length ( self ): return self . size Combined = type ( obj . __class__ . __name__ + \"_Length\" , ( obj . __class__ , IterableDataset ), { \"__len__\" : length }, ) obj . __class__ = Combined return obj webdataset.tariterators Low level iteration functions for tar archives. base_plus_ext ( path ) Split off all file extensions. Parameters: path \u2013 Path with extensions. Returns: \u2013 Tuple containing the base path and all extensions. Source code in webdataset/tariterators.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def base_plus_ext ( path ): \"\"\"Split off all file extensions. Args: path: Path with extensions. Returns: Tuple containing the base path and all extensions. \"\"\" match = re . match ( r \"^((?:.*/|)[^.]+)[.]([^/]*)$\" , path ) if not match : return None , None return match . group ( 1 ), match . group ( 2 ) group_by_keys ( data , keys = base_plus_ext , lcase = True , suffixes = None , handler = reraise_exception ) Group tarfile contents by keys and yield samples. Parameters: data ( Iterable [ Dict [ str , Any ]] ) \u2013 Iterator over tarfile contents. keys ( Callable [[ str ], Tuple [ str , str ]] , default: base_plus_ext ) \u2013 Function that takes a file name and returns a key and a suffix. lcase ( bool , default: True ) \u2013 Whether to lowercase the suffix. suffixes ( Optional [ Set [ str ]] , default: None ) \u2013 List of suffixes to keep. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. Raises: ValueError \u2013 If there are duplicate file names in the tar file. Yields: Dict [ str , Any ] \u2013 Iterator over samples. Source code in webdataset/tariterators.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def group_by_keys ( data : Iterable [ Dict [ str , Any ]], keys : Callable [[ str ], Tuple [ str , str ]] = base_plus_ext , lcase : bool = True , suffixes : Optional [ Set [ str ]] = None , handler : Callable [[ Exception ], bool ] = reraise_exception , ) -> Iterator [ Dict [ str , Any ]]: \"\"\"Group tarfile contents by keys and yield samples. Args: data: Iterator over tarfile contents. keys: Function that takes a file name and returns a key and a suffix. lcase: Whether to lowercase the suffix. suffixes: List of suffixes to keep. handler: Exception handler. Raises: ValueError: If there are duplicate file names in the tar file. Yields: Iterator over samples. \"\"\" current_sample = None for filesample in data : try : assert isinstance ( filesample , dict ) if filesample == {}: if valid_sample ( current_sample ): yield current_sample current_sample = None continue fname , value = filesample [ \"fname\" ], filesample [ \"data\" ] prefix , suffix = keys ( fname ) if trace : print ( prefix , suffix , current_sample . keys () if isinstance ( current_sample , dict ) else None , ) if prefix is None : continue if lcase : suffix = suffix . lower () if current_sample is None or prefix != current_sample [ \"__key__\" ]: if valid_sample ( current_sample ): yield current_sample current_sample = dict ( __key__ = prefix , __url__ = filesample [ \"__url__\" ]) if suffix in current_sample : raise ValueError ( f \" { fname } : duplicate file name in tar file { suffix } { current_sample . keys () } \" ) if suffixes is None or suffix in suffixes : current_sample [ suffix ] = value local_path = filesample . get ( \"__local_path__\" ) if local_path is not None : current_sample [ \"__local_path__\" ] = local_path except Exception as exn : exn . args = exn . args + ( filesample . get ( \"stream\" ), filesample . get ( \"url\" )) if handler ( exn ): continue else : break if valid_sample ( current_sample ): yield current_sample shardlist ( urls , * , shuffle = False ) Generate a list of URLs, possibly shuffled. Parameters: urls \u2013 A string or list of URLs. shuffle \u2013 Whether to shuffle the URLs. Yields: \u2013 Dictionary containing the URL. Source code in webdataset/tariterators.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def shardlist ( urls , * , shuffle = False ): \"\"\"Generate a list of URLs, possibly shuffled. Args: urls: A string or list of URLs. shuffle: Whether to shuffle the URLs. Yields: Dictionary containing the URL. \"\"\" if isinstance ( urls , str ): urls = braceexpand . braceexpand ( urls ) else : urls = list ( urls ) if shuffle : random . shuffle ( urls ) for url in urls : yield dict ( url = url ) tar_file_expander ( data , handler = reraise_exception , select_files = None , rename_files = None , eof_value = {}) Expand tar files. Parameters: data ( Iterable [ Dict [ str , Any ]] ) \u2013 Iterator over opened tar file streams. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. select_files ( Optional [ Callable [[ str ], bool ]] , default: None ) \u2013 Select files from tarfiles by name (permits skipping files). rename_files ( Optional [ Callable [[ str ], str ]] , default: None ) \u2013 Function to rename files. eof_value ( Optional [ Any ] , default: {} ) \u2013 Value to yield at the end of each shard. Yields: Dict [ str , Any ] \u2013 A stream of samples. Source code in webdataset/tariterators.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def tar_file_expander ( data : Iterable [ Dict [ str , Any ]], handler : Callable [[ Exception ], bool ] = reraise_exception , select_files : Optional [ Callable [[ str ], bool ]] = None , rename_files : Optional [ Callable [[ str ], str ]] = None , eof_value : Optional [ Any ] = {}, ) -> Iterator [ Dict [ str , Any ]]: \"\"\"Expand tar files. Args: data: Iterator over opened tar file streams. handler: Exception handler. select_files: Select files from tarfiles by name (permits skipping files). rename_files: Function to rename files. eof_value: Value to yield at the end of each shard. Yields: A stream of samples. \"\"\" for source in data : url = source [ \"url\" ] local_path = source . get ( \"local_path\" ) try : assert isinstance ( source , dict ) assert \"stream\" in source for sample in tar_file_iterator ( source [ \"stream\" ], handler = handler , select_files = select_files , rename_files = rename_files , ): assert ( isinstance ( sample , dict ) and \"data\" in sample and \"fname\" in sample ) sample [ \"__url__\" ] = url if local_path is not None : sample [ \"__local_path__\" ] = local_path yield sample # we yield an EOF marker at the end of each shard so that # samples from different shards don't get mixed up if eof_value is not None : yield eof_value except Exception as exn : exn . args = exn . args + ( source . get ( \"stream\" ), source . get ( \"url\" )) if handler ( exn ): continue else : break tar_file_iterator ( fileobj , skip_meta = '__[^/]*__($|/)' , handler = reraise_exception , select_files = None , rename_files = None ) Iterate over tar file, yielding filename, content pairs for the given tar stream. Parameters: fileobj ( TarFile ) \u2013 The tar file stream. skip_meta ( Optional [ str ] , default: '__[^/]*__($|/)' ) \u2013 Regexp for keys that are skipped entirely. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. select_files ( Optional [ Callable [[ str ], bool ]] , default: None ) \u2013 Predicate for selecting files. rename_files ( Optional [ Callable [[ str ], str ]] , default: None ) \u2013 Function to rename files. Yields: Dict [ str , Any ] \u2013 A stream of samples. Source code in webdataset/tariterators.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def tar_file_iterator ( fileobj : tarfile . TarFile , skip_meta : Optional [ str ] = r \"__[^/]*__($|/)\" , handler : Callable [[ Exception ], bool ] = reraise_exception , select_files : Optional [ Callable [[ str ], bool ]] = None , rename_files : Optional [ Callable [[ str ], str ]] = None , ) -> Iterator [ Dict [ str , Any ]]: \"\"\"Iterate over tar file, yielding filename, content pairs for the given tar stream. Args: fileobj: The tar file stream. skip_meta: Regexp for keys that are skipped entirely. handler: Exception handler. select_files: Predicate for selecting files. rename_files: Function to rename files. Yields: A stream of samples. \"\"\" stream = tarfile . open ( fileobj = fileobj , mode = \"r|*\" ) for tarinfo in stream : fname = tarinfo . name try : if not tarinfo . isreg (): continue if fname is None : continue if ( \"/\" not in fname and fname . startswith ( meta_prefix ) and fname . endswith ( meta_suffix ) ): # skipping metadata for now continue if skip_meta is not None and re . match ( skip_meta , fname ): continue if rename_files : fname = rename_files ( fname ) if select_files is not None and not select_files ( fname ): continue data = stream . extractfile ( tarinfo ) . read () result = dict ( fname = fname , data = data ) yield result stream . members = [] except Exception as exn : if hasattr ( exn , \"args\" ) and len ( exn . args ) > 0 : exn . args = ( str ( exn . args [ 0 ]) + \" @ \" + str ( fileobj ),) + exn . args [ 1 :] if handler ( exn ): continue else : break del stream tarfile_samples ( src , handler = reraise_exception , select_files = None , rename_files = None ) Generate samples from a stream of tar files. Parameters: src ( Iterable [ Dict [ str , Any ]] ) \u2013 Stream of tar files. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. select_files ( Optional [ Callable [[ str ], bool ]] , default: None ) \u2013 Function that selects files to be included. rename_files ( Optional [ Callable [[ str ], str ]] , default: None ) \u2013 Function to rename files. Returns: Iterable [ Dict [ str , Any ]] \u2013 Stream of samples. Source code in webdataset/tariterators.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def tarfile_samples ( src : Iterable [ Dict [ str , Any ]], handler : Callable [[ Exception ], bool ] = reraise_exception , select_files : Optional [ Callable [[ str ], bool ]] = None , rename_files : Optional [ Callable [[ str ], str ]] = None , ) -> Iterable [ Dict [ str , Any ]]: \"\"\"Generate samples from a stream of tar files. Args: src: Stream of tar files. handler: Exception handler. select_files: Function that selects files to be included. rename_files: Function to rename files. Returns: Stream of samples. \"\"\" streams = url_opener ( src , handler = handler ) files = tar_file_expander ( streams , handler = handler , select_files = select_files , rename_files = rename_files ) samples = group_by_keys ( files , handler = handler ) return samples url_opener ( data , handler = reraise_exception , ** kw ) Open URLs and yield a stream of url+stream pairs. Parameters: data ( Iterable [ Dict [ str , Any ]] ) \u2013 Iterator over dict(url=...). handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. **kw ( Dict [ str , Any ] , default: {} ) \u2013 Keyword arguments for gopen.gopen. Yields: \u2013 A stream of url+stream pairs. Source code in webdataset/tariterators.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def url_opener ( data : Iterable [ Dict [ str , Any ]], handler : Callable [[ Exception ], bool ] = reraise_exception , ** kw : Dict [ str , Any ], ): \"\"\"Open URLs and yield a stream of url+stream pairs. Args: data: Iterator over dict(url=...). handler: Exception handler. **kw: Keyword arguments for gopen.gopen. Yields: A stream of url+stream pairs. \"\"\" for sample in data : assert isinstance ( sample , dict ), sample assert \"url\" in sample url = sample [ \"url\" ] try : stream = gopen . gopen ( url , ** kw ) sample . update ( stream = stream ) yield sample except Exception as exn : exn . args = exn . args + ( url ,) if handler ( exn ): continue else : break valid_sample ( sample ) Check whether a sample is valid. Parameters: sample ( Dict [ str , Any ] ) \u2013 A dictionary representing a sample. Returns: bool \u2013 Boolean indicating whether the sample is valid. Source code in webdataset/tariterators.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def valid_sample ( sample : Dict [ str , Any ]) -> bool : \"\"\"Check whether a sample is valid. Args: sample: A dictionary representing a sample. Returns: Boolean indicating whether the sample is valid. \"\"\" return ( sample is not None and isinstance ( sample , dict ) and len ( list ( sample . keys ())) > 0 and not sample . get ( \"__bad__\" , False ) ) webdataset.writer Classes and functions for writing tar files and WebDataset files. ShardWriter Like TarWriter but splits into multiple shards. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. Defaults to 100000. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. Defaults to 3e9. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. Defaults to None. start_shard ( int , default: 0 ) \u2013 Starting shard number. Defaults to 0. verbose ( int , default: 1 ) \u2013 Verbosity level. Defaults to 1. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. Defaults to None. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 class ShardWriter : \"\"\"Like TarWriter but splits into multiple shards. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. Defaults to 100000. maxsize: Maximum size of each shard. Defaults to 3e9. post: Optional callable to be executed after each shard is written. Defaults to None. start_shard: Starting shard number. Defaults to 0. verbose: Verbosity level. Defaults to 1. opener: Optional callable to open output files. Defaults to None. **kw: Other options passed to TarWriter. \"\"\" def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream () def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close () __enter__ () Enter context. Returns: self \u2013 The ShardWriter object. Source code in webdataset/writer.py 593 594 595 596 597 598 599 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self __exit__ ( * args , ** kw ) Exit context. Source code in webdataset/writer.py 601 602 603 def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close () __init__ ( pattern , maxcount = 100000 , maxsize = 3000000000.0 , post = None , start_shard = 0 , verbose = 1 , opener = None , ** kw ) Create a ShardWriter. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. start_shard ( int , default: 0 ) \u2013 Starting shard number. verbose ( int , default: 1 ) \u2013 Verbosity level. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream () close () Close the stream. Source code in webdataset/writer.py 585 586 587 588 589 590 591 def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size finish () Finish all writing (use close instead). Source code in webdataset/writer.py 576 577 578 579 580 581 582 583 def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None next_stream () Close the current stream and move to the next. Source code in webdataset/writer.py 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0 write ( obj ) Write a sample. Parameters: obj \u2013 Sample to be written. Source code in webdataset/writer.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size TarWriter A class for writing dictionaries to tar files. Parameters: fileobj \u2013 File name for tar file (.tgz/.tar) or open file descriptor. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Sample encoding. Defaults to True. compress ( Optional [ bool ] , default: None ) \u2013 Compression flag. Defaults to None. user ( str , default: 'bigdata' ) \u2013 User for tar files. Defaults to \"bigdata\". group ( str , default: 'bigdata' ) \u2013 Group for tar files. Defaults to \"bigdata\". mode ( int , default: 292 ) \u2013 Mode for tar files. Defaults to 0o0444. keep_meta ( bool , default: False ) \u2013 Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime ( Optional [ float ] , default: None ) \u2013 Modification time. Defaults to None. format ( Any , default: None ) \u2013 Tar format. Defaults to None. Returns: \u2013 TarWriter object. Raises: ValueError \u2013 If the encoder doesn't yield bytes for a key. True will use an encoder that behaves similar to the automatic decoder for Dataset . False disables encoding and expects byte strings (except for metadata, which must be strings). The encoder argument can also be a callable , or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: a/b.png and a/b.output.png . tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) Source code in webdataset/writer.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 class TarWriter : \"\"\"A class for writing dictionaries to tar files. Args: fileobj: File name for tar file (.tgz/.tar) or open file descriptor. encoder: Sample encoding. Defaults to True. compress: Compression flag. Defaults to None. user: User for tar files. Defaults to \"bigdata\". group: Group for tar files. Defaults to \"bigdata\". mode: Mode for tar files. Defaults to 0o0444. keep_meta: Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime: Modification time. Defaults to None. format: Tar format. Defaults to None. Returns: TarWriter object. Raises: ValueError: If the encoder doesn't yield bytes for a key. `True` will use an encoder that behaves similar to the automatic decoder for `Dataset`. `False` disables encoding and expects byte strings (except for metadata, which must be strings). The `encoder` argument can also be a `callable`, or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: `a/b.png` and `a/b.output.png`. tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) \"\"\" def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ bool ] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime if isinstance ( fileobj , str ): if compress is False : tarmode = \"w|\" elif compress is True : tarmode = \"w|gz\" else : tarmode = \"w|gz\" if fileobj . endswith ( \"gz\" ) else \"w|\" fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : tarmode = \"w|gz\" if compress is True else \"w|\" self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close () def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total __enter__ () Enter context. Returns: self \u2013 The TarWriter object. Source code in webdataset/writer.py 421 422 423 424 425 426 427 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self __exit__ ( exc_type , exc_val , exc_tb ) Exit context. Source code in webdataset/writer.py 429 430 431 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close () __init__ ( fileobj , user = 'bigdata' , group = 'bigdata' , mode = 292 , compress = None , encoder = True , keep_meta = False , mtime = None , format = None ) Create a tar writer. Parameters: fileobj \u2013 Stream to write data to. user ( str , default: 'bigdata' ) \u2013 User for tar files. group ( str , default: 'bigdata' ) \u2013 Group for tar files. mode ( int , default: 292 ) \u2013 Mode for tar files. compress ( Optional [ bool ] , default: None ) \u2013 Desired compression. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Encoder function. keep_meta ( bool , default: False ) \u2013 Keep metadata (entries starting with \"_\"). mtime ( Optional [ float ] , default: None ) \u2013 Modification time (set this to some fixed value to get reproducible tar files). format ( Any , default: None ) \u2013 Tar format. Source code in webdataset/writer.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ bool ] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime if isinstance ( fileobj , str ): if compress is False : tarmode = \"w|\" elif compress is True : tarmode = \"w|gz\" else : tarmode = \"w|gz\" if fileobj . endswith ( \"gz\" ) else \"w|\" fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : tarmode = \"w|gz\" if compress is True else \"w|\" self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress close () Close the tar file. Source code in webdataset/writer.py 433 434 435 436 437 438 def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None write ( obj ) Write a dictionary to the tar file. Parameters: obj \u2013 Dictionary of objects to be stored. Returns: int \u2013 Size of the entry. Raises: ValueError \u2013 If the object doesn't contain a key or if a key doesn't map to bytes after encoding. Source code in webdataset/writer.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total add_handlers ( d , keys , value ) Add handlers to a dictionary for given keys. Parameters: d \u2013 Dictionary to add handlers to keys \u2013 String of space-separated keys or list of keys value \u2013 Handler function to be added Source code in webdataset/writer.py 193 194 195 196 197 198 199 200 201 202 203 204 def add_handlers ( d , keys , value ): \"\"\"Add handlers to a dictionary for given keys. Args: d: Dictionary to add handlers to keys: String of space-separated keys or list of keys value: Handler function to be added \"\"\" if isinstance ( keys , str ): keys = keys . split () for k in keys : d [ k ] = value bytestr ( data ) Convert data into a bytestring. Uses str and ASCII encoding for data that isn't already in string format. Parameters: data ( Any ) \u2013 Data to be converted Returns: bytes \u2013 Converted bytestring Source code in webdataset/writer.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def bytestr ( data : Any ): \"\"\"Convert data into a bytestring. Uses str and ASCII encoding for data that isn't already in string format. Args: data: Data to be converted Returns: bytes: Converted bytestring \"\"\" if isinstance ( data , bytes ): return data if isinstance ( data , str ): return data . encode ( \"ascii\" ) return str ( data ) . encode ( \"ascii\" ) cbor_dumps ( x ) Dump data into a bytestring using CBOR format. Parameters: x \u2013 Data to be dumped Returns: bytes \u2013 Dumped data as bytestring Source code in webdataset/writer.py 165 166 167 168 169 170 171 172 173 174 175 176 def cbor_dumps ( x ): \"\"\"Dump data into a bytestring using CBOR format. Args: x: Data to be dumped Returns: bytes: Dumped data as bytestring \"\"\" import cbor return cbor . dumps ( x ) encode_based_on_extension ( sample , handlers ) Encode an entire sample with a collection of handlers. Parameters: sample ( dict ) \u2013 Data sample (a dict) handlers ( dict ) \u2013 Handlers for encoding Returns: dict \u2013 Encoded sample Source code in webdataset/writer.py 276 277 278 279 280 281 282 283 284 285 286 287 288 def encode_based_on_extension ( sample : dict , handlers : dict ): \"\"\"Encode an entire sample with a collection of handlers. Args: sample: Data sample (a dict) handlers: Handlers for encoding Returns: dict: Encoded sample \"\"\" return { k : encode_based_on_extension1 ( v , k , handlers ) for k , v in list ( sample . items ()) } encode_based_on_extension1 ( data , tname , handlers ) Encode data based on its extension and a dict of handlers. Parameters: data ( Any ) \u2013 Data to be encoded tname ( str ) \u2013 File extension handlers ( dict ) \u2013 Dictionary of handlers for different data types Raises: ValueError \u2013 If no handler is found for the given extension or if metadata values are not strings Source code in webdataset/writer.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def encode_based_on_extension1 ( data : Any , tname : str , handlers : dict ): \"\"\"Encode data based on its extension and a dict of handlers. Args: data: Data to be encoded tname: File extension handlers: Dictionary of handlers for different data types Raises: ValueError: If no handler is found for the given extension or if metadata values are not strings \"\"\" if tname [ 0 ] == \"_\" : if not isinstance ( data , str ): raise ValueError ( \"the values of metadata must be of string type\" ) return data compress = False if tname . endswith ( \".gz\" ): compress = True tname = tname [: - 3 ] extension = re . sub ( r \".*\\.\" , \"\" , tname ) . lower () if isinstance ( data , bytes ): if compress : data = gzip . compress ( data ) return data if isinstance ( data , str ): data = data . encode ( \"utf-8\" ) if compress : data = gzip . compress ( data ) return data handler = handlers . get ( extension ) if handler is None : raise ValueError ( f \"no handler found for { extension } \" ) result = handler ( data ) if compress : result = gzip . compress ( result ) return result imageencoder ( image , format = 'PNG' ) Compress an image using PIL and return it as a string. Can handle float or uint8 images. Parameters: image ( Any ) \u2013 ndarray representing an image format ( str , default: 'PNG' ) \u2013 compression format (PNG, JPEG, PPM) Returns: bytes \u2013 Compressed image data Raises: ValueError \u2013 If image values are out of range Source code in webdataset/writer.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def imageencoder ( image : Any , format : str = \"PNG\" ): # skipcq: PYL-W0622 \"\"\"Compress an image using PIL and return it as a string. Can handle float or uint8 images. Args: image: ndarray representing an image format: compression format (PNG, JPEG, PPM) Returns: bytes: Compressed image data Raises: ValueError: If image values are out of range \"\"\" import PIL import PIL.Image assert isinstance ( image , ( PIL . Image . Image , np . ndarray )), type ( image ) if isinstance ( image , np . ndarray ): if image . dtype in [ np . dtype ( \"f\" ), np . dtype ( \"d\" )]: if not ( np . amin ( image ) > - 0.001 and np . amax ( image ) < 1.001 ): raise ValueError ( f \"image values out of range { np . amin ( image ) } { np . amax ( image ) } \" ) image = np . clip ( image , 0.0 , 1.0 ) image = np . array ( image * 255.0 , \"uint8\" ) assert image . ndim in [ 2 , 3 ] if image . ndim == 3 : assert image . shape [ 2 ] in [ 1 , 3 ] image = PIL . Image . fromarray ( image ) if format . upper () == \"JPG\" : format = \"JPEG\" elif format . upper () in { \"IMG\" , \"IMAGE\" }: format = \"PPM\" if format in { \"JPEG\" , \"tiff\" }: opts = dict ( quality = 100 ) else : opts = {} with io . BytesIO () as result : image . save ( result , format = format , ** opts ) return result . getvalue () make_encoder ( spec ) Make an encoder function from a specification. Parameters: spec ( Union [ bool , str , dict , Callable ] ) \u2013 Specification for the encoder Returns: Callable \u2013 Encoder function Raises: ValueError \u2013 If the specification is invalid or doesn't yield a callable encoder Source code in webdataset/writer.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def make_encoder ( spec : Union [ bool , str , dict , Callable ]): \"\"\"Make an encoder function from a specification. Args: spec: Specification for the encoder Returns: Callable: Encoder function Raises: ValueError: If the specification is invalid or doesn't yield a callable encoder \"\"\" if spec is False or spec is None : def encoder ( x ): \"\"\"Do not encode at all.\"\"\" return x elif callable ( spec ): encoder = spec elif isinstance ( spec , dict ): def f ( sample ): \"\"\"Encode based on extension.\"\"\" return encode_based_on_extension ( sample , spec ) encoder = f elif spec is True : handlers = default_handlers def g ( sample ): \"\"\"Encode based on extension.\"\"\" return encode_based_on_extension ( sample , handlers ) encoder = g else : raise ValueError ( f \" { spec } : unknown decoder spec\" ) if not callable ( encoder ): raise ValueError ( f \" { spec } did not yield a callable encoder\" ) return encoder make_handlers () Create a list of handlers for encoding data. Returns: dict \u2013 Dictionary of handlers for different data types Source code in webdataset/writer.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def make_handlers (): \"\"\"Create a list of handlers for encoding data. Returns: dict: Dictionary of handlers for different data types \"\"\" handlers = {} add_handlers ( handlers , \"cls cls2 class count index inx id\" , lambda x : str ( x ) . encode ( \"ascii\" ) ) add_handlers ( handlers , \"txt text transcript\" , lambda x : x . encode ( \"utf-8\" )) add_handlers ( handlers , \"html htm\" , lambda x : x . encode ( \"utf-8\" )) add_handlers ( handlers , \"pyd pickle\" , pickle . dumps ) add_handlers ( handlers , \"pth\" , torch_dumps ) add_handlers ( handlers , \"npy\" , numpy_dumps ) add_handlers ( handlers , \"npz\" , numpy_npz_dumps ) add_handlers ( handlers , \"ten tenbin tb\" , tenbin_dumps ) add_handlers ( handlers , \"json jsn\" , lambda x : json . dumps ( x ) . encode ( \"utf-8\" )) add_handlers ( handlers , \"mp msgpack msg\" , mp_dumps ) add_handlers ( handlers , \"cbor\" , cbor_dumps ) add_handlers ( handlers , \"jpg jpeg img image\" , lambda data : imageencoder ( data , \"jpg\" )) add_handlers ( handlers , \"png\" , lambda data : imageencoder ( data , \"png\" )) add_handlers ( handlers , \"pbm\" , lambda data : imageencoder ( data , \"pbm\" )) add_handlers ( handlers , \"pgm\" , lambda data : imageencoder ( data , \"pgm\" )) add_handlers ( handlers , \"ppm\" , lambda data : imageencoder ( data , \"ppm\" )) add_handlers ( handlers , \"tiff tif\" , lambda data : imageencoder ( data , \"tiff\" )) return handlers mp_dumps ( x ) Dump data into a bytestring using MessagePack format. Parameters: x \u2013 Data to be dumped Returns: bytes \u2013 Dumped data as bytestring Source code in webdataset/writer.py 179 180 181 182 183 184 185 186 187 188 189 190 def mp_dumps ( x ): \"\"\"Dump data into a bytestring using MessagePack format. Args: x: Data to be dumped Returns: bytes: Dumped data as bytestring \"\"\" import msgpack return msgpack . packb ( x ) numpy_dumps ( data ) Dump data into a bytestring using numpy npy format. Parameters: data ( ndarray ) \u2013 Data to be dumped Returns: bytes \u2013 Dumped data as bytestring Source code in webdataset/writer.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def numpy_dumps ( data : np . ndarray ): \"\"\"Dump data into a bytestring using numpy npy format. Args: data: Data to be dumped Returns: bytes: Dumped data as bytestring \"\"\" import io import numpy.lib.format stream = io . BytesIO () numpy . lib . format . write_array ( stream , data ) return stream . getvalue () numpy_npz_dumps ( data ) Dump data into a bytestring using numpy npz format. Parameters: data ( Dict [ str , ndarray ] ) \u2013 Dictionary of numpy arrays to be dumped Returns: bytes \u2013 Dumped data as bytestring Raises: AssertionError \u2013 If input is not a dictionary of numpy arrays Source code in webdataset/writer.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def numpy_npz_dumps ( data : Dict [ str , np . ndarray ]): \"\"\"Dump data into a bytestring using numpy npz format. Args: data: Dictionary of numpy arrays to be dumped Returns: bytes: Dumped data as bytestring Raises: AssertionError: If input is not a dictionary of numpy arrays \"\"\" import io assert isinstance ( data , dict ) for k , v in list ( data . items ()): assert isinstance ( k , str ) assert isinstance ( v , np . ndarray ) stream = io . BytesIO () np . savez_compressed ( stream , ** data ) return stream . getvalue () tenbin_dumps ( x ) Dump data into a bytestring using tenbin format. Parameters: x \u2013 Data to be dumped (list or single item) Returns: memoryview \u2013 Dumped data as memoryview Source code in webdataset/writer.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def tenbin_dumps ( x ): \"\"\"Dump data into a bytestring using tenbin format. Args: x: Data to be dumped (list or single item) Returns: memoryview: Dumped data as memoryview \"\"\" from . import tenbin if isinstance ( x , list ): return memoryview ( tenbin . encode_buffer ( x )) else : return memoryview ( tenbin . encode_buffer ([ x ])) torch_dumps ( data ) Dump data into a bytestring using torch.dumps. This delays importing torch until needed. Parameters: data ( Any ) \u2013 Data to be dumped Returns: bytes \u2013 Dumped data as bytestring Source code in webdataset/writer.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def torch_dumps ( data : Any ): \"\"\"Dump data into a bytestring using torch.dumps. This delays importing torch until needed. Args: data: Data to be dumped Returns: bytes: Dumped data as bytestring \"\"\" import io import torch stream = io . BytesIO () torch . save ( data , stream ) return stream . getvalue ()","title":"WebDataset"},{"location":"webdataset2/#webdataset","text":"WebDataset is a PyTorch Dataset (IterableDataset) implementation providing efficient access to large datasets stored in POSIX tar archives. It is designed for use with distributed training and for streaming data directly from web servers into training pipelines.","title":"WebDataset"},{"location":"webdataset2/#webdataset.autodecode","text":"Automatically decode webdataset samples.","title":"autodecode"},{"location":"webdataset2/#webdataset.autodecode.pytorch_weights_only","text":"Extensions passed on to the image decoder.","title":"pytorch_weights_only"},{"location":"webdataset2/#webdataset.autodecode.Continue","text":"Special class for continuing decoding. This is mostly used for decompression, as in: def decompressor(key, data): if key.endswith(\".gz\"): return Continue(key[:-3], decompress(data)) return None Source code in webdataset/autodecode.py 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 class Continue : \"\"\"Special class for continuing decoding. This is mostly used for decompression, as in: def decompressor(key, data): if key.endswith(\".gz\"): return Continue(key[:-3], decompress(data)) return None \"\"\" def __init__ ( self , key , data ): \"\"\"__init__. :param key: :param data: \"\"\" self . key , self . data = key , data","title":"Continue"},{"location":"webdataset2/#webdataset.autodecode.Continue.__init__","text":"init . :param key: :param data: Source code in webdataset/autodecode.py 460 461 462 463 464 465 466 def __init__ ( self , key , data ): \"\"\"__init__. :param key: :param data: \"\"\" self . key , self . data = key , data","title":"__init__"},{"location":"webdataset2/#webdataset.autodecode.Decoder","text":"Decode samples using a list of handlers. For each key/data item, this iterates through the list of handlers until some handler returns something other than None. Source code in webdataset/autodecode.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 class Decoder : \"\"\"Decode samples using a list of handlers. For each key/data item, this iterates through the list of handlers until some handler returns something other than None. \"\"\" def __init__ ( self , handlers , pre = None , post = None , only = None , partial = False ): \"\"\"Create a Decoder. :param handlers: main list of handlers :param pre: handlers called before the main list (.gz handler by default) :param post: handlers called after the main list (default handlers by default) :param only: a list of extensions; when give, only ignores files with those extensions :param partial: allow partial decoding (i.e., don't decode fields that aren't of type bytes) \"\"\" assert isinstance ( handlers , list ), f \"handlers = { handlers } must be a list\" if isinstance ( only , str ): only = only . split () self . only = only if only is None else set ( only ) if pre is None : pre = default_pre_handlers if post is None : post = default_post_handlers assert all ( callable ( h ) for h in handlers ), f \"one of { handlers } not callable\" assert all ( callable ( h ) for h in pre ), f \"one of { pre } not callable\" assert all ( callable ( h ) for h in post ), f \"one of { post } not callable\" self . handlers = pre + handlers + post self . partial = partial def decode1 ( self , key , data ): \"\"\"Decode a single field of a sample. :param key: file name extension :param data: binary data \"\"\" key = \".\" + key for f in self . handlers : result = f ( key , data ) if isinstance ( result , Continue ): key , data = result . key , result . data continue if result is not None : return result return data def decode ( self , sample ): \"\"\"Decode an entire sample. :param sample: the sample, a dictionary of key value pairs \"\"\" result = {} assert isinstance ( sample , dict ), sample for k , v in list ( sample . items ()): if k [: 2 ] == \"__\" : if isinstance ( v , bytes ): try : v = v . decode ( \"utf-8\" ) except Exception : print ( f \"Can't decode v of k = { k } as utf-8: v = { v } \" ) result [ k ] = v continue if self . only is not None and k not in self . only : result [ k ] = v continue assert v is not None if self . partial : if isinstance ( v , bytes ): result [ k ] = self . decode1 ( k , v ) else : result [ k ] = v else : assert isinstance ( v , bytes ), f \"k,v = { k } , { v } \" result [ k ] = self . decode1 ( k , v ) return result def __call__ ( self , sample ): \"\"\"Decode an entire sample. :param sample: the sample \"\"\" assert isinstance ( sample , dict ), ( len ( sample ), sample ) return self . decode ( sample )","title":"Decoder"},{"location":"webdataset2/#webdataset.autodecode.Decoder.__call__","text":"Decode an entire sample. :param sample: the sample Source code in webdataset/autodecode.py 570 571 572 573 574 575 576 def __call__ ( self , sample ): \"\"\"Decode an entire sample. :param sample: the sample \"\"\" assert isinstance ( sample , dict ), ( len ( sample ), sample ) return self . decode ( sample )","title":"__call__"},{"location":"webdataset2/#webdataset.autodecode.Decoder.__init__","text":"Create a Decoder. :param handlers: main list of handlers :param pre: handlers called before the main list (.gz handler by default) :param post: handlers called after the main list (default handlers by default) :param only: a list of extensions; when give, only ignores files with those extensions :param partial: allow partial decoding (i.e., don't decode fields that aren't of type bytes) Source code in webdataset/autodecode.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 def __init__ ( self , handlers , pre = None , post = None , only = None , partial = False ): \"\"\"Create a Decoder. :param handlers: main list of handlers :param pre: handlers called before the main list (.gz handler by default) :param post: handlers called after the main list (default handlers by default) :param only: a list of extensions; when give, only ignores files with those extensions :param partial: allow partial decoding (i.e., don't decode fields that aren't of type bytes) \"\"\" assert isinstance ( handlers , list ), f \"handlers = { handlers } must be a list\" if isinstance ( only , str ): only = only . split () self . only = only if only is None else set ( only ) if pre is None : pre = default_pre_handlers if post is None : post = default_post_handlers assert all ( callable ( h ) for h in handlers ), f \"one of { handlers } not callable\" assert all ( callable ( h ) for h in pre ), f \"one of { pre } not callable\" assert all ( callable ( h ) for h in post ), f \"one of { post } not callable\" self . handlers = pre + handlers + post self . partial = partial","title":"__init__"},{"location":"webdataset2/#webdataset.autodecode.Decoder.decode","text":"Decode an entire sample. :param sample: the sample, a dictionary of key value pairs Source code in webdataset/autodecode.py 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 def decode ( self , sample ): \"\"\"Decode an entire sample. :param sample: the sample, a dictionary of key value pairs \"\"\" result = {} assert isinstance ( sample , dict ), sample for k , v in list ( sample . items ()): if k [: 2 ] == \"__\" : if isinstance ( v , bytes ): try : v = v . decode ( \"utf-8\" ) except Exception : print ( f \"Can't decode v of k = { k } as utf-8: v = { v } \" ) result [ k ] = v continue if self . only is not None and k not in self . only : result [ k ] = v continue assert v is not None if self . partial : if isinstance ( v , bytes ): result [ k ] = self . decode1 ( k , v ) else : result [ k ] = v else : assert isinstance ( v , bytes ), f \"k,v = { k } , { v } \" result [ k ] = self . decode1 ( k , v ) return result","title":"decode"},{"location":"webdataset2/#webdataset.autodecode.Decoder.decode1","text":"Decode a single field of a sample. :param key: file name extension :param data: binary data Source code in webdataset/autodecode.py 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 def decode1 ( self , key , data ): \"\"\"Decode a single field of a sample. :param key: file name extension :param data: binary data \"\"\" key = \".\" + key for f in self . handlers : result = f ( key , data ) if isinstance ( result , Continue ): key , data = result . key , result . data continue if result is not None : return result return data","title":"decode1"},{"location":"webdataset2/#webdataset.autodecode.ImageHandler","text":"Decode image data using the given imagespec . The imagespec specifies whether the image is decoded to numpy/torch/pi, decoded to uint8/float, and decoded to l/rgb/rgba: l8: numpy uint8 l rgb8: numpy uint8 rgb rgba8: numpy uint8 rgba l: numpy float l rgb: numpy float rgb rgba: numpy float rgba torchl8: torch uint8 l torchrgb8: torch uint8 rgb torchrgba8: torch uint8 rgba torchl: torch float l torchrgb: torch float rgb torch: torch float rgb torchrgba: torch float rgba pill: pil None l pil: pil None rgb pilrgb: pil None rgb pilrgba: pil None rgba Source code in webdataset/autodecode.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 class ImageHandler : \"\"\"Decode image data using the given `imagespec`. The `imagespec` specifies whether the image is decoded to numpy/torch/pi, decoded to uint8/float, and decoded to l/rgb/rgba: - l8: numpy uint8 l - rgb8: numpy uint8 rgb - rgba8: numpy uint8 rgba - l: numpy float l - rgb: numpy float rgb - rgba: numpy float rgba - torchl8: torch uint8 l - torchrgb8: torch uint8 rgb - torchrgba8: torch uint8 rgba - torchl: torch float l - torchrgb: torch float rgb - torch: torch float rgb - torchrgba: torch float rgba - pill: pil None l - pil: pil None rgb - pilrgb: pil None rgb - pilrgba: pil None rgba \"\"\" def __init__ ( self , imagespec , extensions = IMAGE_EXTENSIONS ): \"\"\"Create an image handler. :param imagespec: short string indicating the type of decoding :param extensions: list of extensions the image handler is invoked for \"\"\" if imagespec not in list ( imagespecs . keys ()): raise ValueError ( \"Unknown imagespec: %s \" % imagespec ) self . imagespec = imagespec . lower () self . extensions = extensions def __call__ ( self , key , data ): \"\"\"Perform image decoding. :param key: file name extension :param data: binary data \"\"\" import PIL.Image extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension . lower () not in self . extensions : return None imagespec = self . imagespec atype , etype , mode = imagespecs [ imagespec ] with io . BytesIO ( data ) as stream : img = PIL . Image . open ( stream ) img . load () img = img . convert ( mode . upper ()) if atype == \"pil\" : if mode == \"l\" : img = img . convert ( \"L\" ) return img elif mode == \"rgb\" : img = img . convert ( \"RGB\" ) return img elif mode == \"rgba\" : img = img . convert ( \"RGBA\" ) return img else : raise ValueError ( \"Unknown mode: %s \" % mode ) result = np . asarray ( img ) if etype == \"float\" : result = result . astype ( np . float32 ) / 255.0 assert result . ndim in [ 2 , 3 ], result . shape assert mode in [ \"l\" , \"rgb\" , \"rgba\" ], mode if mode == \"l\" : if result . ndim == 3 : result = np . mean ( result [:, :, : 3 ], axis = 2 ) elif mode == \"rgb\" : if result . ndim == 2 : result = np . repeat ( result [:, :, np . newaxis ], 3 , axis = 2 ) elif result . shape [ 2 ] == 4 : result = result [:, :, : 3 ] elif mode == \"rgba\" : if result . ndim == 2 : result = np . repeat ( result [:, :, np . newaxis ], 4 , axis = 2 ) result [:, :, 3 ] = 255 elif result . shape [ 2 ] == 3 : result = np . concatenate ( [ result , 255 * np . ones ( result . shape [: 2 ])], axis = 2 ) assert atype in [ \"numpy\" , \"torch\" ], atype if atype == \"numpy\" : return result elif atype == \"torch\" : import torch if result . ndim == 3 : return torch . from_numpy ( result . transpose ( 2 , 0 , 1 ) . copy ()) else : return torch . from_numpy ( result . copy ()) return None","title":"ImageHandler"},{"location":"webdataset2/#webdataset.autodecode.ImageHandler.__call__","text":"Perform image decoding. :param key: file name extension :param data: binary data Source code in webdataset/autodecode.py 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def __call__ ( self , key , data ): \"\"\"Perform image decoding. :param key: file name extension :param data: binary data \"\"\" import PIL.Image extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension . lower () not in self . extensions : return None imagespec = self . imagespec atype , etype , mode = imagespecs [ imagespec ] with io . BytesIO ( data ) as stream : img = PIL . Image . open ( stream ) img . load () img = img . convert ( mode . upper ()) if atype == \"pil\" : if mode == \"l\" : img = img . convert ( \"L\" ) return img elif mode == \"rgb\" : img = img . convert ( \"RGB\" ) return img elif mode == \"rgba\" : img = img . convert ( \"RGBA\" ) return img else : raise ValueError ( \"Unknown mode: %s \" % mode ) result = np . asarray ( img ) if etype == \"float\" : result = result . astype ( np . float32 ) / 255.0 assert result . ndim in [ 2 , 3 ], result . shape assert mode in [ \"l\" , \"rgb\" , \"rgba\" ], mode if mode == \"l\" : if result . ndim == 3 : result = np . mean ( result [:, :, : 3 ], axis = 2 ) elif mode == \"rgb\" : if result . ndim == 2 : result = np . repeat ( result [:, :, np . newaxis ], 3 , axis = 2 ) elif result . shape [ 2 ] == 4 : result = result [:, :, : 3 ] elif mode == \"rgba\" : if result . ndim == 2 : result = np . repeat ( result [:, :, np . newaxis ], 4 , axis = 2 ) result [:, :, 3 ] = 255 elif result . shape [ 2 ] == 3 : result = np . concatenate ( [ result , 255 * np . ones ( result . shape [: 2 ])], axis = 2 ) assert atype in [ \"numpy\" , \"torch\" ], atype if atype == \"numpy\" : return result elif atype == \"torch\" : import torch if result . ndim == 3 : return torch . from_numpy ( result . transpose ( 2 , 0 , 1 ) . copy ()) else : return torch . from_numpy ( result . copy ()) return None","title":"__call__"},{"location":"webdataset2/#webdataset.autodecode.ImageHandler.__init__","text":"Create an image handler. :param imagespec: short string indicating the type of decoding :param extensions: list of extensions the image handler is invoked for Source code in webdataset/autodecode.py 303 304 305 306 307 308 309 310 311 312 def __init__ ( self , imagespec , extensions = IMAGE_EXTENSIONS ): \"\"\"Create an image handler. :param imagespec: short string indicating the type of decoding :param extensions: list of extensions the image handler is invoked for \"\"\" if imagespec not in list ( imagespecs . keys ()): raise ValueError ( \"Unknown imagespec: %s \" % imagespec ) self . imagespec = imagespec . lower () self . extensions = extensions","title":"__init__"},{"location":"webdataset2/#webdataset.autodecode.basichandlers","text":"Handle basic file decoding. This function is usually part of the post= decoders. This handles the following forms of decoding: txt -> unicode string cls cls2 class count index inx id -> int json jsn -> JSON decoding pyd pickle -> pickle decoding pth -> torch.loads ten tenbin -> fast tensor loading mp messagepack msg -> messagepack decoding npy -> Python NPY decoding :param key: file name extension :param data: binary data to be decoded Source code in webdataset/autodecode.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def basichandlers ( key , data ): \"\"\"Handle basic file decoding. This function is usually part of the post= decoders. This handles the following forms of decoding: - txt -> unicode string - cls cls2 class count index inx id -> int - json jsn -> JSON decoding - pyd pickle -> pickle decoding - pth -> torch.loads - ten tenbin -> fast tensor loading - mp messagepack msg -> messagepack decoding - npy -> Python NPY decoding :param key: file name extension :param data: binary data to be decoded \"\"\" extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension in decoders : return decoders [ extension ]( data ) return None","title":"basichandlers"},{"location":"webdataset2/#webdataset.autodecode.call_extension_handler","text":"Call the function f with the given data if the key matches the extensions. :param key: actual key found in the sample :param data: binary data :param f: decoder function :param extensions: list of matching extensions Source code in webdataset/autodecode.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def call_extension_handler ( key , data , f , extensions ): \"\"\"Call the function f with the given data if the key matches the extensions. :param key: actual key found in the sample :param data: binary data :param f: decoder function :param extensions: list of matching extensions \"\"\" extension = key . lower () . split ( \".\" ) for target in extensions : target = target . split ( \".\" ) if len ( target ) > len ( extension ): continue if extension [ - len ( target ) :] == target : return f ( data ) return None","title":"call_extension_handler"},{"location":"webdataset2/#webdataset.autodecode.cbor_loads","text":"Load data from cbor format. Imports cbor only if necessary. Source code in webdataset/autodecode.py 155 156 157 158 159 def cbor_loads ( data ): \"\"\"Load data from cbor format. Imports cbor only if necessary.\"\"\" import cbor return cbor . loads ( data )","title":"cbor_loads"},{"location":"webdataset2/#webdataset.autodecode.gzfilter","text":"Decode .gz files. This decodes compressed files and the continues decoding. :param key: file name extension :param data: binary data Source code in webdataset/autodecode.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 def gzfilter ( key , data ): \"\"\"Decode .gz files. This decodes compressed files and the continues decoding. :param key: file name extension :param data: binary data \"\"\" import gzip if not key . endswith ( \".gz\" ): return None decompressed = gzip . open ( io . BytesIO ( data )) . read () return Continue ( key [: - 3 ], decompressed )","title":"gzfilter"},{"location":"webdataset2/#webdataset.autodecode.handle_extension","text":"Return a decoder function for the list of extensions. Extensions can be a space separated list of extensions. Extensions can contain dots, in which case the corresponding number of extension components must be present in the key given to f. Comparisons are case insensitive. Examples: handle_extension(\"jpg jpeg\", my_decode_jpg) # invoked for any file.jpg handle_extension(\"seg.jpg\", special_case_jpg) # invoked only for file.seg.jpg Source code in webdataset/autodecode.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def handle_extension ( extensions , f ): \"\"\"Return a decoder function for the list of extensions. Extensions can be a space separated list of extensions. Extensions can contain dots, in which case the corresponding number of extension components must be present in the key given to f. Comparisons are case insensitive. Examples: handle_extension(\"jpg jpeg\", my_decode_jpg) # invoked for any file.jpg handle_extension(\"seg.jpg\", special_case_jpg) # invoked only for file.seg.jpg \"\"\" extensions = extensions . lower () . split () return partial ( call_extension_handler , f = f , extensions = extensions )","title":"handle_extension"},{"location":"webdataset2/#webdataset.autodecode.imagehandler","text":"Create an image handler. This is just a lower case alias for ImageHander. :param imagespec: textual image spec :param extensions: list of extensions the handler should be applied for Source code in webdataset/autodecode.py 385 386 387 388 389 390 391 392 393 def imagehandler ( imagespec , extensions = IMAGE_EXTENSIONS ): \"\"\"Create an image handler. This is just a lower case alias for ImageHander. :param imagespec: textual image spec :param extensions: list of extensions the handler should be applied for \"\"\" return ImageHandler ( imagespec , extensions )","title":"imagehandler"},{"location":"webdataset2/#webdataset.autodecode.msgpack_loads","text":"Load data from msgpack format. Imports msgpack only if necessary. Source code in webdataset/autodecode.py 133 134 135 136 137 def msgpack_loads ( data ): \"\"\"Load data from msgpack format. Imports msgpack only if necessary.\"\"\" import msgpack return msgpack . unpackb ( data )","title":"msgpack_loads"},{"location":"webdataset2/#webdataset.autodecode.npy_loads","text":"Load data from npy format. Imports numpy only if necessary. Source code in webdataset/autodecode.py 140 141 142 143 144 145 def npy_loads ( data ): \"\"\"Load data from npy format. Imports numpy only if necessary.\"\"\" import numpy.lib.format stream = io . BytesIO ( data ) return numpy . lib . format . read_array ( stream )","title":"npy_loads"},{"location":"webdataset2/#webdataset.autodecode.npz_loads","text":"Load data from npz format. Imports numpy only if necessary. Source code in webdataset/autodecode.py 147 148 149 150 151 152 def npz_loads ( data ): \"\"\"Load data from npz format. Imports numpy only if necessary.\"\"\" import numpy.lib.format stream = io . BytesIO ( data ) return dict ( np . load ( stream ))","title":"npz_loads"},{"location":"webdataset2/#webdataset.autodecode.tenbin_loads","text":"Load data from tenbin format. Imports tenbin only if necessary. Source code in webdataset/autodecode.py 126 127 128 129 130 def tenbin_loads ( data ): \"\"\"Load data from tenbin format. Imports tenbin only if necessary.\"\"\" from . import tenbin return tenbin . decode_buffer ( data )","title":"tenbin_loads"},{"location":"webdataset2/#webdataset.autodecode.torch_audio","text":"Decode audio using the torchaudio library. :param key: file name extension :param data: data to be decoded Source code in webdataset/autodecode.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def torch_audio ( key , data ): \"\"\"Decode audio using the torchaudio library. :param key: file name extension :param data: data to be decoded \"\"\" extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension not in [ \"flac\" , \"mp3\" , \"sox\" , \"wav\" , \"m4a\" , \"ogg\" , \"wma\" ]: return None import torchaudio with tempfile . TemporaryDirectory () as dirname : fname = os . path . join ( dirname , f \"file. { extension } \" ) with open ( fname , \"wb\" ) as stream : stream . write ( data ) return torchaudio . load ( fname )","title":"torch_audio"},{"location":"webdataset2/#webdataset.autodecode.torch_loads","text":"Function: torch_loads Description: This function loads data using torch.loads. It first imports torch only if necessary. Then it decodes the input data using torch.load. Parameters: - data (bytes): The data to be decoded. Returns: It returns the decoded input data. Example data = b'...' output = torch_loads(data) Source code in webdataset/autodecode.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def torch_loads ( data : bytes ): \"\"\"Function: torch_loads Description: This function loads data using torch.loads. It first imports torch only if necessary. Then it decodes the input data using torch.load. Parameters: - data (bytes): The data to be decoded. Returns: It returns the decoded input data. Example: data = b'...' output = torch_loads(data) \"\"\" import io import torch stream = io . BytesIO ( data ) return torch . load ( stream , weights_only = pytorch_weights_only )","title":"torch_loads"},{"location":"webdataset2/#webdataset.autodecode.torch_video","text":"Decode video using the torchvideo library. :param key: file name extension :param data: data to be decoded Source code in webdataset/autodecode.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 def torch_video ( key , data ): \"\"\"Decode video using the torchvideo library. :param key: file name extension :param data: data to be decoded \"\"\" extension = re . sub ( r \".*[.]\" , \"\" , key ) if extension not in \"mp4 ogv mjpeg avi mov h264 mpg webm wmv\" . split (): return None import torchvision.io with tempfile . TemporaryDirectory () as dirname : fname = os . path . join ( dirname , f \"file. { extension } \" ) with open ( fname , \"wb\" ) as stream : stream . write ( data ) return torchvision . io . read_video ( fname , pts_unit = \"sec\" )","title":"torch_video"},{"location":"webdataset2/#webdataset.cache","text":"Code related to caching files downloaded from storage servers, object servers, and web servers.","title":"cache"},{"location":"webdataset2/#webdataset.cache.FileCache","text":"Cache files from URLs. This class provides functionality to download and cache files from URLs, with options for validation, error handling, and cache management. Parameters: cache_dir ( Optional [ str ] , default: None ) \u2013 The directory to use for caching. Defaults to None. url_to_name ( Callable [[ str ], str ] , default: url_to_cache_name ) \u2013 Function to convert URLs to cache names. verbose ( bool , default: False ) \u2013 Whether to print verbose output. Defaults to False. validator ( Callable [[ str ], bool ] , default: check_tar_format ) \u2013 Function to validate downloaded files. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Function to handle exceptions. cache_size ( int , default: -1 ) \u2013 Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_cleanup_interval ( int , default: 30 ) \u2013 Interval between cache cleanup operations in seconds. Source code in webdataset/cache.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 class FileCache : \"\"\"Cache files from URLs. This class provides functionality to download and cache files from URLs, with options for validation, error handling, and cache management. Args: cache_dir (Optional[str]): The directory to use for caching. Defaults to None. url_to_name (Callable[[str], str]): Function to convert URLs to cache names. verbose (bool): Whether to print verbose output. Defaults to False. validator (Callable[[str], bool]): Function to validate downloaded files. handler (Callable[[Exception], bool]): Function to handle exceptions. cache_size (int): Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_cleanup_interval (int): Interval between cache cleanup operations in seconds. \"\"\" def __init__ ( self , cache_dir : Optional [ str ] = None , * , url_to_name : Callable [[ str ], str ] = url_to_cache_name , verbose : bool = False , validator : Callable [[ str ], bool ] = check_tar_format , handler : Callable [[ Exception ], bool ] = reraise_exception , cache_size : int = - 1 , cache_cleanup_interval : int = 30 , ): self . url_to_name = url_to_name self . validator = validator self . handler = handler if cache_dir is None : self . cache_dir = default_cache_dir else : self . cache_dir = cache_dir self . verbose = verbose if cache_size > 0 : self . cleaner = LRUCleanup ( self . cache_dir , cache_size , verbose = self . verbose , interval = cache_cleanup_interval , ) else : self . cleaner = None def get_file ( self , url : str ) -> str : \"\"\"Download a file from a given URL and return the path to the downloaded file. Args: url (str): The URL of the file to download. Returns: str: The path to the downloaded file. Raises: ValueError: If the downloaded file fails validation. \"\"\" assert isinstance ( url , str ) if islocal ( url ): return urlparse ( url ) . path cache_name = self . url_to_name ( url ) assert \"/\" not in cache_name , f \"bad cache name { cache_name } for { url } \" destdir = os . path . join ( self . cache_dir , os . path . dirname ( cache_name )) os . makedirs ( destdir , exist_ok = True ) dest = os . path . join ( self . cache_dir , cache_name ) if not os . path . exists ( dest ): if self . verbose : print ( \"# downloading %s to %s \" % ( url , dest ), file = sys . stderr ) if self . cleaner is not None : self . cleaner . cleanup () download ( url , dest , verbose = self . verbose ) if self . validator : if not self . validator ( dest ): ftype = get_filetype ( dest ) with open ( dest , \"rb\" ) as f : data = f . read ( 200 ) os . remove ( dest ) raise ValueError ( \" %s ( %s ) is not a tar archive, but a %s , contains %s \" % ( dest , url , ftype , repr ( data )) ) return dest def __call__ ( self , urls : Iterable [ str ]) -> Iterable [ io . IOBase ]: \"\"\"Download files from a list of URLs and yield file streams. Args: urls (Iterable[str]): An iterable of URLs to download files from. Yields: dict: A dictionary containing the URL, file stream, and local path of each downloaded file. Raises: Exception: If there's an error downloading or opening a file. \"\"\" for url in urls : if isinstance ( url , dict ): url = url [ \"url\" ] for _ in range ( 10 ): try : dest = self . get_file ( url ) stream = open ( dest , \"rb\" ) except Exception as e : if self . handler ( e ): continue else : break yield dict ( url = url , stream = stream , local_path = dest ) break","title":"FileCache"},{"location":"webdataset2/#webdataset.cache.FileCache.__call__","text":"Download files from a list of URLs and yield file streams. Parameters: urls ( Iterable [ str ] ) \u2013 An iterable of URLs to download files from. Yields: dict ( Iterable [ IOBase ] ) \u2013 A dictionary containing the URL, file stream, and local path of each downloaded file. Raises: Exception \u2013 If there's an error downloading or opening a file. Source code in webdataset/cache.py 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def __call__ ( self , urls : Iterable [ str ]) -> Iterable [ io . IOBase ]: \"\"\"Download files from a list of URLs and yield file streams. Args: urls (Iterable[str]): An iterable of URLs to download files from. Yields: dict: A dictionary containing the URL, file stream, and local path of each downloaded file. Raises: Exception: If there's an error downloading or opening a file. \"\"\" for url in urls : if isinstance ( url , dict ): url = url [ \"url\" ] for _ in range ( 10 ): try : dest = self . get_file ( url ) stream = open ( dest , \"rb\" ) except Exception as e : if self . handler ( e ): continue else : break yield dict ( url = url , stream = stream , local_path = dest ) break","title":"__call__"},{"location":"webdataset2/#webdataset.cache.FileCache.get_file","text":"Download a file from a given URL and return the path to the downloaded file. Parameters: url ( str ) \u2013 The URL of the file to download. Returns: str ( str ) \u2013 The path to the downloaded file. Raises: ValueError \u2013 If the downloaded file fails validation. Source code in webdataset/cache.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def get_file ( self , url : str ) -> str : \"\"\"Download a file from a given URL and return the path to the downloaded file. Args: url (str): The URL of the file to download. Returns: str: The path to the downloaded file. Raises: ValueError: If the downloaded file fails validation. \"\"\" assert isinstance ( url , str ) if islocal ( url ): return urlparse ( url ) . path cache_name = self . url_to_name ( url ) assert \"/\" not in cache_name , f \"bad cache name { cache_name } for { url } \" destdir = os . path . join ( self . cache_dir , os . path . dirname ( cache_name )) os . makedirs ( destdir , exist_ok = True ) dest = os . path . join ( self . cache_dir , cache_name ) if not os . path . exists ( dest ): if self . verbose : print ( \"# downloading %s to %s \" % ( url , dest ), file = sys . stderr ) if self . cleaner is not None : self . cleaner . cleanup () download ( url , dest , verbose = self . verbose ) if self . validator : if not self . validator ( dest ): ftype = get_filetype ( dest ) with open ( dest , \"rb\" ) as f : data = f . read ( 200 ) os . remove ( dest ) raise ValueError ( \" %s ( %s ) is not a tar archive, but a %s , contains %s \" % ( dest , url , ftype , repr ( data )) ) return dest","title":"get_file"},{"location":"webdataset2/#webdataset.cache.LRUCleanup","text":"Perform LRU cleanup on a cache directory. Source code in webdataset/cache.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class LRUCleanup : \"\"\"Perform LRU cleanup on a cache directory.\"\"\" def __init__ ( self , cache_dir = None , cache_size = int ( 1e12 ), keyfn = os . path . getctime , verbose = False , interval = 30 , ): \"\"\"Initialize the LRU cleanup object.\"\"\" self . cache_dir = cache_dir self . cache_size = cache_size self . keyfn = keyfn self . verbose = verbose self . interval = interval self . last_run = 0 def set_cache_dir ( self , cache_dir ): \"\"\"Set the cache directory.\"\"\" self . cache_dir = cache_dir def cleanup ( self ): \"\"\"Performs cleanup of the file cache in cache_dir using an LRU strategy, keeping the total size of all remaining files below cache_size. \"\"\" if not os . path . exists ( self . cache_dir ): return if self . interval is not None and time . time () - self . last_run < self . interval : return try : total_size = 0 for dirpath , dirnames , filenames in os . walk ( self . cache_dir ): for filename in filenames : total_size += os . path . getsize ( os . path . join ( dirpath , filename )) if total_size <= self . cache_size : return # sort files by last access time files = [] for dirpath , dirnames , filenames in os . walk ( self . cache_dir ): for filename in filenames : files . append ( os . path . join ( dirpath , filename )) files . sort ( key = self . keyfn , reverse = True ) # delete files until we're under the cache size while len ( files ) > 0 and total_size > self . cache_size : fname = files . pop () total_size -= os . path . getsize ( fname ) if self . verbose : print ( \"# deleting %s \" % fname , file = sys . stderr ) os . remove ( fname ) except ( OSError , FileNotFoundError ): # files may be deleted by other processes between walking the directory and getting their size/deleting them pass self . last_run = time . time ()","title":"LRUCleanup"},{"location":"webdataset2/#webdataset.cache.LRUCleanup.__init__","text":"Initialize the LRU cleanup object. Source code in webdataset/cache.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , cache_dir = None , cache_size = int ( 1e12 ), keyfn = os . path . getctime , verbose = False , interval = 30 , ): \"\"\"Initialize the LRU cleanup object.\"\"\" self . cache_dir = cache_dir self . cache_size = cache_size self . keyfn = keyfn self . verbose = verbose self . interval = interval self . last_run = 0","title":"__init__"},{"location":"webdataset2/#webdataset.cache.LRUCleanup.cleanup","text":"Performs cleanup of the file cache in cache_dir using an LRU strategy, keeping the total size of all remaining files below cache_size. Source code in webdataset/cache.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def cleanup ( self ): \"\"\"Performs cleanup of the file cache in cache_dir using an LRU strategy, keeping the total size of all remaining files below cache_size. \"\"\" if not os . path . exists ( self . cache_dir ): return if self . interval is not None and time . time () - self . last_run < self . interval : return try : total_size = 0 for dirpath , dirnames , filenames in os . walk ( self . cache_dir ): for filename in filenames : total_size += os . path . getsize ( os . path . join ( dirpath , filename )) if total_size <= self . cache_size : return # sort files by last access time files = [] for dirpath , dirnames , filenames in os . walk ( self . cache_dir ): for filename in filenames : files . append ( os . path . join ( dirpath , filename )) files . sort ( key = self . keyfn , reverse = True ) # delete files until we're under the cache size while len ( files ) > 0 and total_size > self . cache_size : fname = files . pop () total_size -= os . path . getsize ( fname ) if self . verbose : print ( \"# deleting %s \" % fname , file = sys . stderr ) os . remove ( fname ) except ( OSError , FileNotFoundError ): # files may be deleted by other processes between walking the directory and getting their size/deleting them pass self . last_run = time . time ()","title":"cleanup"},{"location":"webdataset2/#webdataset.cache.LRUCleanup.set_cache_dir","text":"Set the cache directory. Source code in webdataset/cache.py 109 110 111 def set_cache_dir ( self , cache_dir ): \"\"\"Set the cache directory.\"\"\" self . cache_dir = cache_dir","title":"set_cache_dir"},{"location":"webdataset2/#webdataset.cache.StreamingOpen","text":"Open a stream from a URL. Source code in webdataset/cache.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class StreamingOpen : \"\"\"Open a stream from a URL.\"\"\" def __init__ ( self , verbose = False , handler = reraise_exception ): \"\"\"Initialize the streaming open object.\"\"\" self . verbose = verbose self . handler = handler def __call__ ( self , urls ): \"\"\"Open a stream from a URL.\"\"\" for url in urls : if isinstance ( url , dict ): url = url [ \"url\" ] parsed = urlparse ( url ) try : if parsed . scheme in [ \"\" , \"file\" ]: stream = open ( parsed . path , \"rb\" ) yield dict ( url = url , stream = stream , local_path = parsed . path ) else : stream = gopen . gopen ( url ) yield dict ( url = url , stream = stream ) except Exception as exn : if self . handler ( exn ): continue else : break","title":"StreamingOpen"},{"location":"webdataset2/#webdataset.cache.StreamingOpen.__call__","text":"Open a stream from a URL. Source code in webdataset/cache.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def __call__ ( self , urls ): \"\"\"Open a stream from a URL.\"\"\" for url in urls : if isinstance ( url , dict ): url = url [ \"url\" ] parsed = urlparse ( url ) try : if parsed . scheme in [ \"\" , \"file\" ]: stream = open ( parsed . path , \"rb\" ) yield dict ( url = url , stream = stream , local_path = parsed . path ) else : stream = gopen . gopen ( url ) yield dict ( url = url , stream = stream ) except Exception as exn : if self . handler ( exn ): continue else : break","title":"__call__"},{"location":"webdataset2/#webdataset.cache.StreamingOpen.__init__","text":"Initialize the streaming open object. Source code in webdataset/cache.py 163 164 165 166 def __init__ ( self , verbose = False , handler = reraise_exception ): \"\"\"Initialize the streaming open object.\"\"\" self . verbose = verbose self . handler = handler","title":"__init__"},{"location":"webdataset2/#webdataset.cache.cached_tarfile_samples","text":"Process and yield samples from cached tar files. This function is obsolete. Parameters: src \u2013 An iterable source of URLs or dictionaries containing URLs. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. cache_size ( int , default: -1 ) \u2013 Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_dir ( Optional [ str ] , default: None ) \u2013 The directory to use for caching. Defaults to None. verbose ( bool , default: False ) \u2013 Whether to print verbose output. Defaults to False. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. always ( bool , default: False ) \u2013 Whether to always download files, even if they exist locally. Defaults to False. select_files \u2013 Function to select specific files from the tar archive. Defaults to None. rename_files \u2013 Function to rename files from the tar archive. Defaults to None. Returns: \u2013 An iterable of samples extracted from the cached tar files. Source code in webdataset/cache.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 @obsolete def cached_tarfile_samples ( src , handler = reraise_exception , cache_size =- 1 , cache_dir = None , verbose = False , url_to_name = pipe_cleaner , always = False , select_files = None , rename_files = None , ): \"\"\"Process and yield samples from cached tar files. This function is obsolete. Args: src: An iterable source of URLs or dictionaries containing URLs. handler: Function to handle exceptions. Defaults to reraise_exception. cache_size (int): Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_dir (Optional[str]): The directory to use for caching. Defaults to None. verbose (bool): Whether to print verbose output. Defaults to False. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. always (bool): Whether to always download files, even if they exist locally. Defaults to False. select_files: Function to select specific files from the tar archive. Defaults to None. rename_files: Function to rename files from the tar archive. Defaults to None. Returns: An iterable of samples extracted from the cached tar files. \"\"\" verbose = verbose or int ( os . environ . get ( \"GOPEN_VERBOSE\" , 0 )) streams = cached_url_opener ( src , handler = handler , cache_size = cache_size , cache_dir = cache_dir , verbose = verbose , url_to_name = url_to_name , always = always , ) files = tar_file_expander ( streams , handler = handler , select_files = select_files , rename_files = rename_files ) samples = group_by_keys ( files , handler = handler ) return samples","title":"cached_tarfile_samples"},{"location":"webdataset2/#webdataset.cache.cached_url_opener","text":"Open streams for a sequence of URLs, with caching. Given a stream of URL names (packaged in dict(url=url) ), yield opened streams. Parameters: data \u2013 An iterable of dictionaries containing URLs. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. cache_size ( int , default: -1 ) \u2013 Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_dir ( Optional [ str ] , default: None ) \u2013 The directory to use for caching. Defaults to None. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. validator \u2013 Function to validate downloaded files. Defaults to check_tar_format. verbose ( bool , default: False ) \u2013 Whether to print verbose output. Defaults to False. always ( bool , default: False ) \u2013 Whether to always download files, even if they exist locally. Defaults to False. Yields: dict \u2013 A dictionary containing the original sample data and an opened file stream. Raises: ValueError \u2013 If a downloaded file fails validation. Exception \u2013 For any other errors during download or file opening. Source code in webdataset/cache.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 @obsolete def cached_url_opener ( data , handler = reraise_exception , cache_size =- 1 , cache_dir = None , url_to_name = pipe_cleaner , validator = check_tar_format , verbose = False , always = False , ): \"\"\"Open streams for a sequence of URLs, with caching. Given a stream of URL names (packaged in `dict(url=url)`), yield opened streams. Args: data: An iterable of dictionaries containing URLs. handler: Function to handle exceptions. Defaults to reraise_exception. cache_size (int): Maximum size of the cache in bytes. Defaults to -1 (unlimited). cache_dir (Optional[str]): The directory to use for caching. Defaults to None. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. validator: Function to validate downloaded files. Defaults to check_tar_format. verbose (bool): Whether to print verbose output. Defaults to False. always (bool): Whether to always download files, even if they exist locally. Defaults to False. Yields: dict: A dictionary containing the original sample data and an opened file stream. Raises: ValueError: If a downloaded file fails validation. Exception: For any other errors during download or file opening. \"\"\" verbose = verbose or verbose_cache for sample in data : assert isinstance ( sample , dict ), sample assert \"url\" in sample url = sample [ \"url\" ] attempts = 5 try : if not always and os . path . exists ( url ): dest = url else : dest = get_file_cached ( url , cache_size = cache_size , cache_dir = cache_dir , url_to_name = url_to_name , verbose = verbose , ) if verbose : print ( \"# opening %s \" % dest , file = sys . stderr ) assert os . path . exists ( dest ) if not validator ( dest ): ftype = get_filetype ( dest ) with open ( dest , \"rb\" ) as f : data = f . read ( 200 ) os . remove ( dest ) raise ValueError ( \" %s ( %s ) is not a tar archive, but a %s , contains %s \" % ( dest , url , ftype , repr ( data )) ) try : stream = open ( dest , \"rb\" ) sample . update ( stream = stream ) yield sample except FileNotFoundError as exn : # dealing with race conditions in lru_cleanup attempts -= 1 if attempts > 0 : time . sleep ( random . random () * 10 ) continue raise exn except Exception as exn : exn . args = exn . args + ( url ,) if handler ( exn ): continue else : break","title":"cached_url_opener"},{"location":"webdataset2/#webdataset.cache.check_tar_format","text":"Check whether a file is a tar archive. Source code in webdataset/cache.py 43 44 45 46 47 def check_tar_format ( fname : str ): \"\"\"Check whether a file is a tar archive.\"\"\" assert os . path . exists ( fname ), fname ftype = get_filetype ( fname ) return \"tar archive\" in ftype or \"gzip compressed\" in ftype","title":"check_tar_format"},{"location":"webdataset2/#webdataset.cache.download","text":"Download a file from url to dest . Source code in webdataset/cache.py 147 148 149 150 151 152 153 154 155 156 157 def download ( url , dest , chunk_size = 1024 ** 2 , verbose = False ): \"\"\"Download a file from `url` to `dest`.\"\"\" temp = dest + f \".temp { os . getpid () } \" with gopen . gopen ( url ) as stream : with open ( temp , \"wb\" ) as f : while True : data = stream . read ( chunk_size ) if not data : break f . write ( data ) os . rename ( temp , dest )","title":"download"},{"location":"webdataset2/#webdataset.cache.get_filetype","text":"Get the file type of a file. Source code in webdataset/cache.py 34 35 36 37 38 39 40 def get_filetype ( fname : str ): \"\"\"Get the file type of a file.\"\"\" assert os . path . exists ( fname ), fname assert os . system ( \"file . > /dev/null\" ) == 0 , \"UNIX/Linux file command not available\" with os . popen ( \"file ' %s '\" % fname ) as f : ftype = f . read () return ftype","title":"get_filetype"},{"location":"webdataset2/#webdataset.cache.islocal","text":"Check whether a URL is a local file. Source code in webdataset/cache.py 28 29 30 31 def islocal ( url ): \"\"\"Check whether a URL is a local file.\"\"\" parsed = urlparse ( url ) return parsed . scheme in [ \"\" , \"file\" ]","title":"islocal"},{"location":"webdataset2/#webdataset.cache.pipe_cleaner","text":"Guess the actual URL from a \"pipe:\" specification. Source code in webdataset/cache.py 50 51 52 53 54 55 56 57 58 59 @obsolete def pipe_cleaner ( spec ): \"\"\"Guess the actual URL from a \"pipe:\" specification.\"\"\" if spec . startswith ( \"pipe:\" ): spec = spec [ 5 :] words = spec . split ( \" \" ) for word in words : if re . match ( r \"^(https?|hdfs|gs|ais|s3):\" , word ): return word return spec","title":"pipe_cleaner"},{"location":"webdataset2/#webdataset.cache.url_to_cache_name","text":"Guess the cache name from a URL. Source code in webdataset/cache.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def url_to_cache_name ( url , ndir = 0 ): \"\"\"Guess the cache name from a URL.\"\"\" assert isinstance ( url , str ) parsed = urlparse ( url ) if parsed . scheme in [ None , \"\" , \"file\" , \"http\" , \"https\" , \"ftp\" , \"ftps\" , \"gs\" , \"s3\" , \"ais\" , ]: path = parsed . path path = path . lstrip ( \"/\" ) # always relative list_of_directories = path . split ( \"/\" ) return \"/\" . join ( list_of_directories [ - 1 - ndir :]) else : # don't try to guess, just urlencode the whole thing with \"/\" and \":\" # quoted using the urllib.quote function quoted = urllib . parse . quote ( url , safe = \"_+ {} *,-\" ) quoted = quoted [ - 128 :] return quoted","title":"url_to_cache_name"},{"location":"webdataset2/#webdataset.compat","text":"","title":"compat"},{"location":"webdataset2/#webdataset.compat.FluidInterface","text":"Source code in webdataset/compat.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 class FluidInterface : def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) ) def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ()) def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None )) def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ()) def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile )) def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw )) def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler )) def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler ) def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw )) def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw )) def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw )) def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler )) def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args )) def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw )) def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p )) def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw )) def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw )) def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw )) def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ()) def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw ))","title":"FluidInterface"},{"location":"webdataset2/#webdataset.compat.FluidInterface.batched","text":"Create batches of the given size. This method forwards to the filters.batched function. Parameters: batchsize ( int ) \u2013 Target batch size. collation_fn ( callable , default: default_collation_fn ) \u2013 Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial ( bool , default: True ) \u2013 Whether to return partial batches. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with batched filter. Source code in webdataset/compat.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) )","title":"batched"},{"location":"webdataset2/#webdataset.compat.FluidInterface.decode","text":"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Parameters: *args \u2013 Decoding functions or strings representing image handlers. pre ( callable , default: None ) \u2013 Pre-processing function. post ( callable , default: None ) \u2013 Post-processing function. only ( list , default: None ) \u2013 List of keys to decode. partial ( bool , default: False ) \u2013 Whether to allow partial decoding. Defaults to False. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with decode filter. Source code in webdataset/compat.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler )","title":"decode"},{"location":"webdataset2/#webdataset.compat.FluidInterface.extract_keys","text":"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Parameters: *args \u2013 Keys or patterns to extract. **kw \u2013 Additional keyword arguments for filters.extract_keys. Returns: FluidInterface \u2013 Updated pipeline with extract_keys filter. Source code in webdataset/compat.py 256 257 258 259 260 261 262 263 264 265 266 267 268 def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw ))","title":"extract_keys"},{"location":"webdataset2/#webdataset.compat.FluidInterface.listed","text":"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Parameters: batchsize ( int ) \u2013 Target list size. partial ( bool , default: True ) \u2013 Whether to return partial lists. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with listed filter. Source code in webdataset/compat.py 47 48 49 50 51 52 53 54 55 56 57 58 59 def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None ))","title":"listed"},{"location":"webdataset2/#webdataset.compat.FluidInterface.lmdb_cached","text":"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Parameters: *args \u2013 Positional arguments for filters.LMDBCached. **kw \u2013 Keyword arguments for filters.LMDBCached. Returns: FluidInterface \u2013 Updated pipeline with LMDB caching. Source code in webdataset/compat.py 294 295 296 297 298 299 300 301 302 303 304 305 306 def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw ))","title":"lmdb_cached"},{"location":"webdataset2/#webdataset.compat.FluidInterface.log_keys","text":"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Parameters: logfile ( str , default: None ) \u2013 Path to the log file. If None, logging is disabled. Returns: FluidInterface \u2013 Updated pipeline with log_keys filter. Source code in webdataset/compat.py 71 72 73 74 75 76 77 78 79 80 81 82 def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile ))","title":"log_keys"},{"location":"webdataset2/#webdataset.compat.FluidInterface.map","text":"Apply a function to each sample in the stream. This method forwards to the filters.map function. Parameters: f ( callable ) \u2013 Function to apply to each sample. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map filter. Source code in webdataset/compat.py 101 102 103 104 105 106 107 108 109 110 111 112 113 def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler ))","title":"map"},{"location":"webdataset2/#webdataset.compat.FluidInterface.map_dict","text":"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Parameters: handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. **kw \u2013 Mapping of keys to functions to apply. Returns: FluidInterface \u2013 Updated pipeline with map_dict filter. Source code in webdataset/compat.py 147 148 149 150 151 152 153 154 155 156 157 158 159 def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw ))","title":"map_dict"},{"location":"webdataset2/#webdataset.compat.FluidInterface.map_tuple","text":"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Parameters: *args \u2013 Functions to apply to each element of the tuple. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map_tuple filter. Source code in webdataset/compat.py 189 190 191 192 193 194 195 196 197 198 199 200 201 def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler ))","title":"map_tuple"},{"location":"webdataset2/#webdataset.compat.FluidInterface.mcached","text":"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface \u2013 Updated pipeline with memory caching. Source code in webdataset/compat.py 284 285 286 287 288 289 290 291 292 def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ())","title":"mcached"},{"location":"webdataset2/#webdataset.compat.FluidInterface.rename","text":"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Parameters: **kw \u2013 Mapping of old names to new names. Returns: FluidInterface \u2013 Updated pipeline with rename filter. Source code in webdataset/compat.py 216 217 218 219 220 221 222 223 224 225 226 227 def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw ))","title":"rename"},{"location":"webdataset2/#webdataset.compat.FluidInterface.rename_keys","text":"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Parameters: *args \u2013 Positional arguments for filters.rename_keys. **kw \u2013 Keyword arguments for filters.rename_keys. Returns: FluidInterface \u2013 Updated pipeline with rename_keys filter. Source code in webdataset/compat.py 242 243 244 245 246 247 248 249 250 251 252 253 254 def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw ))","title":"rename_keys"},{"location":"webdataset2/#webdataset.compat.FluidInterface.rsample","text":"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Parameters: p ( float , default: 0.5 ) \u2013 Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface \u2013 Updated pipeline with rsample filter. Source code in webdataset/compat.py 229 230 231 232 233 234 235 236 237 238 239 240 def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p ))","title":"rsample"},{"location":"webdataset2/#webdataset.compat.FluidInterface.select","text":"Select samples based on a predicate. This method forwards to the filters.select function. Parameters: predicate ( callable ) \u2013 Function that returns True for samples to keep. **kw \u2013 Additional keyword arguments for filters.select. Returns: FluidInterface \u2013 Updated pipeline with select filter. Source code in webdataset/compat.py 161 162 163 164 165 166 167 168 169 170 171 172 173 def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw ))","title":"select"},{"location":"webdataset2/#webdataset.compat.FluidInterface.shuffle","text":"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Parameters: size ( int ) \u2013 Buffer size for shuffling. **kw \u2013 Additional keyword arguments for filters.shuffle. Returns: FluidInterface \u2013 Updated pipeline with shuffle filter, or self if size < 1. Source code in webdataset/compat.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw ))","title":"shuffle"},{"location":"webdataset2/#webdataset.compat.FluidInterface.slice","text":"Slice the data stream. This method forwards to the filters.slice function. Parameters: *args \u2013 Arguments for slicing (start, stop, step). Returns: FluidInterface \u2013 Updated pipeline with slice filter. Source code in webdataset/compat.py 203 204 205 206 207 208 209 210 211 212 213 214 def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args ))","title":"slice"},{"location":"webdataset2/#webdataset.compat.FluidInterface.to_tuple","text":"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Parameters: *args \u2013 Keys to extract from the dict. **kw \u2013 Additional keyword arguments for filters.to_tuple. Returns: FluidInterface \u2013 Updated pipeline with to_tuple filter. Source code in webdataset/compat.py 175 176 177 178 179 180 181 182 183 184 185 186 187 def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw ))","title":"to_tuple"},{"location":"webdataset2/#webdataset.compat.FluidInterface.unbatched","text":"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface \u2013 Updated pipeline with unbatched filter. Source code in webdataset/compat.py 37 38 39 40 41 42 43 44 45 def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ())","title":"unbatched"},{"location":"webdataset2/#webdataset.compat.FluidInterface.unlisted","text":"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface \u2013 Updated pipeline with unlisted filter. Source code in webdataset/compat.py 61 62 63 64 65 66 67 68 69 def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ())","title":"unlisted"},{"location":"webdataset2/#webdataset.compat.FluidInterface.xdecode","text":"Decode data based on file extensions. This method forwards to the filters.xdecode function. Parameters: *args \u2013 Positional arguments for filters.xdecode. **kw \u2013 Keyword arguments for filters.xdecode. Returns: FluidInterface \u2013 Updated pipeline with xdecode filter. Source code in webdataset/compat.py 270 271 272 273 274 275 276 277 278 279 280 281 282 def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw ))","title":"xdecode"},{"location":"webdataset2/#webdataset.compat.FluidWrapper","text":"Bases: DataPipeline , FluidInterface Small fluid-interface wrapper for DataPipeline. Source code in webdataset/compat.py 520 521 522 523 524 525 class FluidWrapper ( DataPipeline , FluidInterface ): \"\"\"Small fluid-interface wrapper for DataPipeline.\"\"\" def __init__ ( self , initial ): super () . __init__ () self . append ( initial )","title":"FluidWrapper"},{"location":"webdataset2/#webdataset.compat.WebDataset","text":"Bases: DataPipeline , FluidInterface Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Parameters: urls \u2013 The source URLs or specifications for the dataset. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. mode \u2013 The mode of operation. Defaults to None. resampled \u2013 Whether to use resampled mode. Defaults to False. repeat \u2013 Whether to repeat the dataset. Defaults to False. shardshuffle \u2013 The number of shards to shuffle, or None. Defaults to None. cache_size \u2013 The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir \u2013 The directory to use for caching. Defaults to None. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle \u2013 Whether to use deterministic shuffling. Defaults to False. nodesplitter \u2013 Function to split data by node. Defaults to single_node_only. workersplitter \u2013 Function to split data by worker. Defaults to split_by_worker. select_files \u2013 Function to select files from tar archives. Defaults to None. rename_files \u2013 Function to rename files from tar archives. Defaults to None. empty_check \u2013 Whether to check for empty datasets. Defaults to True. verbose \u2013 Whether to print verbose output. Defaults to False. seed \u2013 Random seed for shuffling. Defaults to None. Raises: ValueError \u2013 If the cache directory does not exist or if the URL type is not supported. Source code in webdataset/compat.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 class WebDataset ( DataPipeline , FluidInterface ): \"\"\"Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Args: urls: The source URLs or specifications for the dataset. handler: Function to handle exceptions. Defaults to reraise_exception. mode: The mode of operation. Defaults to None. resampled: Whether to use resampled mode. Defaults to False. repeat: Whether to repeat the dataset. Defaults to False. shardshuffle: The number of shards to shuffle, or None. Defaults to None. cache_size: The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir: The directory to use for caching. Defaults to None. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle: Whether to use deterministic shuffling. Defaults to False. nodesplitter: Function to split data by node. Defaults to single_node_only. workersplitter: Function to split data by worker. Defaults to split_by_worker. select_files: Function to select files from tar archives. Defaults to None. rename_files: Function to rename files from tar archives. Defaults to None. empty_check: Whether to check for empty datasets. Defaults to True. verbose: Whether to print verbose output. Defaults to False. seed: Random seed for shuffling. Defaults to None. Raises: ValueError: If the cache directory does not exist or if the URL type is not supported. \"\"\" def __init__ ( self , urls , handler = reraise_exception , mode = None , resampled = False , repeat = False , shardshuffle = None , cache_size =- 1 , cache_dir = None , url_to_name = cache . pipe_cleaner , detshuffle = False , nodesplitter = shardlists . single_node_only , workersplitter = shardlists . split_by_worker , select_files = None , rename_files = None , empty_check = True , verbose = False , seed = None , ): super () . __init__ () if resampled : mode = \"resampled\" if mode == \"resampled\" and shardshuffle not in ( False , None ): warnings . warn ( \"WebDataset(shardshuffle=...) is ignored for resampled datasets\" ) elif shardshuffle is None : warnings . warn ( \"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\" ) if shardshuffle is True : warnings . warn ( \"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\" ) shardshuffle = 100 args = SimpleNamespace ( ** locals ()) self . seed = seed or os . environ . get ( \"WDS_SEED\" , random . randint ( 0 , 1000000 )) self . update_cache_info ( args ) # first, we add a generator for the urls to used # this generates a stream of dict(url=...) self . create_url_iterator ( args ) # split by node (for distributed processing) if nodesplitter is not None : self . append ( nodesplitter ) # split by worker (for DataLoader) if workersplitter : self . append ( shardlists . split_by_worker ) # add a shard shuffler if args . shardshuffle is not None : if args . detshuffle : self . append ( filters . detshuffle ( args . shardshuffle , seed = args . seed )) else : self . append ( filters . shuffle ( args . shardshuffle , seed = args . seed )) # next, we select a URL opener, either with or without caching # this generates a stream of dict(url=..., stream=...) if cache_dir is None or cache_size == 0 : opener = cache . StreamingOpen ( handler = handler ) else : opener = cache . FileCache ( cache_dir = cache_dir , cache_size = cache_size , handler = handler ) self . append ( opener ) # now we need to open each stream and read the tar files contained in it # this generates a stream of dict(fname=..., data=...) objects expander = pipelinefilter ( tar_file_expander ) self . append ( expander ( handler = handler , select_files = select_files , rename_files = rename_files ) ) # finally, the files need to be groups into samples # this generates a stream of dict(__key__=..., ...=...) objects grouper = pipelinefilter ( group_by_keys ) self . append ( grouper ( handler = handler )) # check for empty datasets if empty_check : self . append ( check_empty ) def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" ) def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardlist ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" ) def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close ()","title":"WebDataset"},{"location":"webdataset2/#webdataset.compat.WebDataset.__enter__","text":"Enter the runtime context for the WebDataset. Returns: self \u2013 The WebDataset instance. Source code in webdataset/compat.py 503 504 505 506 507 508 509 def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self","title":"__enter__"},{"location":"webdataset2/#webdataset.compat.WebDataset.__exit__","text":"Exit the runtime context for the WebDataset. Parameters: *args \u2013 Exception type, value, and traceback if an exception occurred. Source code in webdataset/compat.py 511 512 513 514 515 516 517 def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close ()","title":"__exit__"},{"location":"webdataset2/#webdataset.compat.WebDataset.create_url_iterator","text":"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the URL type is not supported or implemented. Source code in webdataset/compat.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardlist ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" )","title":"create_url_iterator"},{"location":"webdataset2/#webdataset.compat.WebDataset.update_cache_info","text":"Update cache information based on arguments and environment variables. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the specified cache directory does not exist. Source code in webdataset/compat.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" )","title":"update_cache_info"},{"location":"webdataset2/#webdataset.compat.WebLoader","text":"Bases: DataPipeline , FluidInterface A wrapper for DataLoader that adds a fluid interface. Source code in webdataset/compat.py 528 529 530 531 class WebLoader ( DataPipeline , FluidInterface ): \"\"\"A wrapper for DataLoader that adds a fluid interface.\"\"\" def __init__ ( self , * args , ** kw ): super () . __init__ ( DataLoader ( * args , ** kw ))","title":"WebLoader"},{"location":"webdataset2/#webdataset.compat.check_empty","text":"Check if the dataset is empty and yield samples. Parameters: source \u2013 An iterable source of samples. Yields: \u2013 The samples from the source. Raises: ValueError \u2013 If no samples are found in the dataset. Source code in webdataset/compat.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def check_empty ( source ): \"\"\"Check if the dataset is empty and yield samples. Args: source: An iterable source of samples. Yields: The samples from the source. Raises: ValueError: If no samples are found in the dataset. \"\"\" count = 0 for sample in source : yield sample count += 1 if count == 0 : raise ValueError ( \"No samples found in dataset; perhaps you have fewer shards than workers. \\n \" + \"Turn off using empty_check=False in the WebDataset constructor.\" )","title":"check_empty"},{"location":"webdataset2/#webdataset.downloader","text":"","title":"downloader"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader","text":"Download shards randomly from a source to a directory. This class can be run in two modes: - update_every: Keep filling the directory with shards until it contains nshards shards. - replace_every: Keep filling the directory with shards, removing a shard every polling period. Parameters: shards ( list ) \u2013 List of shard URLs to download from. nshards ( int ) \u2013 Number of shards to maintain in the directory. directory ( str , default: None ) \u2013 Directory to download shards to. pattern ( str , default: '*.{tar,tgz,tar.gz}' ) \u2013 Glob pattern for matching shard files. increment ( int , default: 999999 ) \u2013 Maximum number of shards to add in one update. maxsize ( int , default: 999999999999 ) \u2013 Maximum total size of downloaded shards in bytes. verbose ( bool , default: False ) \u2013 Whether to print verbose output. download ( function , default: None ) \u2013 Custom download function to use. errors ( str , default: 'ignore' ) \u2013 Error handling strategy ('ignore', 'warn', or 'fail'). Raises: AssertionError \u2013 If a shard filename doesn't match the given pattern. Source code in webdataset/downloader.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 class RandomShardDownloader : \"\"\"Download shards randomly from a source to a directory. This class can be run in two modes: - update_every: Keep filling the directory with shards until it contains nshards shards. - replace_every: Keep filling the directory with shards, removing a shard every polling period. Args: shards (list): List of shard URLs to download from. nshards (int): Number of shards to maintain in the directory. directory (str, optional): Directory to download shards to. pattern (str, optional): Glob pattern for matching shard files. increment (int, optional): Maximum number of shards to add in one update. maxsize (int, optional): Maximum total size of downloaded shards in bytes. verbose (bool, optional): Whether to print verbose output. download (function, optional): Custom download function to use. errors (str, optional): Error handling strategy ('ignore', 'warn', or 'fail'). Raises: AssertionError: If a shard filename doesn't match the given pattern. \"\"\" def __init__ ( self , shards , nshards , * , directory = None , pattern = \"*.{tar,tgz,tar.gz}\" , increment = 999999 , maxsize = 999999999999 , verbose = False , download = None , errors = \"ignore\" , # ignore, warn, fail ): \"\"\"Initialize the downloader with the given parameters.\"\"\" self . shards = shards self . directory = directory self . nshards = nshards self . pattern = pattern self . increment = increment self . errors = errors self . maxsize = maxsize self . verbose = verbose if isinstance ( download , str ): download = download_with ( download ) self . download = download or download_file # check that the file name components of the shards match the glob pattern; use fnmatch for shard in shards : assert fnmatch_with_braces ( os . path . basename ( shard ), pattern ), f \"shard { os . path . basename ( shard ) } does not match pattern { pattern } \" def list_files ( self , inactive = False ): \"\"\"List files in the download directory matching the given pattern. Args: inactive (bool, optional): Whether to include only inactive (non-temporary) files. Returns: list: A list of file paths matching the pattern. \"\"\" files = glob_with_braces ( os . path . join ( self . directory , self . pattern )) if not inactive : tempfiles = glob_with_braces ( os . path . join ( self . directory , \"*._*_\" )) files += [ file_of_tempfile ( tempfile ) for tempfile in tempfiles ] return list ( set ( files )) def set_directory ( self , directory ): \"\"\"Set the directory to download shards to. Args: directory (str): The path to the download directory. \"\"\" self . directory = directory def update ( self ): \"\"\"Download shards randomly from the source to the directory. Ensures that there are nshards shards in the directory. If there are fewer, download random shards from the source. Raises: RuntimeError: If unable to download the required number of shards. \"\"\" assert self . directory is not None , \"directory must be set\" files = self . list_files () start = len ( files ) for _ in range ( 10 * self . nshards ): files = self . list_files () total_size = total_file_size ( files ) if ( len ( files ) >= min ( self . nshards , start + self . increment ) or total_size > self . maxsize ): return shard = random . choice ( self . shards ) filename = os . path . basename ( shard ) if filename in files : continue tempdest = os . path . join ( self . directory , filename + f \"._ { os . getpid () } _\" ) if self . verbose : print ( f \"downloading { shard } to { tempdest } \" , file = sys . stderr ) try : self . download ( shard , tempdest ) except Exception as exn : print ( f \"download failed: { exn } \" , file = sys . stderr ) if self . errors == \"ignore\" : continue if self . errors == \"warn\" : print ( f \"ignoring error { exn } \" , file = sys . stderr ) continue raise try : os . rename ( tempdest , os . path . join ( self . directory , filename )) except FileExistsError : # some other process downloaded the same file os . unlink ( tempdest ) raise RuntimeError ( f \"unable to download { self . nshards } shards\" ) def sleep ( self , poll = 10 ): \"\"\"Sleep for a randomized duration based on the poll interval. Args: poll (float, optional): The base polling interval in seconds. \"\"\" delta = poll * random . uniform ( 0.7 , 1.3 ) time . sleep ( delta ) def update_every ( self , poll = 10 ): \"\"\"Repeatedly call update with a given delay. Args: poll (float, optional): The polling interval in seconds. \"\"\" while True : self . update () delta = poll * random . uniform ( 0.7 , 1.3 ) time . sleep ( delta ) def maybe_remove ( self , strategy = \"oldest\" ): \"\"\"Attempt to remove a shard if the number of shards exceeds the limit. Args: strategy (str, optional): The strategy for selecting which file to remove ('oldest' or 'random'). Returns: bool: True if a file was removed, False otherwise. Raises: ValueError: If an unknown strategy is provided. \"\"\" files = self . list_files () if len ( files ) > self . nshards : inactive = self . list_files ( inactive = True ) if len ( inactive ) == 0 : return False if strategy == \"oldest\" : selected = get_oldest_file ( inactive ) elif strategy == \"random\" : selected = random . choice ( inactive ) else : raise ValueError ( f \"unknown strategy { strategy } \" ) try : os . unlink ( selected ) return True except FileNotFoundError : return False return False def replace_every ( self , poll = 60 , strategy = \"oldest\" ): \"\"\"Repeatedly update and remove shards to maintain the desired number. Args: poll (float, optional): The polling interval in seconds. strategy (str, optional): The strategy for selecting which file to remove. \"\"\" while len ( self . list_files ()) >= self . nshards : if self . maybe_remove ( strategy = strategy ): self . update () self . sleep ( poll ) def run_job ( self , poll = 10 , mode = \"update\" , strategy = \"oldest\" ): \"\"\"Run the downloader job in the specified mode. Args: poll (float, optional): The polling interval in seconds. mode (str, optional): The mode to run in ('update' or 'replace'). strategy (str, optional): The strategy for selecting which file to remove in replace mode. Raises: ValueError: If an unknown mode is provided. \"\"\" if mode == \"update\" : self . update_every ( poll ) elif mode == \"replace\" : self . replace_every ( poll , strategy = strategy ) else : raise ValueError ( f \"unknown mode { mode } \" )","title":"RandomShardDownloader"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader.__init__","text":"Initialize the downloader with the given parameters. Source code in webdataset/downloader.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __init__ ( self , shards , nshards , * , directory = None , pattern = \"*.{tar,tgz,tar.gz}\" , increment = 999999 , maxsize = 999999999999 , verbose = False , download = None , errors = \"ignore\" , # ignore, warn, fail ): \"\"\"Initialize the downloader with the given parameters.\"\"\" self . shards = shards self . directory = directory self . nshards = nshards self . pattern = pattern self . increment = increment self . errors = errors self . maxsize = maxsize self . verbose = verbose if isinstance ( download , str ): download = download_with ( download ) self . download = download or download_file # check that the file name components of the shards match the glob pattern; use fnmatch for shard in shards : assert fnmatch_with_braces ( os . path . basename ( shard ), pattern ), f \"shard { os . path . basename ( shard ) } does not match pattern { pattern } \"","title":"__init__"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader.list_files","text":"List files in the download directory matching the given pattern. Parameters: inactive ( bool , default: False ) \u2013 Whether to include only inactive (non-temporary) files. Returns: list \u2013 A list of file paths matching the pattern. Source code in webdataset/downloader.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def list_files ( self , inactive = False ): \"\"\"List files in the download directory matching the given pattern. Args: inactive (bool, optional): Whether to include only inactive (non-temporary) files. Returns: list: A list of file paths matching the pattern. \"\"\" files = glob_with_braces ( os . path . join ( self . directory , self . pattern )) if not inactive : tempfiles = glob_with_braces ( os . path . join ( self . directory , \"*._*_\" )) files += [ file_of_tempfile ( tempfile ) for tempfile in tempfiles ] return list ( set ( files ))","title":"list_files"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader.maybe_remove","text":"Attempt to remove a shard if the number of shards exceeds the limit. Parameters: strategy ( str , default: 'oldest' ) \u2013 The strategy for selecting which file to remove ('oldest' or 'random'). Returns: bool \u2013 True if a file was removed, False otherwise. Raises: ValueError \u2013 If an unknown strategy is provided. Source code in webdataset/downloader.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def maybe_remove ( self , strategy = \"oldest\" ): \"\"\"Attempt to remove a shard if the number of shards exceeds the limit. Args: strategy (str, optional): The strategy for selecting which file to remove ('oldest' or 'random'). Returns: bool: True if a file was removed, False otherwise. Raises: ValueError: If an unknown strategy is provided. \"\"\" files = self . list_files () if len ( files ) > self . nshards : inactive = self . list_files ( inactive = True ) if len ( inactive ) == 0 : return False if strategy == \"oldest\" : selected = get_oldest_file ( inactive ) elif strategy == \"random\" : selected = random . choice ( inactive ) else : raise ValueError ( f \"unknown strategy { strategy } \" ) try : os . unlink ( selected ) return True except FileNotFoundError : return False return False","title":"maybe_remove"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader.replace_every","text":"Repeatedly update and remove shards to maintain the desired number. Parameters: poll ( float , default: 60 ) \u2013 The polling interval in seconds. strategy ( str , default: 'oldest' ) \u2013 The strategy for selecting which file to remove. Source code in webdataset/downloader.py 270 271 272 273 274 275 276 277 278 279 280 def replace_every ( self , poll = 60 , strategy = \"oldest\" ): \"\"\"Repeatedly update and remove shards to maintain the desired number. Args: poll (float, optional): The polling interval in seconds. strategy (str, optional): The strategy for selecting which file to remove. \"\"\" while len ( self . list_files ()) >= self . nshards : if self . maybe_remove ( strategy = strategy ): self . update () self . sleep ( poll )","title":"replace_every"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader.run_job","text":"Run the downloader job in the specified mode. Parameters: poll ( float , default: 10 ) \u2013 The polling interval in seconds. mode ( str , default: 'update' ) \u2013 The mode to run in ('update' or 'replace'). strategy ( str , default: 'oldest' ) \u2013 The strategy for selecting which file to remove in replace mode. Raises: ValueError \u2013 If an unknown mode is provided. Source code in webdataset/downloader.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def run_job ( self , poll = 10 , mode = \"update\" , strategy = \"oldest\" ): \"\"\"Run the downloader job in the specified mode. Args: poll (float, optional): The polling interval in seconds. mode (str, optional): The mode to run in ('update' or 'replace'). strategy (str, optional): The strategy for selecting which file to remove in replace mode. Raises: ValueError: If an unknown mode is provided. \"\"\" if mode == \"update\" : self . update_every ( poll ) elif mode == \"replace\" : self . replace_every ( poll , strategy = strategy ) else : raise ValueError ( f \"unknown mode { mode } \" )","title":"run_job"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader.set_directory","text":"Set the directory to download shards to. Parameters: directory ( str ) \u2013 The path to the download directory. Source code in webdataset/downloader.py 167 168 169 170 171 172 173 def set_directory ( self , directory ): \"\"\"Set the directory to download shards to. Args: directory (str): The path to the download directory. \"\"\" self . directory = directory","title":"set_directory"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader.sleep","text":"Sleep for a randomized duration based on the poll interval. Parameters: poll ( float , default: 10 ) \u2013 The base polling interval in seconds. Source code in webdataset/downloader.py 220 221 222 223 224 225 226 227 def sleep ( self , poll = 10 ): \"\"\"Sleep for a randomized duration based on the poll interval. Args: poll (float, optional): The base polling interval in seconds. \"\"\" delta = poll * random . uniform ( 0.7 , 1.3 ) time . sleep ( delta )","title":"sleep"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader.update","text":"Download shards randomly from the source to the directory. Ensures that there are nshards shards in the directory. If there are fewer, download random shards from the source. Raises: RuntimeError \u2013 If unable to download the required number of shards. Source code in webdataset/downloader.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def update ( self ): \"\"\"Download shards randomly from the source to the directory. Ensures that there are nshards shards in the directory. If there are fewer, download random shards from the source. Raises: RuntimeError: If unable to download the required number of shards. \"\"\" assert self . directory is not None , \"directory must be set\" files = self . list_files () start = len ( files ) for _ in range ( 10 * self . nshards ): files = self . list_files () total_size = total_file_size ( files ) if ( len ( files ) >= min ( self . nshards , start + self . increment ) or total_size > self . maxsize ): return shard = random . choice ( self . shards ) filename = os . path . basename ( shard ) if filename in files : continue tempdest = os . path . join ( self . directory , filename + f \"._ { os . getpid () } _\" ) if self . verbose : print ( f \"downloading { shard } to { tempdest } \" , file = sys . stderr ) try : self . download ( shard , tempdest ) except Exception as exn : print ( f \"download failed: { exn } \" , file = sys . stderr ) if self . errors == \"ignore\" : continue if self . errors == \"warn\" : print ( f \"ignoring error { exn } \" , file = sys . stderr ) continue raise try : os . rename ( tempdest , os . path . join ( self . directory , filename )) except FileExistsError : # some other process downloaded the same file os . unlink ( tempdest ) raise RuntimeError ( f \"unable to download { self . nshards } shards\" )","title":"update"},{"location":"webdataset2/#webdataset.downloader.RandomShardDownloader.update_every","text":"Repeatedly call update with a given delay. Parameters: poll ( float , default: 10 ) \u2013 The polling interval in seconds. Source code in webdataset/downloader.py 229 230 231 232 233 234 235 236 237 238 def update_every ( self , poll = 10 ): \"\"\"Repeatedly call update with a given delay. Args: poll (float, optional): The polling interval in seconds. \"\"\" while True : self . update () delta = poll * random . uniform ( 0.7 , 1.3 ) time . sleep ( delta )","title":"update_every"},{"location":"webdataset2/#webdataset.downloader.download_file","text":"Download a file from a URL. Parameters: url ( str ) \u2013 The URL of the file to download. filename ( str ) \u2013 The local path where the file will be saved. Returns: \u2013 None Source code in webdataset/downloader.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def download_file ( url , filename ): \"\"\"Download a file from a URL. Args: url (str): The URL of the file to download. filename (str): The local path where the file will be saved. Returns: None \"\"\" with gopen . gopen ( url , \"rb\" ) as stream : with open ( filename , \"wb\" ) as out : while True : chunk = stream . read ( 1024 * 1024 ) if len ( chunk ) == 0 : break out . write ( chunk )","title":"download_file"},{"location":"webdataset2/#webdataset.downloader.download_with","text":"Create a download function using a custom command. Parameters: command ( str ) \u2013 The command to use for downloading, containing {url} and {output} placeholders. Returns: function \u2013 A function that takes a URL and filename as arguments and downloads the file. Source code in webdataset/downloader.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def download_with ( command ): \"\"\"Create a download function using a custom command. Args: command (str): The command to use for downloading, containing {url} and {output} placeholders. Returns: function: A function that takes a URL and filename as arguments and downloads the file. \"\"\" def download ( url , filename ): return subprocess . check_call ( command . format ( url = url , output = filename ), shell = True ) return download","title":"download_with"},{"location":"webdataset2/#webdataset.downloader.file_of_tempfile","text":"Get the original file name from a temporary file name. Parameters: tempfile ( str ) \u2013 The temporary file name. Returns: str \u2013 The original file name without the temporary suffix. Raises: AssertionError \u2013 If the tempfile doesn't end with '_' or doesn't contain a period. Source code in webdataset/downloader.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def file_of_tempfile ( tempfile ): \"\"\"Get the original file name from a temporary file name. Args: tempfile (str): The temporary file name. Returns: str: The original file name without the temporary suffix. Raises: AssertionError: If the tempfile doesn't end with '_' or doesn't contain a period. \"\"\" assert tempfile . endswith ( \"_\" ) and \".\" in tempfile return tempfile . rsplit ( \".\" , 1 )[ 0 ]","title":"file_of_tempfile"},{"location":"webdataset2/#webdataset.downloader.get_oldest_file","text":"Find the oldest file in a list of files. Parameters: files ( list ) \u2013 A list of file paths. Returns: str \u2013 The path of the oldest file. Source code in webdataset/downloader.py 87 88 89 90 91 92 93 94 95 96 def get_oldest_file ( files ): \"\"\"Find the oldest file in a list of files. Args: files (list): A list of file paths. Returns: str: The path of the oldest file. \"\"\" return min ( files , key = os . path . getmtime )","title":"get_oldest_file"},{"location":"webdataset2/#webdataset.downloader.random_downloader","text":"Start multiple jobs to download shards randomly from the given list of shards. Parameters: shards ( List [ str ] ) \u2013 List of shard URLs or patterns to download from. directory ( Optional [ str ] , default: None ) \u2013 Directory to download shards to. nshards ( int , default: 10 ) \u2013 Number of shards to maintain in the directory. command ( Optional [ str ] , default: None ) \u2013 Custom download command to use. pattern ( str , default: '*.{tar,tgz,tar.gz}' ) \u2013 Glob pattern for matching shard files. increment ( int , default: 999999 ) \u2013 Maximum number of shards to add in one update. maxsize ( int , default: 999999999999 ) \u2013 Maximum total size of downloaded shards in bytes. njobs ( int , default: 1 ) \u2013 Number of parallel download jobs to run. poll ( float , default: 10 ) \u2013 Polling interval in seconds. mode ( str , default: 'update' ) \u2013 Mode to run in ('update' or 'replace'). errors ( str , default: 'ignore' ) \u2013 Error handling strategy ('ignore', 'warn', or 'fail'). verbose ( bool , default: False ) \u2013 Whether to print verbose output. Raises: AssertionError \u2013 If the directory is not provided. Source code in webdataset/downloader.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 @app . command () def random_downloader ( shards : List [ str ], * , directory : Optional [ str ] = None , nshards : int = 10 , command : Optional [ str ] = None , pattern : str = \"*.{tar,tgz,tar.gz}\" , increment : int = 999999 , maxsize : int = 999999999999 , njobs : int = 1 , poll : float = 10 , mode : str = \"update\" , errors : str = \"ignore\" , verbose : bool = False , ): \"\"\"Start multiple jobs to download shards randomly from the given list of shards. Args: shards (List[str]): List of shard URLs or patterns to download from. directory (Optional[str]): Directory to download shards to. nshards (int): Number of shards to maintain in the directory. command (Optional[str]): Custom download command to use. pattern (str): Glob pattern for matching shard files. increment (int): Maximum number of shards to add in one update. maxsize (int): Maximum total size of downloaded shards in bytes. njobs (int): Number of parallel download jobs to run. poll (float): Polling interval in seconds. mode (str): Mode to run in ('update' or 'replace'). errors (str): Error handling strategy ('ignore', 'warn', or 'fail'). verbose (bool): Whether to print verbose output. Raises: AssertionError: If the directory is not provided. \"\"\" assert directory is not None shards = [ fname for shard in shards for fname in braceexpand . braceexpand ( shard )] print ( f \"got { len ( shards ) } shards\" , file = sys . stderr ) if njobs > 1 : pool = multiprocessing . Pool ( njobs ) for _ in range ( njobs ): pool . apply_async ( RandomShardDownloader ( shards , nshards , directory = directory , pattern = pattern , increment = increment , maxsize = maxsize , download = command , errors = errors , verbose = verbose , ) . run_job , kwds = dict ( poll = poll , mode = mode ), ) else : RandomShardDownloader ( shards , nshards , directory = directory , pattern = pattern , increment = increment , maxsize = maxsize , download = command , errors = errors , verbose = verbose , ) . run_job ( poll = poll , mode = mode )","title":"random_downloader"},{"location":"webdataset2/#webdataset.downloader.total_file_size","text":"Calculate the total size of a list of files. Parameters: files ( list ) \u2013 A list of file paths. Returns: int \u2013 The total size of all files in bytes. Source code in webdataset/downloader.py 59 60 61 62 63 64 65 66 67 68 def total_file_size ( files ): \"\"\"Calculate the total size of a list of files. Args: files (list): A list of file paths. Returns: int: The total size of all files in bytes. \"\"\" return sum ( os . path . getsize ( f ) for f in files )","title":"total_file_size"},{"location":"webdataset2/#webdataset.extradatasets","text":"Train PyTorch models directly from POSIX tar archive. Code works locally or over HTTP connections.","title":"extradatasets"},{"location":"webdataset2/#webdataset.extradatasets.MockDataset","text":"Bases: IterableDataset Create a mock dataset for performance testing and unit testing. Parameters: sample \u2013 The sample to be returned repeatedly. length ( int ) \u2013 The length of the mock dataset. Source code in webdataset/extradatasets.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class MockDataset ( IterableDataset ): \"\"\"Create a mock dataset for performance testing and unit testing. Args: sample: The sample to be returned repeatedly. length (int): The length of the mock dataset. \"\"\" def __init__ ( self , sample , length ): self . sample = sample self . length = length def __iter__ ( self ): \"\"\"Yield samples from the mock dataset. Returns: Iterator: An iterator that yields the same sample repeatedly. \"\"\" for _ in range ( self . length ): yield self . sample","title":"MockDataset"},{"location":"webdataset2/#webdataset.extradatasets.MockDataset.__iter__","text":"Yield samples from the mock dataset. Returns: Iterator \u2013 An iterator that yields the same sample repeatedly. Source code in webdataset/extradatasets.py 31 32 33 34 35 36 37 38 def __iter__ ( self ): \"\"\"Yield samples from the mock dataset. Returns: Iterator: An iterator that yields the same sample repeatedly. \"\"\" for _ in range ( self . length ): yield self . sample","title":"__iter__"},{"location":"webdataset2/#webdataset.extradatasets.repeatedly","text":"Bases: IterableDataset , PipelineStage Repeatedly yield samples from a dataset. Parameters: source \u2013 The source dataset to repeat. nepochs ( int , default: None ) \u2013 Maximum number of epochs to repeat. nbatches ( int , default: None ) \u2013 Maximum number of batches to repeat. length ( int , default: None ) \u2013 Length of the repeated dataset. Source code in webdataset/extradatasets.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class repeatedly ( IterableDataset , PipelineStage ): \"\"\"Repeatedly yield samples from a dataset. Args: source: The source dataset to repeat. nepochs (int, optional): Maximum number of epochs to repeat. nbatches (int, optional): Maximum number of batches to repeat. length (int, optional): Length of the repeated dataset. \"\"\" def __init__ ( self , source , nepochs = None , nbatches = None , length = None ): self . source = source self . length = length self . nbatches = nbatches def invoke ( self , source ): \"\"\"Return an iterator that iterates repeatedly over a source. Args: source: The source dataset to repeat. Returns: Iterator: An iterator that repeatedly yields samples from the source. \"\"\" return utils . repeatedly ( source , nepochs = self . nepochs , nbatches = self . nbatches , )","title":"repeatedly"},{"location":"webdataset2/#webdataset.extradatasets.repeatedly.invoke","text":"Return an iterator that iterates repeatedly over a source. Parameters: source \u2013 The source dataset to repeat. Returns: Iterator \u2013 An iterator that repeatedly yields samples from the source. Source code in webdataset/extradatasets.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def invoke ( self , source ): \"\"\"Return an iterator that iterates repeatedly over a source. Args: source: The source dataset to repeat. Returns: Iterator: An iterator that repeatedly yields samples from the source. \"\"\" return utils . repeatedly ( source , nepochs = self . nepochs , nbatches = self . nbatches , )","title":"invoke"},{"location":"webdataset2/#webdataset.extradatasets.with_epoch","text":"Bases: IterableDataset Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Parameters: dataset \u2013 The source IterableDataset. length ( int ) \u2013 Declared length of the dataset. Source code in webdataset/extradatasets.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class with_epoch ( IterableDataset ): \"\"\"Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Args: dataset: The source IterableDataset. length (int): Declared length of the dataset. \"\"\" def __init__ ( self , dataset , length ): super () . __init__ () self . length = length self . source = None def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None","title":"with_epoch"},{"location":"webdataset2/#webdataset.extradatasets.with_epoch.__getstate__","text":"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict \u2013 A dictionary representing the pickled state of the dataset. Source code in webdataset/extradatasets.py 91 92 93 94 95 96 97 98 99 100 101 def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result","title":"__getstate__"},{"location":"webdataset2/#webdataset.extradatasets.with_epoch.invoke","text":"Return an iterator over the dataset. This iterator returns as many samples as given by the length parameter. Parameters: dataset \u2013 The source dataset to iterate over. Yields: Sample \u2013 The next sample from the dataset. Source code in webdataset/extradatasets.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None","title":"invoke"},{"location":"webdataset2/#webdataset.extradatasets.with_length","text":"Bases: IterableDataset , PipelineStage Repeatedly yield samples from a dataset with a specified length. Parameters: dataset \u2013 The source dataset. length ( int ) \u2013 The stated length of the dataset. Source code in webdataset/extradatasets.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class with_length ( IterableDataset , PipelineStage ): \"\"\"Repeatedly yield samples from a dataset with a specified length. Args: dataset: The source dataset. length (int): The stated length of the dataset. \"\"\" def __init__ ( self , dataset , length ): super () . __init__ () self . dataset = dataset self . length = length def invoke ( self , dataset ): \"\"\"Return an iterator that iterates over the source dataset. Args: dataset: The source dataset to iterate over. Returns: Iterator: An iterator over the source dataset. \"\"\" return iter ( dataset ) def __len__ ( self ): \"\"\"Return the user specified length. Returns: int: The specified length of the dataset. \"\"\" return self . length","title":"with_length"},{"location":"webdataset2/#webdataset.extradatasets.with_length.__len__","text":"Return the user specified length. Returns: int \u2013 The specified length of the dataset. Source code in webdataset/extradatasets.py 153 154 155 156 157 158 159 def __len__ ( self ): \"\"\"Return the user specified length. Returns: int: The specified length of the dataset. \"\"\" return self . length","title":"__len__"},{"location":"webdataset2/#webdataset.extradatasets.with_length.invoke","text":"Return an iterator that iterates over the source dataset. Parameters: dataset \u2013 The source dataset to iterate over. Returns: Iterator \u2013 An iterator over the source dataset. Source code in webdataset/extradatasets.py 142 143 144 145 146 147 148 149 150 151 def invoke ( self , dataset ): \"\"\"Return an iterator that iterates over the source dataset. Args: dataset: The source dataset to iterate over. Returns: Iterator: An iterator over the source dataset. \"\"\" return iter ( dataset )","title":"invoke"},{"location":"webdataset2/#webdataset.filters","text":"A collection of iterators for data transformations. These functions are plain iterator functions. You can find curried versions in webdataset.filters, and you can find IterableDataset wrappers in webdataset.processing.","title":"filters"},{"location":"webdataset2/#webdataset.filters.Cached","text":"Bases: PipelineStage A pipeline stage that caches its output. This stage will cache all samples that pass through it, allowing subsequent iterations to use the cached data instead of recomputing it. Source code in webdataset/filters.py 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 class Cached ( PipelineStage ): \"\"\" A pipeline stage that caches its output. This stage will cache all samples that pass through it, allowing subsequent iterations to use the cached data instead of recomputing it. \"\"\" def __init__ ( self ): \"\"\"Initialize the Cached pipeline stage.\"\"\" super () . __init__ () self . cached = None def run ( self , source ): \"\"\" Run the caching process on the input source. Args: source: Input data source to be cached. Yields: Samples from the source, caching them for future use. \"\"\" if self . cached is None : self . temp = [] for sample in source : self . temp . append ( sample ) yield sample self . cached = self . temp else : yield from self . cached","title":"Cached"},{"location":"webdataset2/#webdataset.filters.Cached.__init__","text":"Initialize the Cached pipeline stage. Source code in webdataset/filters.py 1064 1065 1066 1067 def __init__ ( self ): \"\"\"Initialize the Cached pipeline stage.\"\"\" super () . __init__ () self . cached = None","title":"__init__"},{"location":"webdataset2/#webdataset.filters.Cached.run","text":"Run the caching process on the input source. Parameters: source \u2013 Input data source to be cached. Yields: \u2013 Samples from the source, caching them for future use. Source code in webdataset/filters.py 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 def run ( self , source ): \"\"\" Run the caching process on the input source. Args: source: Input data source to be cached. Yields: Samples from the source, caching them for future use. \"\"\" if self . cached is None : self . temp = [] for sample in source : self . temp . append ( sample ) yield sample self . cached = self . temp else : yield from self . cached","title":"run"},{"location":"webdataset2/#webdataset.filters.FilterFunction","text":"Bases: object Helper class for currying pipeline stages. This class is used to create a curried function that can be pickled. Attributes: f \u2013 The function to be curried. args \u2013 Positional arguments for the function. kw \u2013 Keyword arguments for the function. Source code in webdataset/filters.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class FilterFunction ( object ): \"\"\" Helper class for currying pipeline stages. This class is used to create a curried function that can be pickled. Attributes: f: The function to be curried. args: Positional arguments for the function. kw: Keyword arguments for the function. \"\"\" def __init__ ( self , f , * args , ** kw ): \"\"\" Create a curried function. Args: f: The function to be curried. *args: Positional arguments for the function. **kw: Keyword arguments for the function. \"\"\" self . f = f self . args = args self . kw = kw def __call__ ( self , data ): \"\"\" Call the curried function with the given argument. Args: data: The data to be processed by the curried function. Returns: The result of calling the curried function with the given data and stored arguments. \"\"\" return self . f ( data , * self . args , ** self . kw ) def __str__ ( self ): \"\"\" Compute a string representation. Returns: str: A string representation of the FilterFunction object. \"\"\" return f \"< { self . f . __name__ } { self . args } { self . kw } >\" def __repr__ ( self ): \"\"\" Compute a string representation. Returns: str: A string representation of the FilterFunction object. \"\"\" return f \"< { self . f . __name__ } { self . args } { self . kw } >\"","title":"FilterFunction"},{"location":"webdataset2/#webdataset.filters.FilterFunction.__call__","text":"Call the curried function with the given argument. Parameters: data \u2013 The data to be processed by the curried function. Returns: \u2013 The result of calling the curried function with the given data and stored arguments. Source code in webdataset/filters.py 58 59 60 61 62 63 64 65 66 67 68 def __call__ ( self , data ): \"\"\" Call the curried function with the given argument. Args: data: The data to be processed by the curried function. Returns: The result of calling the curried function with the given data and stored arguments. \"\"\" return self . f ( data , * self . args , ** self . kw )","title":"__call__"},{"location":"webdataset2/#webdataset.filters.FilterFunction.__init__","text":"Create a curried function. Parameters: f \u2013 The function to be curried. *args \u2013 Positional arguments for the function. **kw \u2013 Keyword arguments for the function. Source code in webdataset/filters.py 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , f , * args , ** kw ): \"\"\" Create a curried function. Args: f: The function to be curried. *args: Positional arguments for the function. **kw: Keyword arguments for the function. \"\"\" self . f = f self . args = args self . kw = kw","title":"__init__"},{"location":"webdataset2/#webdataset.filters.FilterFunction.__repr__","text":"Compute a string representation. Returns: str \u2013 A string representation of the FilterFunction object. Source code in webdataset/filters.py 79 80 81 82 83 84 85 86 def __repr__ ( self ): \"\"\" Compute a string representation. Returns: str: A string representation of the FilterFunction object. \"\"\" return f \"< { self . f . __name__ } { self . args } { self . kw } >\"","title":"__repr__"},{"location":"webdataset2/#webdataset.filters.FilterFunction.__str__","text":"Compute a string representation. Returns: str \u2013 A string representation of the FilterFunction object. Source code in webdataset/filters.py 70 71 72 73 74 75 76 77 def __str__ ( self ): \"\"\" Compute a string representation. Returns: str: A string representation of the FilterFunction object. \"\"\" return f \"< { self . f . __name__ } { self . args } { self . kw } >\"","title":"__str__"},{"location":"webdataset2/#webdataset.filters.LMDBCached","text":"Bases: PipelineStage A pipeline stage that caches its output in an LMDB database. This stage will cache all samples that pass through it in an LMDB database, allowing subsequent iterations to use the cached data instead of recomputing it. Source code in webdataset/filters.py 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 class LMDBCached ( PipelineStage ): \"\"\" A pipeline stage that caches its output in an LMDB database. This stage will cache all samples that pass through it in an LMDB database, allowing subsequent iterations to use the cached data instead of recomputing it. \"\"\" def __init__ ( self , fname , map_size = 1e12 , pickler = pickle , chunksize = 500 ): \"\"\" Initialize the LMDBCached pipeline stage. Args: fname (str): Filename for the LMDB database. map_size (int): Maximum size database may grow to. pickler: Module to use for pickling (default is Python's pickle module). chunksize (int): Number of samples to write in each transaction. \"\"\" import lmdb self . db = lmdb . open ( fname , readonly = False , map_size = int ( map_size )) self . pickler = pickler self . chunksize = chunksize def is_complete ( self ): \"\"\" Check if the database is complete. Returns: bool: True if the database is complete, False otherwise. \"\"\" with self . db . begin ( write = False ) as txn : return txn . get ( b \"_\" ) is not None def add_samples ( self , samples ): \"\"\" Add samples to the database. Args: samples: Iterable of (key, sample) pairs to add to the database. \"\"\" with self . db . begin ( write = True ) as txn : for key , sample in samples : txn . put ( key . encode (), self . pickler . dumps ( sample )) def run ( self , source ): \"\"\" Run the caching process on the input source. If the database is complete, yield samples from the database. Otherwise, yield samples from the source and cache them in the database. Args: source: Input data source to be cached. Yields: Samples from the source or the database. \"\"\" if self . is_complete (): with self . db . begin ( write = False ) as txn : for key , value in txn . cursor (): if key == b \"_\" : continue yield self . pickler . loads ( value ) else : buffer = [] for i , sample in enumerate ( source ): key = ( isinstance ( sample , dict ) and sample . get ( \"__key__\" )) or str ( i ) buffer . append (( key , sample )) if len ( buffer ) >= self . chunksize : self . add_samples ( buffer ) buffer = [] yield sample if len ( buffer ) > 0 : self . add_samples ( buffer ) with self . db . begin ( write = True ) as txn : txn . put ( b \"_\" , b \"1\" )","title":"LMDBCached"},{"location":"webdataset2/#webdataset.filters.LMDBCached.__init__","text":"Initialize the LMDBCached pipeline stage. Parameters: fname ( str ) \u2013 Filename for the LMDB database. map_size ( int , default: 1000000000000.0 ) \u2013 Maximum size database may grow to. pickler \u2013 Module to use for pickling (default is Python's pickle module). chunksize ( int , default: 500 ) \u2013 Number of samples to write in each transaction. Source code in webdataset/filters.py 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 def __init__ ( self , fname , map_size = 1e12 , pickler = pickle , chunksize = 500 ): \"\"\" Initialize the LMDBCached pipeline stage. Args: fname (str): Filename for the LMDB database. map_size (int): Maximum size database may grow to. pickler: Module to use for pickling (default is Python's pickle module). chunksize (int): Number of samples to write in each transaction. \"\"\" import lmdb self . db = lmdb . open ( fname , readonly = False , map_size = int ( map_size )) self . pickler = pickler self . chunksize = chunksize","title":"__init__"},{"location":"webdataset2/#webdataset.filters.LMDBCached.add_samples","text":"Add samples to the database. Parameters: samples \u2013 Iterable of (key, sample) pairs to add to the database. Source code in webdataset/filters.py 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 def add_samples ( self , samples ): \"\"\" Add samples to the database. Args: samples: Iterable of (key, sample) pairs to add to the database. \"\"\" with self . db . begin ( write = True ) as txn : for key , sample in samples : txn . put ( key . encode (), self . pickler . dumps ( sample ))","title":"add_samples"},{"location":"webdataset2/#webdataset.filters.LMDBCached.is_complete","text":"Check if the database is complete. Returns: bool \u2013 True if the database is complete, False otherwise. Source code in webdataset/filters.py 1113 1114 1115 1116 1117 1118 1119 1120 1121 def is_complete ( self ): \"\"\" Check if the database is complete. Returns: bool: True if the database is complete, False otherwise. \"\"\" with self . db . begin ( write = False ) as txn : return txn . get ( b \"_\" ) is not None","title":"is_complete"},{"location":"webdataset2/#webdataset.filters.LMDBCached.run","text":"Run the caching process on the input source. If the database is complete, yield samples from the database. Otherwise, yield samples from the source and cache them in the database. Parameters: source \u2013 Input data source to be cached. Yields: \u2013 Samples from the source or the database. Source code in webdataset/filters.py 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 def run ( self , source ): \"\"\" Run the caching process on the input source. If the database is complete, yield samples from the database. Otherwise, yield samples from the source and cache them in the database. Args: source: Input data source to be cached. Yields: Samples from the source or the database. \"\"\" if self . is_complete (): with self . db . begin ( write = False ) as txn : for key , value in txn . cursor (): if key == b \"_\" : continue yield self . pickler . loads ( value ) else : buffer = [] for i , sample in enumerate ( source ): key = ( isinstance ( sample , dict ) and sample . get ( \"__key__\" )) or str ( i ) buffer . append (( key , sample )) if len ( buffer ) >= self . chunksize : self . add_samples ( buffer ) buffer = [] yield sample if len ( buffer ) > 0 : self . add_samples ( buffer ) with self . db . begin ( write = True ) as txn : txn . put ( b \"_\" , b \"1\" )","title":"run"},{"location":"webdataset2/#webdataset.filters.RestCurried","text":"Bases: object Helper class for currying pipeline stages. This class is used to create a curried function that can be pickled. Attributes: f \u2013 The function to be curried. Source code in webdataset/filters.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class RestCurried ( object ): \"\"\" Helper class for currying pipeline stages. This class is used to create a curried function that can be pickled. Attributes: f: The function to be curried. \"\"\" def __init__ ( self , f ): \"\"\" Store the function for future currying. Args: f: The function to be curried. \"\"\" self . f = f def __call__ ( self , * args , ** kw ): \"\"\" Curry with the given arguments. Args: *args: Positional arguments for the function. **kw: Keyword arguments for the function. Returns: FilterFunction: A FilterFunction object with the curried function and arguments. \"\"\" return FilterFunction ( self . f , * args , ** kw )","title":"RestCurried"},{"location":"webdataset2/#webdataset.filters.RestCurried.__call__","text":"Curry with the given arguments. Parameters: *args \u2013 Positional arguments for the function. **kw \u2013 Keyword arguments for the function. Returns: FilterFunction \u2013 A FilterFunction object with the curried function and arguments. Source code in webdataset/filters.py 108 109 110 111 112 113 114 115 116 117 118 119 def __call__ ( self , * args , ** kw ): \"\"\" Curry with the given arguments. Args: *args: Positional arguments for the function. **kw: Keyword arguments for the function. Returns: FilterFunction: A FilterFunction object with the curried function and arguments. \"\"\" return FilterFunction ( self . f , * args , ** kw )","title":"__call__"},{"location":"webdataset2/#webdataset.filters.RestCurried.__init__","text":"Store the function for future currying. Parameters: f \u2013 The function to be curried. Source code in webdataset/filters.py 99 100 101 102 103 104 105 106 def __init__ ( self , f ): \"\"\" Store the function for future currying. Args: f: The function to be curried. \"\"\" self . f = f","title":"__init__"},{"location":"webdataset2/#webdataset.filters.detshuffle","text":"Bases: PipelineStage A deterministic shuffling stage for the pipeline. This class provides a reproducible shuffling mechanism based on a seed and epoch. Attributes: bufsize ( int ) \u2013 Size of the buffer for shuffling. initial ( int ) \u2013 Initial number of samples to collect before shuffling. seed ( int ) \u2013 Seed for the random number generator. epoch ( int ) \u2013 Current epoch number. Source code in webdataset/filters.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 class detshuffle ( PipelineStage ): \"\"\" A deterministic shuffling stage for the pipeline. This class provides a reproducible shuffling mechanism based on a seed and epoch. Attributes: bufsize (int): Size of the buffer for shuffling. initial (int): Initial number of samples to collect before shuffling. seed (int): Seed for the random number generator. epoch (int): Current epoch number. \"\"\" def __init__ ( self , bufsize = 1000 , initial = 100 , seed = 0 , epoch =- 1 ): \"\"\" Initialize the detshuffle stage. Args: bufsize (int): Size of the buffer for shuffling. initial (int): Initial number of samples to collect before shuffling. seed (int): Seed for the random number generator. epoch (int): Starting epoch number. \"\"\" self . bufsize = bufsize self . initial = initial self . seed = seed self . epoch = epoch def run ( self , src ): \"\"\" Run the shuffling process on the input source. Args: src: Input data source to be shuffled. Returns: Iterator: Shuffled data iterator. \"\"\" self . epoch += 1 rng = random . Random () rng . seed ( self . seed + self . epoch ) return _shuffle ( src , self . bufsize , self . initial , rng )","title":"detshuffle"},{"location":"webdataset2/#webdataset.filters.detshuffle.__init__","text":"Initialize the detshuffle stage. Parameters: bufsize ( int , default: 1000 ) \u2013 Size of the buffer for shuffling. initial ( int , default: 100 ) \u2013 Initial number of samples to collect before shuffling. seed ( int , default: 0 ) \u2013 Seed for the random number generator. epoch ( int , default: -1 ) \u2013 Starting epoch number. Source code in webdataset/filters.py 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def __init__ ( self , bufsize = 1000 , initial = 100 , seed = 0 , epoch =- 1 ): \"\"\" Initialize the detshuffle stage. Args: bufsize (int): Size of the buffer for shuffling. initial (int): Initial number of samples to collect before shuffling. seed (int): Seed for the random number generator. epoch (int): Starting epoch number. \"\"\" self . bufsize = bufsize self . initial = initial self . seed = seed self . epoch = epoch","title":"__init__"},{"location":"webdataset2/#webdataset.filters.detshuffle.run","text":"Run the shuffling process on the input source. Parameters: src \u2013 Input data source to be shuffled. Returns: Iterator \u2013 Shuffled data iterator. Source code in webdataset/filters.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def run ( self , src ): \"\"\" Run the shuffling process on the input source. Args: src: Input data source to be shuffled. Returns: Iterator: Shuffled data iterator. \"\"\" self . epoch += 1 rng = random . Random () rng . seed ( self . seed + self . epoch ) return _shuffle ( src , self . bufsize , self . initial , rng )","title":"run"},{"location":"webdataset2/#webdataset.filters.compose","text":"Compose a sequence of functions (left-to-right). Parameters: *args \u2013 Functions to be composed. Returns: function \u2013 A new function that applies all input functions in sequence. Source code in webdataset/filters.py 177 178 179 180 181 182 183 184 185 186 187 def compose ( * args ): \"\"\" Compose a sequence of functions (left-to-right). Args: *args: Functions to be composed. Returns: function: A new function that applies all input functions in sequence. \"\"\" return reduce ( compose2 , args )","title":"compose"},{"location":"webdataset2/#webdataset.filters.compose2","text":"Compose two functions, g(f(x)). Parameters: f \u2013 The first function to be composed. g \u2013 The second function to be composed. Returns: function \u2013 A new function that applies f and then g to its input. Source code in webdataset/filters.py 163 164 165 166 167 168 169 170 171 172 173 174 def compose2 ( f , g ): \"\"\" Compose two functions, g(f(x)). Args: f: The first function to be composed. g: The second function to be composed. Returns: function: A new function that applies f and then g to its input. \"\"\" return lambda x : g ( f ( x ))","title":"compose2"},{"location":"webdataset2/#webdataset.filters.decode_bin","text":"Decode binary data from a stream. Parameters: stream \u2013 A file-like object containing binary data. Returns: bytes \u2013 The binary data read from the stream. Source code in webdataset/filters.py 941 942 943 944 945 946 947 948 949 950 951 def decode_bin ( stream ): \"\"\" Decode binary data from a stream. Args: stream: A file-like object containing binary data. Returns: bytes: The binary data read from the stream. \"\"\" return stream . read ()","title":"decode_bin"},{"location":"webdataset2/#webdataset.filters.decode_pickle","text":"Decode pickle data from a stream. Parameters: stream \u2013 A file-like object containing pickle data. Returns: \u2013 The unpickled object. Source code in webdataset/filters.py 968 969 970 971 972 973 974 975 976 977 978 def decode_pickle ( stream ): \"\"\" Decode pickle data from a stream. Args: stream: A file-like object containing pickle data. Returns: The unpickled object. \"\"\" return pickle . load ( stream )","title":"decode_pickle"},{"location":"webdataset2/#webdataset.filters.decode_text","text":"Decode text data from a stream. Parameters: stream \u2013 A file-like object containing text data. Returns: str \u2013 The decoded text data. Source code in webdataset/filters.py 954 955 956 957 958 959 960 961 962 963 964 965 def decode_text ( stream ): \"\"\" Decode text data from a stream. Args: stream: A file-like object containing text data. Returns: str: The decoded text data. \"\"\" binary = stream . read () return binary . decode ( \"utf-8\" )","title":"decode_text"},{"location":"webdataset2/#webdataset.filters.default_collation_fn","text":"Take a collection of samples (dictionaries) and create a batch. Parameters: samples ( list ) \u2013 List of samples to be batched. combine_tensors ( bool , default: True ) \u2013 Whether to combine tensor-like objects into batches. combine_scalars ( bool , default: True ) \u2013 Whether to combine scalar values into numpy arrays. Returns: list \u2013 A batch of samples. Source code in webdataset/filters.py 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 def default_collation_fn ( samples , combine_tensors = True , combine_scalars = True ): \"\"\" Take a collection of samples (dictionaries) and create a batch. Args: samples (list): List of samples to be batched. combine_tensors (bool): Whether to combine tensor-like objects into batches. combine_scalars (bool): Whether to combine scalar values into numpy arrays. Returns: list: A batch of samples. \"\"\" assert isinstance ( samples [ 0 ], ( list , tuple )), type ( samples [ 0 ]) batched = list ( zip ( * samples )) result = [] for b in batched : if isinstance ( b [ 0 ], ( int , float )): if combine_scalars : b = np . array ( list ( b )) elif isinstance ( b [ 0 ], TorchTensor ): if combine_tensors : import torch b = torch . stack ( list ( b )) elif isinstance ( b [ 0 ], np . ndarray ): if combine_tensors : b = np . array ( list ( b )) else : b = list ( b ) result . append ( b ) return result","title":"default_collation_fn"},{"location":"webdataset2/#webdataset.filters.find_decoder","text":"Find the appropriate decoder for a given path. Parameters: decoders \u2013 List of (pattern, decoder_function) pairs. path \u2013 The path to find a decoder for. Returns: callable \u2013 The decoder function for the given path, or None if no match is found. Source code in webdataset/filters.py 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 def find_decoder ( decoders , path ): \"\"\" Find the appropriate decoder for a given path. Args: decoders: List of (pattern, decoder_function) pairs. path: The path to find a decoder for. Returns: callable: The decoder function for the given path, or None if no match is found. \"\"\" fname = re . sub ( r \".*/\" , \"\" , path ) if fname . startswith ( \"__\" ): return lambda x : x for pattern , fun in decoders [:: - 1 ]: if fnmatch ( fname . lower (), pattern ) or fnmatch ( \".\" + fname . lower (), pattern ): return fun return None","title":"find_decoder"},{"location":"webdataset2/#webdataset.filters.getfirst","text":"Get the first matching key from a dictionary. Keys can be specified as a list, or as a string of keys separated by ';'. Parameters: a ( dict ) \u2013 The dictionary to search. keys ( str or list ) \u2013 The keys to search for. default \u2013 The default value to return if no key is found. missing_is_error ( bool , default: True ) \u2013 If True, raise an error when no key is found. Returns: \u2013 The value of the first matching key found in the dictionary. Raises: ValueError \u2013 If no matching key is found and missing_is_error is True. Source code in webdataset/filters.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def getfirst ( a , keys , default = None , missing_is_error = True ): \"\"\" Get the first matching key from a dictionary. Keys can be specified as a list, or as a string of keys separated by ';'. Args: a (dict): The dictionary to search. keys (str or list): The keys to search for. default: The default value to return if no key is found. missing_is_error (bool): If True, raise an error when no key is found. Returns: The value of the first matching key found in the dictionary. Raises: ValueError: If no matching key is found and missing_is_error is True. \"\"\" if isinstance ( keys , str ): assert \" \" not in keys keys = keys . split ( \";\" ) for k in keys : if k in a : return a [ k ] if missing_is_error : raise ValueError ( f \"didn't find { keys } in { list ( a . keys ()) } \" ) return default","title":"getfirst"},{"location":"webdataset2/#webdataset.filters.identity","text":"Return the argument unchanged. Parameters: x \u2013 The input value. Returns: \u2013 The input value unchanged. Source code in webdataset/filters.py 150 151 152 153 154 155 156 157 158 159 160 def identity ( x ): \"\"\" Return the argument unchanged. Args: x: The input value. Returns: The input value unchanged. \"\"\" return x","title":"identity"},{"location":"webdataset2/#webdataset.filters.parse_field_spec","text":"Parse a specification for a list of fields to be extracted. Keys are separated by spaces in the spec. Each key can itself be composed of key alternatives separated by ';'. Parameters: fields ( str or list ) \u2013 The field specification to parse. Returns: list \u2013 A list of parsed field specifications. Source code in webdataset/filters.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def parse_field_spec ( fields ): \"\"\" Parse a specification for a list of fields to be extracted. Keys are separated by spaces in the spec. Each key can itself be composed of key alternatives separated by ';'. Args: fields (str or list): The field specification to parse. Returns: list: A list of parsed field specifications. \"\"\" if isinstance ( fields , str ): fields = fields . split () return [ field . split ( \";\" ) for field in fields ]","title":"parse_field_spec"},{"location":"webdataset2/#webdataset.filters.pick","text":"Pick a random item from the buffer and remove it. Parameters: buf ( list ) \u2013 The buffer to pick from. rng \u2013 Random number generator. Returns: \u2013 The randomly picked item. Source code in webdataset/filters.py 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def pick ( buf , rng ): \"\"\" Pick a random item from the buffer and remove it. Args: buf (list): The buffer to pick from. rng: Random number generator. Returns: The randomly picked item. \"\"\" k = rng . randint ( 0 , len ( buf ) - 1 ) sample = buf [ k ] buf [ k ] = buf [ - 1 ] buf . pop () return sample","title":"pick"},{"location":"webdataset2/#webdataset.filters.pipeline","text":"Write an input pipeline; first argument is source, rest are filters. Parameters: source \u2013 The data source for the pipeline. *args \u2013 Filters to be applied to the data. Returns: \u2013 The result of applying all filters to the source data. Source code in webdataset/filters.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def pipeline ( source , * args ): \"\"\" Write an input pipeline; first argument is source, rest are filters. Args: source: The data source for the pipeline. *args: Filters to be applied to the data. Returns: The result of applying all filters to the source data. \"\"\" if len ( args ) == 0 : return source return compose ( * args )( source )","title":"pipeline"},{"location":"webdataset2/#webdataset.filters.pipelinefilter","text":"Turn the decorated function into one that is partially applied for all arguments other than the first. Parameters: f \u2013 The function to be decorated. Returns: RestCurried \u2013 A RestCurried object that can be used to create a FilterFunction. Source code in webdataset/filters.py 122 123 124 125 126 127 128 129 130 131 132 133 134 def pipelinefilter ( f ): \"\"\" Turn the decorated function into one that is partially applied for all arguments other than the first. Args: f: The function to be decorated. Returns: RestCurried: A RestCurried object that can be used to create a FilterFunction. \"\"\" result = RestCurried ( f ) functools . update_wrapper ( result , f ) return result","title":"pipelinefilter"},{"location":"webdataset2/#webdataset.filters.reraise_exception","text":"Reraise the given exception. Parameters: exn \u2013 The exception to be reraised. Source code in webdataset/filters.py 137 138 139 140 141 142 143 144 145 146 147 def reraise_exception ( exn ): \"\"\" Reraise the given exception. Args: exn: The exception to be reraised. Raises: The input exception. \"\"\" raise exn","title":"reraise_exception"},{"location":"webdataset2/#webdataset.filters.transform_with","text":"Transform a list of values using a list of functions. If there are fewer transformers than inputs, or if a transformer function is None, then the identity function is used for the corresponding sample fields. Parameters: sample ( list ) \u2013 List of values to transform. transformers ( list ) \u2013 List of functions to apply to the sample. Returns: list \u2013 The transformed sample. Source code in webdataset/filters.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def transform_with ( sample , transformers ): \"\"\" Transform a list of values using a list of functions. If there are fewer transformers than inputs, or if a transformer function is None, then the identity function is used for the corresponding sample fields. Args: sample (list): List of values to transform. transformers (list): List of functions to apply to the sample. Returns: list: The transformed sample. \"\"\" if transformers is None or len ( transformers ) == 0 : return sample result = list ( sample ) assert len ( transformers ) <= len ( sample ) for i in range ( len ( transformers )): # skipcq: PYL-C0200 f = transformers [ i ] if f is not None : result [ i ] = f ( sample [ i ]) return result","title":"transform_with"},{"location":"webdataset2/#webdataset.handlers","text":"Pluggable exception handlers. These are functions that take an exception as an argument and then return... the exception (in order to re-raise it) True (in order to continue and ignore the exception) False (in order to ignore the exception and stop processing) They are used as handler= arguments in much of the library.","title":"handlers"},{"location":"webdataset2/#webdataset.handlers.ignore_and_continue","text":"Ignore the exception and continue processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 34 35 36 37 38 39 40 41 42 43 def ignore_and_continue ( exn ): \"\"\"Ignore the exception and continue processing. Args: exn: The exception to be ignored. Returns: bool: Always returns True to indicate continuation. \"\"\" return True","title":"ignore_and_continue"},{"location":"webdataset2/#webdataset.handlers.ignore_and_stop","text":"Ignore the exception and stop further processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 60 61 62 63 64 65 66 67 68 69 def ignore_and_stop ( exn ): \"\"\"Ignore the exception and stop further processing. Args: exn: The exception to be ignored. Returns: bool: Always returns False to indicate stopping. \"\"\" return False","title":"ignore_and_stop"},{"location":"webdataset2/#webdataset.handlers.reraise_exception","text":"Re-raise the given exception. Parameters: exn \u2013 The exception to be re-raised. Source code in webdataset/handlers.py 22 23 24 25 26 27 28 29 30 31 def reraise_exception ( exn ): \"\"\"Re-raise the given exception. Args: exn: The exception to be re-raised. Raises: The input exception. \"\"\" raise exn","title":"reraise_exception"},{"location":"webdataset2/#webdataset.handlers.warn_and_continue","text":"Issue a warning for the exception and continue processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 46 47 48 49 50 51 52 53 54 55 56 57 def warn_and_continue ( exn ): \"\"\"Issue a warning for the exception and continue processing. Args: exn: The exception to be warned about. Returns: bool: Always returns True to indicate continuation. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return True","title":"warn_and_continue"},{"location":"webdataset2/#webdataset.handlers.warn_and_stop","text":"Issue a warning for the exception and stop further processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 72 73 74 75 76 77 78 79 80 81 82 83 def warn_and_stop ( exn ): \"\"\"Issue a warning for the exception and stop further processing. Args: exn: The exception to be warned about. Returns: bool: Always returns False to indicate stopping. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return False","title":"warn_and_stop"},{"location":"webdataset2/#webdataset.pipeline","text":"","title":"pipeline"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline","text":"Bases: IterableDataset , PipelineStage A pipeline starting with an IterableDataset and a series of filters. Parameters: *args \u2013 Variable length argument list of pipeline stages. **kwargs \u2013 Arbitrary keyword arguments. Source code in webdataset/pipeline.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class DataPipeline ( IterableDataset , PipelineStage ): \"\"\"A pipeline starting with an IterableDataset and a series of filters. Args: *args: Variable length argument list of pipeline stages. **kwargs: Arbitrary keyword arguments. \"\"\" def __init__ ( self , * args , ** kwargs ): super () . __init__ () self . pipeline = [] self . length = - 1 self . repetitions = 1 self . nsamples = - 1 for arg in args : if arg is None : continue if isinstance ( arg , list ): self . pipeline . extend ( arg ) else : self . pipeline . append ( arg ) def close ( self ): \"\"\"Close the pipeline and release resources.\"\"\" for step in self . pipeline : if hasattr ( step , \"close\" ): step . close () del self . pipeline def invoke ( self , f , * args , ** kwargs ): \"\"\"Apply a pipeline stage, possibly to the output of a previous stage. Args: f: The pipeline stage to invoke. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. Returns: The result of invoking the pipeline stage. Raises: ValueError: If the pipeline stage is not valid. \"\"\" if isinstance ( f , PipelineStage ): return f . run ( * args , ** kwargs ) if isinstance ( f , ( IterableDataset , DataLoader )) and len ( args ) == 0 : return iter ( f ) if isinstance ( f , list ): return iter ( f ) if callable ( f ): result = f ( * args , ** kwargs ) return result raise ValueError ( f \" { f } : not a valid pipeline stage\" ) def iterator1 ( self ): \"\"\"Create an iterator through one epoch in the pipeline. Returns: An iterator for one epoch of the pipeline. \"\"\" source = self . invoke ( self . pipeline [ 0 ]) for step in self . pipeline [ 1 :]: source = self . invoke ( step , source ) return source def iterator ( self ): \"\"\"Create an iterator through the entire dataset, using the given number of repetitions. Yields: Samples from the dataset. \"\"\" for _ in range ( self . repetitions ): count = 0 for sample in self . iterator1 (): yield sample count += 1 if count == 0 : # if the dataset is empty, don't keep looping break def __iter__ ( self ): \"\"\"Create an iterator through the pipeline, repeating and slicing as requested. Returns: An iterator through the pipeline. \"\"\" if self . repetitions != 1 : if self . nsamples > 0 : return islice ( self . iterator (), self . nsamples ) else : return self . iterator () else : return self . iterator () def stage ( self , i ): \"\"\"Return pipeline stage i. Args: i: The index of the pipeline stage to return. Returns: The pipeline stage at index i. \"\"\" return self . pipeline [ i ] def append ( self , f ): \"\"\"Append a pipeline stage (modifies the object). Args: f: The pipeline stage to append. \"\"\" self . pipeline . append ( f ) def compose ( self , * args ): \"\"\"Append pipeline stages to a copy of the pipeline and return the copy. Args: *args: Variable length argument list of pipeline stages to append. Returns: A new DataPipeline object with the appended stages. \"\"\" result = copy . copy ( self ) result . pipeline = copy . copy ( result . pipeline ) for arg in args : result . append ( arg ) return result def with_length ( self , n , silent = False ): \"\"\"Add a __len__ method returning the desired value. This does not change the actual number of samples in an epoch. PyTorch IterableDataset should not have a __len__ method. This is provided only as a workaround for some broken training environments that require a __len__ method. Args: n: The length value to set. silent: If True, suppress the warning message. Returns: The modified DataPipeline object with a __len__ method. \"\"\" if not silent : warnings . warn ( \".with_length() only sets the value of __len__ for compatibility with some training environments. It does not change the number of samples in an epoch.\" ) self . size = n return add_length_method ( self ) def with_epoch ( self , nsamples =- 1 , nbatches =- 1 ): \"\"\"Change the epoch to return the given number of samples/batches. Args: nsamples: The number of samples per epoch. nbatches: The number of batches per epoch. Returns: The modified DataPipeline object. \"\"\" self . repetitions = sys . maxsize self . nsamples = max ( nsamples , nbatches ) return self def repeat ( self , nepochs =- 1 , nbatches =- 1 ): \"\"\"Repeat iterating through the dataset for the given number of epochs up to the given number of samples. Args: nepochs: The number of epochs to repeat. nbatches: The number of batches to limit per repetition. Returns: The modified DataPipeline object. \"\"\" if nepochs > 0 : self . repetitions = nepochs self . nsamples = nbatches else : self . repetitions = sys . maxsize self . nsamples = nbatches return self","title":"DataPipeline"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.__iter__","text":"Create an iterator through the pipeline, repeating and slicing as requested. Returns: \u2013 An iterator through the pipeline. Source code in webdataset/pipeline.py 111 112 113 114 115 116 117 118 119 120 121 122 123 def __iter__ ( self ): \"\"\"Create an iterator through the pipeline, repeating and slicing as requested. Returns: An iterator through the pipeline. \"\"\" if self . repetitions != 1 : if self . nsamples > 0 : return islice ( self . iterator (), self . nsamples ) else : return self . iterator () else : return self . iterator ()","title":"__iter__"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.append","text":"Append a pipeline stage (modifies the object). Parameters: f \u2013 The pipeline stage to append. Source code in webdataset/pipeline.py 136 137 138 139 140 141 142 def append ( self , f ): \"\"\"Append a pipeline stage (modifies the object). Args: f: The pipeline stage to append. \"\"\" self . pipeline . append ( f )","title":"append"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.close","text":"Close the pipeline and release resources. Source code in webdataset/pipeline.py 53 54 55 56 57 58 def close ( self ): \"\"\"Close the pipeline and release resources.\"\"\" for step in self . pipeline : if hasattr ( step , \"close\" ): step . close () del self . pipeline","title":"close"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.compose","text":"Append pipeline stages to a copy of the pipeline and return the copy. Parameters: *args \u2013 Variable length argument list of pipeline stages to append. Returns: \u2013 A new DataPipeline object with the appended stages. Source code in webdataset/pipeline.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def compose ( self , * args ): \"\"\"Append pipeline stages to a copy of the pipeline and return the copy. Args: *args: Variable length argument list of pipeline stages to append. Returns: A new DataPipeline object with the appended stages. \"\"\" result = copy . copy ( self ) result . pipeline = copy . copy ( result . pipeline ) for arg in args : result . append ( arg ) return result","title":"compose"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.invoke","text":"Apply a pipeline stage, possibly to the output of a previous stage. Parameters: f \u2013 The pipeline stage to invoke. *args \u2013 Variable length argument list. **kwargs \u2013 Arbitrary keyword arguments. Returns: \u2013 The result of invoking the pipeline stage. Raises: ValueError \u2013 If the pipeline stage is not valid. Source code in webdataset/pipeline.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def invoke ( self , f , * args , ** kwargs ): \"\"\"Apply a pipeline stage, possibly to the output of a previous stage. Args: f: The pipeline stage to invoke. *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. Returns: The result of invoking the pipeline stage. Raises: ValueError: If the pipeline stage is not valid. \"\"\" if isinstance ( f , PipelineStage ): return f . run ( * args , ** kwargs ) if isinstance ( f , ( IterableDataset , DataLoader )) and len ( args ) == 0 : return iter ( f ) if isinstance ( f , list ): return iter ( f ) if callable ( f ): result = f ( * args , ** kwargs ) return result raise ValueError ( f \" { f } : not a valid pipeline stage\" )","title":"invoke"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.iterator","text":"Create an iterator through the entire dataset, using the given number of repetitions. Yields: \u2013 Samples from the dataset. Source code in webdataset/pipeline.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def iterator ( self ): \"\"\"Create an iterator through the entire dataset, using the given number of repetitions. Yields: Samples from the dataset. \"\"\" for _ in range ( self . repetitions ): count = 0 for sample in self . iterator1 (): yield sample count += 1 if count == 0 : # if the dataset is empty, don't keep looping break","title":"iterator"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.iterator1","text":"Create an iterator through one epoch in the pipeline. Returns: \u2013 An iterator for one epoch of the pipeline. Source code in webdataset/pipeline.py 85 86 87 88 89 90 91 92 93 94 def iterator1 ( self ): \"\"\"Create an iterator through one epoch in the pipeline. Returns: An iterator for one epoch of the pipeline. \"\"\" source = self . invoke ( self . pipeline [ 0 ]) for step in self . pipeline [ 1 :]: source = self . invoke ( step , source ) return source","title":"iterator1"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.repeat","text":"Repeat iterating through the dataset for the given number of epochs up to the given number of samples. Parameters: nepochs \u2013 The number of epochs to repeat. nbatches \u2013 The number of batches to limit per repetition. Returns: \u2013 The modified DataPipeline object. Source code in webdataset/pipeline.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def repeat ( self , nepochs =- 1 , nbatches =- 1 ): \"\"\"Repeat iterating through the dataset for the given number of epochs up to the given number of samples. Args: nepochs: The number of epochs to repeat. nbatches: The number of batches to limit per repetition. Returns: The modified DataPipeline object. \"\"\" if nepochs > 0 : self . repetitions = nepochs self . nsamples = nbatches else : self . repetitions = sys . maxsize self . nsamples = nbatches return self","title":"repeat"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.stage","text":"Return pipeline stage i. Parameters: i \u2013 The index of the pipeline stage to return. Returns: \u2013 The pipeline stage at index i. Source code in webdataset/pipeline.py 125 126 127 128 129 130 131 132 133 134 def stage ( self , i ): \"\"\"Return pipeline stage i. Args: i: The index of the pipeline stage to return. Returns: The pipeline stage at index i. \"\"\" return self . pipeline [ i ]","title":"stage"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.with_epoch","text":"Change the epoch to return the given number of samples/batches. Parameters: nsamples \u2013 The number of samples per epoch. nbatches \u2013 The number of batches per epoch. Returns: \u2013 The modified DataPipeline object. Source code in webdataset/pipeline.py 179 180 181 182 183 184 185 186 187 188 189 190 191 def with_epoch ( self , nsamples =- 1 , nbatches =- 1 ): \"\"\"Change the epoch to return the given number of samples/batches. Args: nsamples: The number of samples per epoch. nbatches: The number of batches per epoch. Returns: The modified DataPipeline object. \"\"\" self . repetitions = sys . maxsize self . nsamples = max ( nsamples , nbatches ) return self","title":"with_epoch"},{"location":"webdataset2/#webdataset.pipeline.DataPipeline.with_length","text":"Add a len method returning the desired value. This does not change the actual number of samples in an epoch. PyTorch IterableDataset should not have a len method. This is provided only as a workaround for some broken training environments that require a len method. Parameters: n \u2013 The length value to set. silent \u2013 If True, suppress the warning message. Returns: \u2013 The modified DataPipeline object with a len method. Source code in webdataset/pipeline.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def with_length ( self , n , silent = False ): \"\"\"Add a __len__ method returning the desired value. This does not change the actual number of samples in an epoch. PyTorch IterableDataset should not have a __len__ method. This is provided only as a workaround for some broken training environments that require a __len__ method. Args: n: The length value to set. silent: If True, suppress the warning message. Returns: The modified DataPipeline object with a __len__ method. \"\"\" if not silent : warnings . warn ( \".with_length() only sets the value of __len__ for compatibility with some training environments. It does not change the number of samples in an epoch.\" ) self . size = n return add_length_method ( self )","title":"with_length"},{"location":"webdataset2/#webdataset.pipeline.add_length_method","text":"Add a length method to the given object. Parameters: obj \u2013 The object to which the length method will be added. Returns: \u2013 The modified object with a new length method. Source code in webdataset/pipeline.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def add_length_method ( obj ): \"\"\"Add a length method to the given object. Args: obj: The object to which the length method will be added. Returns: The modified object with a new length method. \"\"\" def length ( self ): return self . size Combined = type ( obj . __class__ . __name__ + \"_Length\" , ( obj . __class__ , IterableDataset ), { \"__len__\" : length }, ) obj . __class__ = Combined return obj","title":"add_length_method"},{"location":"webdataset2/#webdataset.tariterators","text":"Low level iteration functions for tar archives.","title":"tariterators"},{"location":"webdataset2/#webdataset.tariterators.base_plus_ext","text":"Split off all file extensions. Parameters: path \u2013 Path with extensions. Returns: \u2013 Tuple containing the base path and all extensions. Source code in webdataset/tariterators.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def base_plus_ext ( path ): \"\"\"Split off all file extensions. Args: path: Path with extensions. Returns: Tuple containing the base path and all extensions. \"\"\" match = re . match ( r \"^((?:.*/|)[^.]+)[.]([^/]*)$\" , path ) if not match : return None , None return match . group ( 1 ), match . group ( 2 )","title":"base_plus_ext"},{"location":"webdataset2/#webdataset.tariterators.group_by_keys","text":"Group tarfile contents by keys and yield samples. Parameters: data ( Iterable [ Dict [ str , Any ]] ) \u2013 Iterator over tarfile contents. keys ( Callable [[ str ], Tuple [ str , str ]] , default: base_plus_ext ) \u2013 Function that takes a file name and returns a key and a suffix. lcase ( bool , default: True ) \u2013 Whether to lowercase the suffix. suffixes ( Optional [ Set [ str ]] , default: None ) \u2013 List of suffixes to keep. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. Raises: ValueError \u2013 If there are duplicate file names in the tar file. Yields: Dict [ str , Any ] \u2013 Iterator over samples. Source code in webdataset/tariterators.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def group_by_keys ( data : Iterable [ Dict [ str , Any ]], keys : Callable [[ str ], Tuple [ str , str ]] = base_plus_ext , lcase : bool = True , suffixes : Optional [ Set [ str ]] = None , handler : Callable [[ Exception ], bool ] = reraise_exception , ) -> Iterator [ Dict [ str , Any ]]: \"\"\"Group tarfile contents by keys and yield samples. Args: data: Iterator over tarfile contents. keys: Function that takes a file name and returns a key and a suffix. lcase: Whether to lowercase the suffix. suffixes: List of suffixes to keep. handler: Exception handler. Raises: ValueError: If there are duplicate file names in the tar file. Yields: Iterator over samples. \"\"\" current_sample = None for filesample in data : try : assert isinstance ( filesample , dict ) if filesample == {}: if valid_sample ( current_sample ): yield current_sample current_sample = None continue fname , value = filesample [ \"fname\" ], filesample [ \"data\" ] prefix , suffix = keys ( fname ) if trace : print ( prefix , suffix , current_sample . keys () if isinstance ( current_sample , dict ) else None , ) if prefix is None : continue if lcase : suffix = suffix . lower () if current_sample is None or prefix != current_sample [ \"__key__\" ]: if valid_sample ( current_sample ): yield current_sample current_sample = dict ( __key__ = prefix , __url__ = filesample [ \"__url__\" ]) if suffix in current_sample : raise ValueError ( f \" { fname } : duplicate file name in tar file { suffix } { current_sample . keys () } \" ) if suffixes is None or suffix in suffixes : current_sample [ suffix ] = value local_path = filesample . get ( \"__local_path__\" ) if local_path is not None : current_sample [ \"__local_path__\" ] = local_path except Exception as exn : exn . args = exn . args + ( filesample . get ( \"stream\" ), filesample . get ( \"url\" )) if handler ( exn ): continue else : break if valid_sample ( current_sample ): yield current_sample","title":"group_by_keys"},{"location":"webdataset2/#webdataset.tariterators.shardlist","text":"Generate a list of URLs, possibly shuffled. Parameters: urls \u2013 A string or list of URLs. shuffle \u2013 Whether to shuffle the URLs. Yields: \u2013 Dictionary containing the URL. Source code in webdataset/tariterators.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def shardlist ( urls , * , shuffle = False ): \"\"\"Generate a list of URLs, possibly shuffled. Args: urls: A string or list of URLs. shuffle: Whether to shuffle the URLs. Yields: Dictionary containing the URL. \"\"\" if isinstance ( urls , str ): urls = braceexpand . braceexpand ( urls ) else : urls = list ( urls ) if shuffle : random . shuffle ( urls ) for url in urls : yield dict ( url = url )","title":"shardlist"},{"location":"webdataset2/#webdataset.tariterators.tar_file_expander","text":"Expand tar files. Parameters: data ( Iterable [ Dict [ str , Any ]] ) \u2013 Iterator over opened tar file streams. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. select_files ( Optional [ Callable [[ str ], bool ]] , default: None ) \u2013 Select files from tarfiles by name (permits skipping files). rename_files ( Optional [ Callable [[ str ], str ]] , default: None ) \u2013 Function to rename files. eof_value ( Optional [ Any ] , default: {} ) \u2013 Value to yield at the end of each shard. Yields: Dict [ str , Any ] \u2013 A stream of samples. Source code in webdataset/tariterators.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def tar_file_expander ( data : Iterable [ Dict [ str , Any ]], handler : Callable [[ Exception ], bool ] = reraise_exception , select_files : Optional [ Callable [[ str ], bool ]] = None , rename_files : Optional [ Callable [[ str ], str ]] = None , eof_value : Optional [ Any ] = {}, ) -> Iterator [ Dict [ str , Any ]]: \"\"\"Expand tar files. Args: data: Iterator over opened tar file streams. handler: Exception handler. select_files: Select files from tarfiles by name (permits skipping files). rename_files: Function to rename files. eof_value: Value to yield at the end of each shard. Yields: A stream of samples. \"\"\" for source in data : url = source [ \"url\" ] local_path = source . get ( \"local_path\" ) try : assert isinstance ( source , dict ) assert \"stream\" in source for sample in tar_file_iterator ( source [ \"stream\" ], handler = handler , select_files = select_files , rename_files = rename_files , ): assert ( isinstance ( sample , dict ) and \"data\" in sample and \"fname\" in sample ) sample [ \"__url__\" ] = url if local_path is not None : sample [ \"__local_path__\" ] = local_path yield sample # we yield an EOF marker at the end of each shard so that # samples from different shards don't get mixed up if eof_value is not None : yield eof_value except Exception as exn : exn . args = exn . args + ( source . get ( \"stream\" ), source . get ( \"url\" )) if handler ( exn ): continue else : break","title":"tar_file_expander"},{"location":"webdataset2/#webdataset.tariterators.tar_file_iterator","text":"Iterate over tar file, yielding filename, content pairs for the given tar stream. Parameters: fileobj ( TarFile ) \u2013 The tar file stream. skip_meta ( Optional [ str ] , default: '__[^/]*__($|/)' ) \u2013 Regexp for keys that are skipped entirely. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. select_files ( Optional [ Callable [[ str ], bool ]] , default: None ) \u2013 Predicate for selecting files. rename_files ( Optional [ Callable [[ str ], str ]] , default: None ) \u2013 Function to rename files. Yields: Dict [ str , Any ] \u2013 A stream of samples. Source code in webdataset/tariterators.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def tar_file_iterator ( fileobj : tarfile . TarFile , skip_meta : Optional [ str ] = r \"__[^/]*__($|/)\" , handler : Callable [[ Exception ], bool ] = reraise_exception , select_files : Optional [ Callable [[ str ], bool ]] = None , rename_files : Optional [ Callable [[ str ], str ]] = None , ) -> Iterator [ Dict [ str , Any ]]: \"\"\"Iterate over tar file, yielding filename, content pairs for the given tar stream. Args: fileobj: The tar file stream. skip_meta: Regexp for keys that are skipped entirely. handler: Exception handler. select_files: Predicate for selecting files. rename_files: Function to rename files. Yields: A stream of samples. \"\"\" stream = tarfile . open ( fileobj = fileobj , mode = \"r|*\" ) for tarinfo in stream : fname = tarinfo . name try : if not tarinfo . isreg (): continue if fname is None : continue if ( \"/\" not in fname and fname . startswith ( meta_prefix ) and fname . endswith ( meta_suffix ) ): # skipping metadata for now continue if skip_meta is not None and re . match ( skip_meta , fname ): continue if rename_files : fname = rename_files ( fname ) if select_files is not None and not select_files ( fname ): continue data = stream . extractfile ( tarinfo ) . read () result = dict ( fname = fname , data = data ) yield result stream . members = [] except Exception as exn : if hasattr ( exn , \"args\" ) and len ( exn . args ) > 0 : exn . args = ( str ( exn . args [ 0 ]) + \" @ \" + str ( fileobj ),) + exn . args [ 1 :] if handler ( exn ): continue else : break del stream","title":"tar_file_iterator"},{"location":"webdataset2/#webdataset.tariterators.tarfile_samples","text":"Generate samples from a stream of tar files. Parameters: src ( Iterable [ Dict [ str , Any ]] ) \u2013 Stream of tar files. handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. select_files ( Optional [ Callable [[ str ], bool ]] , default: None ) \u2013 Function that selects files to be included. rename_files ( Optional [ Callable [[ str ], str ]] , default: None ) \u2013 Function to rename files. Returns: Iterable [ Dict [ str , Any ]] \u2013 Stream of samples. Source code in webdataset/tariterators.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def tarfile_samples ( src : Iterable [ Dict [ str , Any ]], handler : Callable [[ Exception ], bool ] = reraise_exception , select_files : Optional [ Callable [[ str ], bool ]] = None , rename_files : Optional [ Callable [[ str ], str ]] = None , ) -> Iterable [ Dict [ str , Any ]]: \"\"\"Generate samples from a stream of tar files. Args: src: Stream of tar files. handler: Exception handler. select_files: Function that selects files to be included. rename_files: Function to rename files. Returns: Stream of samples. \"\"\" streams = url_opener ( src , handler = handler ) files = tar_file_expander ( streams , handler = handler , select_files = select_files , rename_files = rename_files ) samples = group_by_keys ( files , handler = handler ) return samples","title":"tarfile_samples"},{"location":"webdataset2/#webdataset.tariterators.url_opener","text":"Open URLs and yield a stream of url+stream pairs. Parameters: data ( Iterable [ Dict [ str , Any ]] ) \u2013 Iterator over dict(url=...). handler ( Callable [[ Exception ], bool ] , default: reraise_exception ) \u2013 Exception handler. **kw ( Dict [ str , Any ] , default: {} ) \u2013 Keyword arguments for gopen.gopen. Yields: \u2013 A stream of url+stream pairs. Source code in webdataset/tariterators.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def url_opener ( data : Iterable [ Dict [ str , Any ]], handler : Callable [[ Exception ], bool ] = reraise_exception , ** kw : Dict [ str , Any ], ): \"\"\"Open URLs and yield a stream of url+stream pairs. Args: data: Iterator over dict(url=...). handler: Exception handler. **kw: Keyword arguments for gopen.gopen. Yields: A stream of url+stream pairs. \"\"\" for sample in data : assert isinstance ( sample , dict ), sample assert \"url\" in sample url = sample [ \"url\" ] try : stream = gopen . gopen ( url , ** kw ) sample . update ( stream = stream ) yield sample except Exception as exn : exn . args = exn . args + ( url ,) if handler ( exn ): continue else : break","title":"url_opener"},{"location":"webdataset2/#webdataset.tariterators.valid_sample","text":"Check whether a sample is valid. Parameters: sample ( Dict [ str , Any ] ) \u2013 A dictionary representing a sample. Returns: bool \u2013 Boolean indicating whether the sample is valid. Source code in webdataset/tariterators.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def valid_sample ( sample : Dict [ str , Any ]) -> bool : \"\"\"Check whether a sample is valid. Args: sample: A dictionary representing a sample. Returns: Boolean indicating whether the sample is valid. \"\"\" return ( sample is not None and isinstance ( sample , dict ) and len ( list ( sample . keys ())) > 0 and not sample . get ( \"__bad__\" , False ) )","title":"valid_sample"},{"location":"webdataset2/#webdataset.writer","text":"Classes and functions for writing tar files and WebDataset files.","title":"writer"},{"location":"webdataset2/#webdataset.writer.ShardWriter","text":"Like TarWriter but splits into multiple shards. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. Defaults to 100000. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. Defaults to 3e9. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. Defaults to None. start_shard ( int , default: 0 ) \u2013 Starting shard number. Defaults to 0. verbose ( int , default: 1 ) \u2013 Verbosity level. Defaults to 1. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. Defaults to None. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 class ShardWriter : \"\"\"Like TarWriter but splits into multiple shards. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. Defaults to 100000. maxsize: Maximum size of each shard. Defaults to 3e9. post: Optional callable to be executed after each shard is written. Defaults to None. start_shard: Starting shard number. Defaults to 0. verbose: Verbosity level. Defaults to 1. opener: Optional callable to open output files. Defaults to None. **kw: Other options passed to TarWriter. \"\"\" def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream () def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close ()","title":"ShardWriter"},{"location":"webdataset2/#webdataset.writer.ShardWriter.__enter__","text":"Enter context. Returns: self \u2013 The ShardWriter object. Source code in webdataset/writer.py 593 594 595 596 597 598 599 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self","title":"__enter__"},{"location":"webdataset2/#webdataset.writer.ShardWriter.__exit__","text":"Exit context. Source code in webdataset/writer.py 601 602 603 def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close ()","title":"__exit__"},{"location":"webdataset2/#webdataset.writer.ShardWriter.__init__","text":"Create a ShardWriter. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. start_shard ( int , default: 0 ) \u2013 Starting shard number. verbose ( int , default: 1 ) \u2013 Verbosity level. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream ()","title":"__init__"},{"location":"webdataset2/#webdataset.writer.ShardWriter.close","text":"Close the stream. Source code in webdataset/writer.py 585 586 587 588 589 590 591 def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size","title":"close"},{"location":"webdataset2/#webdataset.writer.ShardWriter.finish","text":"Finish all writing (use close instead). Source code in webdataset/writer.py 576 577 578 579 580 581 582 583 def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None","title":"finish"},{"location":"webdataset2/#webdataset.writer.ShardWriter.next_stream","text":"Close the current stream and move to the next. Source code in webdataset/writer.py 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0","title":"next_stream"},{"location":"webdataset2/#webdataset.writer.ShardWriter.write","text":"Write a sample. Parameters: obj \u2013 Sample to be written. Source code in webdataset/writer.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size","title":"write"},{"location":"webdataset2/#webdataset.writer.TarWriter","text":"A class for writing dictionaries to tar files. Parameters: fileobj \u2013 File name for tar file (.tgz/.tar) or open file descriptor. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Sample encoding. Defaults to True. compress ( Optional [ bool ] , default: None ) \u2013 Compression flag. Defaults to None. user ( str , default: 'bigdata' ) \u2013 User for tar files. Defaults to \"bigdata\". group ( str , default: 'bigdata' ) \u2013 Group for tar files. Defaults to \"bigdata\". mode ( int , default: 292 ) \u2013 Mode for tar files. Defaults to 0o0444. keep_meta ( bool , default: False ) \u2013 Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime ( Optional [ float ] , default: None ) \u2013 Modification time. Defaults to None. format ( Any , default: None ) \u2013 Tar format. Defaults to None. Returns: \u2013 TarWriter object. Raises: ValueError \u2013 If the encoder doesn't yield bytes for a key. True will use an encoder that behaves similar to the automatic decoder for Dataset . False disables encoding and expects byte strings (except for metadata, which must be strings). The encoder argument can also be a callable , or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: a/b.png and a/b.output.png . tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) Source code in webdataset/writer.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 class TarWriter : \"\"\"A class for writing dictionaries to tar files. Args: fileobj: File name for tar file (.tgz/.tar) or open file descriptor. encoder: Sample encoding. Defaults to True. compress: Compression flag. Defaults to None. user: User for tar files. Defaults to \"bigdata\". group: Group for tar files. Defaults to \"bigdata\". mode: Mode for tar files. Defaults to 0o0444. keep_meta: Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime: Modification time. Defaults to None. format: Tar format. Defaults to None. Returns: TarWriter object. Raises: ValueError: If the encoder doesn't yield bytes for a key. `True` will use an encoder that behaves similar to the automatic decoder for `Dataset`. `False` disables encoding and expects byte strings (except for metadata, which must be strings). The `encoder` argument can also be a `callable`, or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: `a/b.png` and `a/b.output.png`. tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) \"\"\" def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ bool ] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime if isinstance ( fileobj , str ): if compress is False : tarmode = \"w|\" elif compress is True : tarmode = \"w|gz\" else : tarmode = \"w|gz\" if fileobj . endswith ( \"gz\" ) else \"w|\" fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : tarmode = \"w|gz\" if compress is True else \"w|\" self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close () def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total","title":"TarWriter"},{"location":"webdataset2/#webdataset.writer.TarWriter.__enter__","text":"Enter context. Returns: self \u2013 The TarWriter object. Source code in webdataset/writer.py 421 422 423 424 425 426 427 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self","title":"__enter__"},{"location":"webdataset2/#webdataset.writer.TarWriter.__exit__","text":"Exit context. Source code in webdataset/writer.py 429 430 431 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close ()","title":"__exit__"},{"location":"webdataset2/#webdataset.writer.TarWriter.__init__","text":"Create a tar writer. Parameters: fileobj \u2013 Stream to write data to. user ( str , default: 'bigdata' ) \u2013 User for tar files. group ( str , default: 'bigdata' ) \u2013 Group for tar files. mode ( int , default: 292 ) \u2013 Mode for tar files. compress ( Optional [ bool ] , default: None ) \u2013 Desired compression. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Encoder function. keep_meta ( bool , default: False ) \u2013 Keep metadata (entries starting with \"_\"). mtime ( Optional [ float ] , default: None ) \u2013 Modification time (set this to some fixed value to get reproducible tar files). format ( Any , default: None ) \u2013 Tar format. Source code in webdataset/writer.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ bool ] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime if isinstance ( fileobj , str ): if compress is False : tarmode = \"w|\" elif compress is True : tarmode = \"w|gz\" else : tarmode = \"w|gz\" if fileobj . endswith ( \"gz\" ) else \"w|\" fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : tarmode = \"w|gz\" if compress is True else \"w|\" self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress","title":"__init__"},{"location":"webdataset2/#webdataset.writer.TarWriter.close","text":"Close the tar file. Source code in webdataset/writer.py 433 434 435 436 437 438 def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None","title":"close"},{"location":"webdataset2/#webdataset.writer.TarWriter.write","text":"Write a dictionary to the tar file. Parameters: obj \u2013 Dictionary of objects to be stored. Returns: int \u2013 Size of the entry. Raises: ValueError \u2013 If the object doesn't contain a key or if a key doesn't map to bytes after encoding. Source code in webdataset/writer.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total","title":"write"},{"location":"webdataset2/#webdataset.writer.add_handlers","text":"Add handlers to a dictionary for given keys. Parameters: d \u2013 Dictionary to add handlers to keys \u2013 String of space-separated keys or list of keys value \u2013 Handler function to be added Source code in webdataset/writer.py 193 194 195 196 197 198 199 200 201 202 203 204 def add_handlers ( d , keys , value ): \"\"\"Add handlers to a dictionary for given keys. Args: d: Dictionary to add handlers to keys: String of space-separated keys or list of keys value: Handler function to be added \"\"\" if isinstance ( keys , str ): keys = keys . split () for k in keys : d [ k ] = value","title":"add_handlers"},{"location":"webdataset2/#webdataset.writer.bytestr","text":"Convert data into a bytestring. Uses str and ASCII encoding for data that isn't already in string format. Parameters: data ( Any ) \u2013 Data to be converted Returns: bytes \u2013 Converted bytestring Source code in webdataset/writer.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def bytestr ( data : Any ): \"\"\"Convert data into a bytestring. Uses str and ASCII encoding for data that isn't already in string format. Args: data: Data to be converted Returns: bytes: Converted bytestring \"\"\" if isinstance ( data , bytes ): return data if isinstance ( data , str ): return data . encode ( \"ascii\" ) return str ( data ) . encode ( \"ascii\" )","title":"bytestr"},{"location":"webdataset2/#webdataset.writer.cbor_dumps","text":"Dump data into a bytestring using CBOR format. Parameters: x \u2013 Data to be dumped Returns: bytes \u2013 Dumped data as bytestring Source code in webdataset/writer.py 165 166 167 168 169 170 171 172 173 174 175 176 def cbor_dumps ( x ): \"\"\"Dump data into a bytestring using CBOR format. Args: x: Data to be dumped Returns: bytes: Dumped data as bytestring \"\"\" import cbor return cbor . dumps ( x )","title":"cbor_dumps"},{"location":"webdataset2/#webdataset.writer.encode_based_on_extension","text":"Encode an entire sample with a collection of handlers. Parameters: sample ( dict ) \u2013 Data sample (a dict) handlers ( dict ) \u2013 Handlers for encoding Returns: dict \u2013 Encoded sample Source code in webdataset/writer.py 276 277 278 279 280 281 282 283 284 285 286 287 288 def encode_based_on_extension ( sample : dict , handlers : dict ): \"\"\"Encode an entire sample with a collection of handlers. Args: sample: Data sample (a dict) handlers: Handlers for encoding Returns: dict: Encoded sample \"\"\" return { k : encode_based_on_extension1 ( v , k , handlers ) for k , v in list ( sample . items ()) }","title":"encode_based_on_extension"},{"location":"webdataset2/#webdataset.writer.encode_based_on_extension1","text":"Encode data based on its extension and a dict of handlers. Parameters: data ( Any ) \u2013 Data to be encoded tname ( str ) \u2013 File extension handlers ( dict ) \u2013 Dictionary of handlers for different data types Raises: ValueError \u2013 If no handler is found for the given extension or if metadata values are not strings Source code in webdataset/writer.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def encode_based_on_extension1 ( data : Any , tname : str , handlers : dict ): \"\"\"Encode data based on its extension and a dict of handlers. Args: data: Data to be encoded tname: File extension handlers: Dictionary of handlers for different data types Raises: ValueError: If no handler is found for the given extension or if metadata values are not strings \"\"\" if tname [ 0 ] == \"_\" : if not isinstance ( data , str ): raise ValueError ( \"the values of metadata must be of string type\" ) return data compress = False if tname . endswith ( \".gz\" ): compress = True tname = tname [: - 3 ] extension = re . sub ( r \".*\\.\" , \"\" , tname ) . lower () if isinstance ( data , bytes ): if compress : data = gzip . compress ( data ) return data if isinstance ( data , str ): data = data . encode ( \"utf-8\" ) if compress : data = gzip . compress ( data ) return data handler = handlers . get ( extension ) if handler is None : raise ValueError ( f \"no handler found for { extension } \" ) result = handler ( data ) if compress : result = gzip . compress ( result ) return result","title":"encode_based_on_extension1"},{"location":"webdataset2/#webdataset.writer.imageencoder","text":"Compress an image using PIL and return it as a string. Can handle float or uint8 images. Parameters: image ( Any ) \u2013 ndarray representing an image format ( str , default: 'PNG' ) \u2013 compression format (PNG, JPEG, PPM) Returns: bytes \u2013 Compressed image data Raises: ValueError \u2013 If image values are out of range Source code in webdataset/writer.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def imageencoder ( image : Any , format : str = \"PNG\" ): # skipcq: PYL-W0622 \"\"\"Compress an image using PIL and return it as a string. Can handle float or uint8 images. Args: image: ndarray representing an image format: compression format (PNG, JPEG, PPM) Returns: bytes: Compressed image data Raises: ValueError: If image values are out of range \"\"\" import PIL import PIL.Image assert isinstance ( image , ( PIL . Image . Image , np . ndarray )), type ( image ) if isinstance ( image , np . ndarray ): if image . dtype in [ np . dtype ( \"f\" ), np . dtype ( \"d\" )]: if not ( np . amin ( image ) > - 0.001 and np . amax ( image ) < 1.001 ): raise ValueError ( f \"image values out of range { np . amin ( image ) } { np . amax ( image ) } \" ) image = np . clip ( image , 0.0 , 1.0 ) image = np . array ( image * 255.0 , \"uint8\" ) assert image . ndim in [ 2 , 3 ] if image . ndim == 3 : assert image . shape [ 2 ] in [ 1 , 3 ] image = PIL . Image . fromarray ( image ) if format . upper () == \"JPG\" : format = \"JPEG\" elif format . upper () in { \"IMG\" , \"IMAGE\" }: format = \"PPM\" if format in { \"JPEG\" , \"tiff\" }: opts = dict ( quality = 100 ) else : opts = {} with io . BytesIO () as result : image . save ( result , format = format , ** opts ) return result . getvalue ()","title":"imageencoder"},{"location":"webdataset2/#webdataset.writer.make_encoder","text":"Make an encoder function from a specification. Parameters: spec ( Union [ bool , str , dict , Callable ] ) \u2013 Specification for the encoder Returns: Callable \u2013 Encoder function Raises: ValueError \u2013 If the specification is invalid or doesn't yield a callable encoder Source code in webdataset/writer.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def make_encoder ( spec : Union [ bool , str , dict , Callable ]): \"\"\"Make an encoder function from a specification. Args: spec: Specification for the encoder Returns: Callable: Encoder function Raises: ValueError: If the specification is invalid or doesn't yield a callable encoder \"\"\" if spec is False or spec is None : def encoder ( x ): \"\"\"Do not encode at all.\"\"\" return x elif callable ( spec ): encoder = spec elif isinstance ( spec , dict ): def f ( sample ): \"\"\"Encode based on extension.\"\"\" return encode_based_on_extension ( sample , spec ) encoder = f elif spec is True : handlers = default_handlers def g ( sample ): \"\"\"Encode based on extension.\"\"\" return encode_based_on_extension ( sample , handlers ) encoder = g else : raise ValueError ( f \" { spec } : unknown decoder spec\" ) if not callable ( encoder ): raise ValueError ( f \" { spec } did not yield a callable encoder\" ) return encoder","title":"make_encoder"},{"location":"webdataset2/#webdataset.writer.make_handlers","text":"Create a list of handlers for encoding data. Returns: dict \u2013 Dictionary of handlers for different data types Source code in webdataset/writer.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def make_handlers (): \"\"\"Create a list of handlers for encoding data. Returns: dict: Dictionary of handlers for different data types \"\"\" handlers = {} add_handlers ( handlers , \"cls cls2 class count index inx id\" , lambda x : str ( x ) . encode ( \"ascii\" ) ) add_handlers ( handlers , \"txt text transcript\" , lambda x : x . encode ( \"utf-8\" )) add_handlers ( handlers , \"html htm\" , lambda x : x . encode ( \"utf-8\" )) add_handlers ( handlers , \"pyd pickle\" , pickle . dumps ) add_handlers ( handlers , \"pth\" , torch_dumps ) add_handlers ( handlers , \"npy\" , numpy_dumps ) add_handlers ( handlers , \"npz\" , numpy_npz_dumps ) add_handlers ( handlers , \"ten tenbin tb\" , tenbin_dumps ) add_handlers ( handlers , \"json jsn\" , lambda x : json . dumps ( x ) . encode ( \"utf-8\" )) add_handlers ( handlers , \"mp msgpack msg\" , mp_dumps ) add_handlers ( handlers , \"cbor\" , cbor_dumps ) add_handlers ( handlers , \"jpg jpeg img image\" , lambda data : imageencoder ( data , \"jpg\" )) add_handlers ( handlers , \"png\" , lambda data : imageencoder ( data , \"png\" )) add_handlers ( handlers , \"pbm\" , lambda data : imageencoder ( data , \"pbm\" )) add_handlers ( handlers , \"pgm\" , lambda data : imageencoder ( data , \"pgm\" )) add_handlers ( handlers , \"ppm\" , lambda data : imageencoder ( data , \"ppm\" )) add_handlers ( handlers , \"tiff tif\" , lambda data : imageencoder ( data , \"tiff\" )) return handlers","title":"make_handlers"},{"location":"webdataset2/#webdataset.writer.mp_dumps","text":"Dump data into a bytestring using MessagePack format. Parameters: x \u2013 Data to be dumped Returns: bytes \u2013 Dumped data as bytestring Source code in webdataset/writer.py 179 180 181 182 183 184 185 186 187 188 189 190 def mp_dumps ( x ): \"\"\"Dump data into a bytestring using MessagePack format. Args: x: Data to be dumped Returns: bytes: Dumped data as bytestring \"\"\" import msgpack return msgpack . packb ( x )","title":"mp_dumps"},{"location":"webdataset2/#webdataset.writer.numpy_dumps","text":"Dump data into a bytestring using numpy npy format. Parameters: data ( ndarray ) \u2013 Data to be dumped Returns: bytes \u2013 Dumped data as bytestring Source code in webdataset/writer.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def numpy_dumps ( data : np . ndarray ): \"\"\"Dump data into a bytestring using numpy npy format. Args: data: Data to be dumped Returns: bytes: Dumped data as bytestring \"\"\" import io import numpy.lib.format stream = io . BytesIO () numpy . lib . format . write_array ( stream , data ) return stream . getvalue ()","title":"numpy_dumps"},{"location":"webdataset2/#webdataset.writer.numpy_npz_dumps","text":"Dump data into a bytestring using numpy npz format. Parameters: data ( Dict [ str , ndarray ] ) \u2013 Dictionary of numpy arrays to be dumped Returns: bytes \u2013 Dumped data as bytestring Raises: AssertionError \u2013 If input is not a dictionary of numpy arrays Source code in webdataset/writer.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def numpy_npz_dumps ( data : Dict [ str , np . ndarray ]): \"\"\"Dump data into a bytestring using numpy npz format. Args: data: Dictionary of numpy arrays to be dumped Returns: bytes: Dumped data as bytestring Raises: AssertionError: If input is not a dictionary of numpy arrays \"\"\" import io assert isinstance ( data , dict ) for k , v in list ( data . items ()): assert isinstance ( k , str ) assert isinstance ( v , np . ndarray ) stream = io . BytesIO () np . savez_compressed ( stream , ** data ) return stream . getvalue ()","title":"numpy_npz_dumps"},{"location":"webdataset2/#webdataset.writer.tenbin_dumps","text":"Dump data into a bytestring using tenbin format. Parameters: x \u2013 Data to be dumped (list or single item) Returns: memoryview \u2013 Dumped data as memoryview Source code in webdataset/writer.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def tenbin_dumps ( x ): \"\"\"Dump data into a bytestring using tenbin format. Args: x: Data to be dumped (list or single item) Returns: memoryview: Dumped data as memoryview \"\"\" from . import tenbin if isinstance ( x , list ): return memoryview ( tenbin . encode_buffer ( x )) else : return memoryview ( tenbin . encode_buffer ([ x ]))","title":"tenbin_dumps"},{"location":"webdataset2/#webdataset.writer.torch_dumps","text":"Dump data into a bytestring using torch.dumps. This delays importing torch until needed. Parameters: data ( Any ) \u2013 Data to be dumped Returns: bytes \u2013 Dumped data as bytestring Source code in webdataset/writer.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def torch_dumps ( data : Any ): \"\"\"Dump data into a bytestring using torch.dumps. This delays importing torch until needed. Args: data: Data to be dumped Returns: bytes: Dumped data as bytestring \"\"\" import io import torch stream = io . BytesIO () torch . save ( data , stream ) return stream . getvalue ()","title":"torch_dumps"},{"location":"wids/","text":"WIDS API wids.ShardListDataset Bases: Dataset [ T ] An indexable dataset based on a list of shards. The dataset is either given as a list of shards with optional options and name, or as a URL pointing to a JSON descriptor file. Datasets can reference other datasets via source_url . Shard references within a dataset are resolve relative to an explicitly given base property, or relative to the URL from which the dataset descriptor was loaded. Source code in wids/wids.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 class ShardListDataset ( Dataset [ T ]): \"\"\"An indexable dataset based on a list of shards. The dataset is either given as a list of shards with optional options and name, or as a URL pointing to a JSON descriptor file. Datasets can reference other datasets via `source_url`. Shard references within a dataset are resolve relative to an explicitly given `base` property, or relative to the URL from which the dataset descriptor was loaded. \"\"\" def __init__ ( self , shards , * , cache_size = int ( 1e12 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = \"PIL\" , keep = False , base = None , options = None , ): \"\"\"Create a ShardListDataset. Args: shards: a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size: the number of shards to keep in the cache lru_size: the number of shards to keep in the LRU cache localname: a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. \"\"\" if options is None : options = {} super ( ShardListDataset , self ) . __init__ () # shards is a list of (filename, length) pairs. We'll need to # keep track of the lengths and cumulative lengths to know how # to map indices to shards and indices within shards. if isinstance ( shards , ( str , io . IOBase )): if base is None and isinstance ( shards , str ): base = urldir ( shards ) self . base = base self . spec = load_dsdesc_and_resolve ( shards , options = options , base = base ) self . shards = self . spec . get ( \"shardlist\" , []) self . dataset_name = self . spec . get ( \"name\" ) or hash_dataset_name ( str ( shards )) else : self . base = None self . spec = options self . shards = shards self . dataset_name = dataset_name or hash_dataset_name ( str ( shards )) self . lengths = [ shard [ \"nsamples\" ] for shard in self . shards ] self . cum_lengths = np . cumsum ( self . lengths ) self . total_length = self . cum_lengths [ - 1 ] if cache_dir is not None : # when a cache dir is explicitly given, we download files into # that directory without any changes self . cache_dir = cache_dir self . localname = cache_localname ( cache_dir ) elif localname is not None : # when a localname function is given, we use that self . cache_dir = None self . localname = localname else : # when no cache dir or localname are given, use the cache from the environment self . cache_dir = os . environ . get ( \"WIDS_CACHE\" , \"/tmp/_wids_cache\" ) self . localname = default_localname ( self . cache_dir ) if True or int ( os . environ . get ( \"WIDS_VERBOSE\" , 0 )): nbytes = sum ( shard . get ( \"filesize\" , 0 ) for shard in self . shards ) nsamples = sum ( shard [ \"nsamples\" ] for shard in self . shards ) print ( str ( shards )[: 50 ], \"base:\" , self . base , \"name:\" , self . spec . get ( \"name\" ), \"nfiles:\" , len ( self . shards ), \"nbytes:\" , nbytes , \"samples:\" , nsamples , \"cache:\" , self . cache_dir , file = sys . stderr , ) self . transformations = interpret_transformations ( transformations ) if lru_size > 200 : warnings . warn ( \"LRU size is very large; consider reducing it to avoid running out of file descriptors\" ) self . cache = LRUShards ( lru_size , localname = self . localname , keep = keep ) def add_transform ( self , transform ): \"\"\"Add a transformation to the dataset.\"\"\" self . transformations . append ( transform ) return self def __len__ ( self ): \"\"\"Return the total number of samples in the dataset.\"\"\" return self . total_length def get_stats ( self ): \"\"\"Return the number of cache accesses and misses.\"\"\" return self . cache . accesses , self . cache . misses def check_cache_misses ( self ): \"\"\"Check if the cache miss rate is too high.\"\"\" accesses , misses = self . get_stats () if accesses > 100 and misses / accesses > 0.3 : # output a warning only once self . check_cache_misses = lambda : None print ( \"Warning: ShardListDataset has a cache miss rate of {:.1%} %\" . format ( misses * 100.0 / accesses ) ) def get_shard ( self , index ): \"\"\"Get the shard and index within the shard corresponding to the given index.\"\"\" # Find the shard corresponding to the given index. shard_idx = np . searchsorted ( self . cum_lengths , index , side = \"right\" ) # Figure out which index within the shard corresponds to the # given index. if shard_idx == 0 : inner_idx = index else : inner_idx = index - self . cum_lengths [ shard_idx - 1 ] # Get the shard and return the corresponding element. desc = self . shards [ shard_idx ] url = desc [ \"url\" ] shard = self . cache . get_shard ( url ) return shard , inner_idx , desc def __getitem__ ( self , index ): \"\"\"Return the sample corresponding to the given index.\"\"\" shard , inner_idx , desc = self . get_shard ( index ) sample = shard [ inner_idx ] # Check if we're missing the cache too often. self . check_cache_misses () sample [ \"__dataset__\" ] = desc . get ( \"dataset\" ) sample [ \"__index__\" ] = index sample [ \"__shard__\" ] = desc [ \"url\" ] sample [ \"__shardindex__\" ] = inner_idx # Apply transformations for transform in self . transformations : sample = transform ( sample ) return sample def close ( self ): \"\"\"Close the dataset.\"\"\" self . cache . clear () __getitem__ ( index ) Return the sample corresponding to the given index. Source code in wids/wids.py 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 def __getitem__ ( self , index ): \"\"\"Return the sample corresponding to the given index.\"\"\" shard , inner_idx , desc = self . get_shard ( index ) sample = shard [ inner_idx ] # Check if we're missing the cache too often. self . check_cache_misses () sample [ \"__dataset__\" ] = desc . get ( \"dataset\" ) sample [ \"__index__\" ] = index sample [ \"__shard__\" ] = desc [ \"url\" ] sample [ \"__shardindex__\" ] = inner_idx # Apply transformations for transform in self . transformations : sample = transform ( sample ) return sample __init__ ( shards , * , cache_size = int ( 1000000000000.0 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = 'PIL' , keep = False , base = None , options = None ) Create a ShardListDataset. Parameters: shards \u2013 a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size \u2013 the number of shards to keep in the cache lru_size \u2013 the number of shards to keep in the LRU cache localname \u2013 a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. Source code in wids/wids.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 def __init__ ( self , shards , * , cache_size = int ( 1e12 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = \"PIL\" , keep = False , base = None , options = None , ): \"\"\"Create a ShardListDataset. Args: shards: a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size: the number of shards to keep in the cache lru_size: the number of shards to keep in the LRU cache localname: a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. \"\"\" if options is None : options = {} super ( ShardListDataset , self ) . __init__ () # shards is a list of (filename, length) pairs. We'll need to # keep track of the lengths and cumulative lengths to know how # to map indices to shards and indices within shards. if isinstance ( shards , ( str , io . IOBase )): if base is None and isinstance ( shards , str ): base = urldir ( shards ) self . base = base self . spec = load_dsdesc_and_resolve ( shards , options = options , base = base ) self . shards = self . spec . get ( \"shardlist\" , []) self . dataset_name = self . spec . get ( \"name\" ) or hash_dataset_name ( str ( shards )) else : self . base = None self . spec = options self . shards = shards self . dataset_name = dataset_name or hash_dataset_name ( str ( shards )) self . lengths = [ shard [ \"nsamples\" ] for shard in self . shards ] self . cum_lengths = np . cumsum ( self . lengths ) self . total_length = self . cum_lengths [ - 1 ] if cache_dir is not None : # when a cache dir is explicitly given, we download files into # that directory without any changes self . cache_dir = cache_dir self . localname = cache_localname ( cache_dir ) elif localname is not None : # when a localname function is given, we use that self . cache_dir = None self . localname = localname else : # when no cache dir or localname are given, use the cache from the environment self . cache_dir = os . environ . get ( \"WIDS_CACHE\" , \"/tmp/_wids_cache\" ) self . localname = default_localname ( self . cache_dir ) if True or int ( os . environ . get ( \"WIDS_VERBOSE\" , 0 )): nbytes = sum ( shard . get ( \"filesize\" , 0 ) for shard in self . shards ) nsamples = sum ( shard [ \"nsamples\" ] for shard in self . shards ) print ( str ( shards )[: 50 ], \"base:\" , self . base , \"name:\" , self . spec . get ( \"name\" ), \"nfiles:\" , len ( self . shards ), \"nbytes:\" , nbytes , \"samples:\" , nsamples , \"cache:\" , self . cache_dir , file = sys . stderr , ) self . transformations = interpret_transformations ( transformations ) if lru_size > 200 : warnings . warn ( \"LRU size is very large; consider reducing it to avoid running out of file descriptors\" ) self . cache = LRUShards ( lru_size , localname = self . localname , keep = keep ) __len__ () Return the total number of samples in the dataset. Source code in wids/wids.py 534 535 536 def __len__ ( self ): \"\"\"Return the total number of samples in the dataset.\"\"\" return self . total_length add_transform ( transform ) Add a transformation to the dataset. Source code in wids/wids.py 529 530 531 532 def add_transform ( self , transform ): \"\"\"Add a transformation to the dataset.\"\"\" self . transformations . append ( transform ) return self check_cache_misses () Check if the cache miss rate is too high. Source code in wids/wids.py 542 543 544 545 546 547 548 549 550 551 552 def check_cache_misses ( self ): \"\"\"Check if the cache miss rate is too high.\"\"\" accesses , misses = self . get_stats () if accesses > 100 and misses / accesses > 0.3 : # output a warning only once self . check_cache_misses = lambda : None print ( \"Warning: ShardListDataset has a cache miss rate of {:.1%} %\" . format ( misses * 100.0 / accesses ) ) close () Close the dataset. Source code in wids/wids.py 591 592 593 def close ( self ): \"\"\"Close the dataset.\"\"\" self . cache . clear () get_shard ( index ) Get the shard and index within the shard corresponding to the given index. Source code in wids/wids.py 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 def get_shard ( self , index ): \"\"\"Get the shard and index within the shard corresponding to the given index.\"\"\" # Find the shard corresponding to the given index. shard_idx = np . searchsorted ( self . cum_lengths , index , side = \"right\" ) # Figure out which index within the shard corresponds to the # given index. if shard_idx == 0 : inner_idx = index else : inner_idx = index - self . cum_lengths [ shard_idx - 1 ] # Get the shard and return the corresponding element. desc = self . shards [ shard_idx ] url = desc [ \"url\" ] shard = self . cache . get_shard ( url ) return shard , inner_idx , desc get_stats () Return the number of cache accesses and misses. Source code in wids/wids.py 538 539 540 def get_stats ( self ): \"\"\"Return the number of cache accesses and misses.\"\"\" return self . cache . accesses , self . cache . misses wids.ChunkedSampler Bases: Sampler A sampler that samples in chunks and then shuffles the samples within each chunk. This preserves locality of reference while still shuffling the data. Source code in wids/wids.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 class ChunkedSampler ( Sampler ): \"\"\"A sampler that samples in chunks and then shuffles the samples within each chunk. This preserves locality of reference while still shuffling the data. \"\"\" def __init__ ( self , dataset , * , num_samples = None , chunksize = 2000 , seed = 0 , shuffle = True , shufflefirst = False , ): if isinstance ( num_samples , int ): lo , hi = 0 , num_samples elif num_samples is None : lo , hi = 0 , len ( dataset ) else : lo , hi = num_samples self . ranges = [( i , min ( i + chunksize , hi )) for i in range ( lo , hi , chunksize )] self . seed = seed self . shuffle = shuffle self . shufflefirst = shufflefirst self . epoch = 0 def set_epoch ( self , epoch ): self . epoch = epoch def __iter__ ( self ): self . rng = random . Random ( self . seed + 1289738273 * self . epoch ) shardshuffle = self . shufflefirst or self . epoch > 0 yield from iterate_ranges ( self . ranges , self . rng , indexshuffle = self . shuffle , shardshuffle = ( self . shuffle and shardshuffle ), ) self . epoch += 1 wids . DistributedChunkedSampler ( dataset , * , num_replicas = None , num_samples = None , rank = None , shuffle = True , shufflefirst = False , seed = 0 , drop_last = None , chunksize = 1000000 ) Return a ChunkedSampler for the current worker in distributed training. Reverts to a simple ChunkedSampler if not running in distributed mode. Since the split among workers takes place before the chunk shuffle, workers end up with a fixed set of shards they need to download. The more workers, the fewer shards are used by each worker. Source code in wids/wids.py 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 def DistributedChunkedSampler ( dataset : Dataset , * , num_replicas : Optional [ int ] = None , num_samples : Optional [ int ] = None , rank : Optional [ int ] = None , shuffle : bool = True , shufflefirst : bool = False , seed : int = 0 , drop_last : bool = None , chunksize : int = 1000000 , ) -> ChunkedSampler : \"\"\"Return a ChunkedSampler for the current worker in distributed training. Reverts to a simple ChunkedSampler if not running in distributed mode. Since the split among workers takes place before the chunk shuffle, workers end up with a fixed set of shards they need to download. The more workers, the fewer shards are used by each worker. \"\"\" if drop_last is not None : warnings . warn ( \"DistributedChunkedSampler does not support drop_last, thus it will be ignored\" ) if not dist . is_initialized (): warnings . warn ( \"DistributedChunkedSampler is called without distributed initialized; assuming single process\" ) num_replicas = 1 rank = 0 else : num_replicas = num_replicas or dist . get_world_size () rank = rank or dist . get_rank () assert rank >= 0 and rank < num_replicas num_samples = num_samples or len ( dataset ) worker_chunk = ( num_samples + num_replicas - 1 ) // num_replicas worker_start = rank * worker_chunk worker_end = min ( worker_start + worker_chunk , num_samples ) return ChunkedSampler ( dataset , num_samples = ( worker_start , worker_end ), chunksize = chunksize , seed = seed , shuffle = shuffle , shufflefirst = shufflefirst , ) wids . ShardedSampler = ShardListSampler module-attribute","title":"WIDS"},{"location":"wids/#wids-api","text":"","title":"WIDS API"},{"location":"wids/#wids.ShardListDataset","text":"Bases: Dataset [ T ] An indexable dataset based on a list of shards. The dataset is either given as a list of shards with optional options and name, or as a URL pointing to a JSON descriptor file. Datasets can reference other datasets via source_url . Shard references within a dataset are resolve relative to an explicitly given base property, or relative to the URL from which the dataset descriptor was loaded. Source code in wids/wids.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 class ShardListDataset ( Dataset [ T ]): \"\"\"An indexable dataset based on a list of shards. The dataset is either given as a list of shards with optional options and name, or as a URL pointing to a JSON descriptor file. Datasets can reference other datasets via `source_url`. Shard references within a dataset are resolve relative to an explicitly given `base` property, or relative to the URL from which the dataset descriptor was loaded. \"\"\" def __init__ ( self , shards , * , cache_size = int ( 1e12 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = \"PIL\" , keep = False , base = None , options = None , ): \"\"\"Create a ShardListDataset. Args: shards: a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size: the number of shards to keep in the cache lru_size: the number of shards to keep in the LRU cache localname: a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. \"\"\" if options is None : options = {} super ( ShardListDataset , self ) . __init__ () # shards is a list of (filename, length) pairs. We'll need to # keep track of the lengths and cumulative lengths to know how # to map indices to shards and indices within shards. if isinstance ( shards , ( str , io . IOBase )): if base is None and isinstance ( shards , str ): base = urldir ( shards ) self . base = base self . spec = load_dsdesc_and_resolve ( shards , options = options , base = base ) self . shards = self . spec . get ( \"shardlist\" , []) self . dataset_name = self . spec . get ( \"name\" ) or hash_dataset_name ( str ( shards )) else : self . base = None self . spec = options self . shards = shards self . dataset_name = dataset_name or hash_dataset_name ( str ( shards )) self . lengths = [ shard [ \"nsamples\" ] for shard in self . shards ] self . cum_lengths = np . cumsum ( self . lengths ) self . total_length = self . cum_lengths [ - 1 ] if cache_dir is not None : # when a cache dir is explicitly given, we download files into # that directory without any changes self . cache_dir = cache_dir self . localname = cache_localname ( cache_dir ) elif localname is not None : # when a localname function is given, we use that self . cache_dir = None self . localname = localname else : # when no cache dir or localname are given, use the cache from the environment self . cache_dir = os . environ . get ( \"WIDS_CACHE\" , \"/tmp/_wids_cache\" ) self . localname = default_localname ( self . cache_dir ) if True or int ( os . environ . get ( \"WIDS_VERBOSE\" , 0 )): nbytes = sum ( shard . get ( \"filesize\" , 0 ) for shard in self . shards ) nsamples = sum ( shard [ \"nsamples\" ] for shard in self . shards ) print ( str ( shards )[: 50 ], \"base:\" , self . base , \"name:\" , self . spec . get ( \"name\" ), \"nfiles:\" , len ( self . shards ), \"nbytes:\" , nbytes , \"samples:\" , nsamples , \"cache:\" , self . cache_dir , file = sys . stderr , ) self . transformations = interpret_transformations ( transformations ) if lru_size > 200 : warnings . warn ( \"LRU size is very large; consider reducing it to avoid running out of file descriptors\" ) self . cache = LRUShards ( lru_size , localname = self . localname , keep = keep ) def add_transform ( self , transform ): \"\"\"Add a transformation to the dataset.\"\"\" self . transformations . append ( transform ) return self def __len__ ( self ): \"\"\"Return the total number of samples in the dataset.\"\"\" return self . total_length def get_stats ( self ): \"\"\"Return the number of cache accesses and misses.\"\"\" return self . cache . accesses , self . cache . misses def check_cache_misses ( self ): \"\"\"Check if the cache miss rate is too high.\"\"\" accesses , misses = self . get_stats () if accesses > 100 and misses / accesses > 0.3 : # output a warning only once self . check_cache_misses = lambda : None print ( \"Warning: ShardListDataset has a cache miss rate of {:.1%} %\" . format ( misses * 100.0 / accesses ) ) def get_shard ( self , index ): \"\"\"Get the shard and index within the shard corresponding to the given index.\"\"\" # Find the shard corresponding to the given index. shard_idx = np . searchsorted ( self . cum_lengths , index , side = \"right\" ) # Figure out which index within the shard corresponds to the # given index. if shard_idx == 0 : inner_idx = index else : inner_idx = index - self . cum_lengths [ shard_idx - 1 ] # Get the shard and return the corresponding element. desc = self . shards [ shard_idx ] url = desc [ \"url\" ] shard = self . cache . get_shard ( url ) return shard , inner_idx , desc def __getitem__ ( self , index ): \"\"\"Return the sample corresponding to the given index.\"\"\" shard , inner_idx , desc = self . get_shard ( index ) sample = shard [ inner_idx ] # Check if we're missing the cache too often. self . check_cache_misses () sample [ \"__dataset__\" ] = desc . get ( \"dataset\" ) sample [ \"__index__\" ] = index sample [ \"__shard__\" ] = desc [ \"url\" ] sample [ \"__shardindex__\" ] = inner_idx # Apply transformations for transform in self . transformations : sample = transform ( sample ) return sample def close ( self ): \"\"\"Close the dataset.\"\"\" self . cache . clear ()","title":"ShardListDataset"},{"location":"wids/#wids.ShardListDataset.__getitem__","text":"Return the sample corresponding to the given index. Source code in wids/wids.py 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 def __getitem__ ( self , index ): \"\"\"Return the sample corresponding to the given index.\"\"\" shard , inner_idx , desc = self . get_shard ( index ) sample = shard [ inner_idx ] # Check if we're missing the cache too often. self . check_cache_misses () sample [ \"__dataset__\" ] = desc . get ( \"dataset\" ) sample [ \"__index__\" ] = index sample [ \"__shard__\" ] = desc [ \"url\" ] sample [ \"__shardindex__\" ] = inner_idx # Apply transformations for transform in self . transformations : sample = transform ( sample ) return sample","title":"__getitem__"},{"location":"wids/#wids.ShardListDataset.__init__","text":"Create a ShardListDataset. Parameters: shards \u2013 a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size \u2013 the number of shards to keep in the cache lru_size \u2013 the number of shards to keep in the LRU cache localname \u2013 a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. Source code in wids/wids.py 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 def __init__ ( self , shards , * , cache_size = int ( 1e12 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = \"PIL\" , keep = False , base = None , options = None , ): \"\"\"Create a ShardListDataset. Args: shards: a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size: the number of shards to keep in the cache lru_size: the number of shards to keep in the LRU cache localname: a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. \"\"\" if options is None : options = {} super ( ShardListDataset , self ) . __init__ () # shards is a list of (filename, length) pairs. We'll need to # keep track of the lengths and cumulative lengths to know how # to map indices to shards and indices within shards. if isinstance ( shards , ( str , io . IOBase )): if base is None and isinstance ( shards , str ): base = urldir ( shards ) self . base = base self . spec = load_dsdesc_and_resolve ( shards , options = options , base = base ) self . shards = self . spec . get ( \"shardlist\" , []) self . dataset_name = self . spec . get ( \"name\" ) or hash_dataset_name ( str ( shards )) else : self . base = None self . spec = options self . shards = shards self . dataset_name = dataset_name or hash_dataset_name ( str ( shards )) self . lengths = [ shard [ \"nsamples\" ] for shard in self . shards ] self . cum_lengths = np . cumsum ( self . lengths ) self . total_length = self . cum_lengths [ - 1 ] if cache_dir is not None : # when a cache dir is explicitly given, we download files into # that directory without any changes self . cache_dir = cache_dir self . localname = cache_localname ( cache_dir ) elif localname is not None : # when a localname function is given, we use that self . cache_dir = None self . localname = localname else : # when no cache dir or localname are given, use the cache from the environment self . cache_dir = os . environ . get ( \"WIDS_CACHE\" , \"/tmp/_wids_cache\" ) self . localname = default_localname ( self . cache_dir ) if True or int ( os . environ . get ( \"WIDS_VERBOSE\" , 0 )): nbytes = sum ( shard . get ( \"filesize\" , 0 ) for shard in self . shards ) nsamples = sum ( shard [ \"nsamples\" ] for shard in self . shards ) print ( str ( shards )[: 50 ], \"base:\" , self . base , \"name:\" , self . spec . get ( \"name\" ), \"nfiles:\" , len ( self . shards ), \"nbytes:\" , nbytes , \"samples:\" , nsamples , \"cache:\" , self . cache_dir , file = sys . stderr , ) self . transformations = interpret_transformations ( transformations ) if lru_size > 200 : warnings . warn ( \"LRU size is very large; consider reducing it to avoid running out of file descriptors\" ) self . cache = LRUShards ( lru_size , localname = self . localname , keep = keep )","title":"__init__"},{"location":"wids/#wids.ShardListDataset.__len__","text":"Return the total number of samples in the dataset. Source code in wids/wids.py 534 535 536 def __len__ ( self ): \"\"\"Return the total number of samples in the dataset.\"\"\" return self . total_length","title":"__len__"},{"location":"wids/#wids.ShardListDataset.add_transform","text":"Add a transformation to the dataset. Source code in wids/wids.py 529 530 531 532 def add_transform ( self , transform ): \"\"\"Add a transformation to the dataset.\"\"\" self . transformations . append ( transform ) return self","title":"add_transform"},{"location":"wids/#wids.ShardListDataset.check_cache_misses","text":"Check if the cache miss rate is too high. Source code in wids/wids.py 542 543 544 545 546 547 548 549 550 551 552 def check_cache_misses ( self ): \"\"\"Check if the cache miss rate is too high.\"\"\" accesses , misses = self . get_stats () if accesses > 100 and misses / accesses > 0.3 : # output a warning only once self . check_cache_misses = lambda : None print ( \"Warning: ShardListDataset has a cache miss rate of {:.1%} %\" . format ( misses * 100.0 / accesses ) )","title":"check_cache_misses"},{"location":"wids/#wids.ShardListDataset.close","text":"Close the dataset. Source code in wids/wids.py 591 592 593 def close ( self ): \"\"\"Close the dataset.\"\"\" self . cache . clear ()","title":"close"},{"location":"wids/#wids.ShardListDataset.get_shard","text":"Get the shard and index within the shard corresponding to the given index. Source code in wids/wids.py 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 def get_shard ( self , index ): \"\"\"Get the shard and index within the shard corresponding to the given index.\"\"\" # Find the shard corresponding to the given index. shard_idx = np . searchsorted ( self . cum_lengths , index , side = \"right\" ) # Figure out which index within the shard corresponds to the # given index. if shard_idx == 0 : inner_idx = index else : inner_idx = index - self . cum_lengths [ shard_idx - 1 ] # Get the shard and return the corresponding element. desc = self . shards [ shard_idx ] url = desc [ \"url\" ] shard = self . cache . get_shard ( url ) return shard , inner_idx , desc","title":"get_shard"},{"location":"wids/#wids.ShardListDataset.get_stats","text":"Return the number of cache accesses and misses. Source code in wids/wids.py 538 539 540 def get_stats ( self ): \"\"\"Return the number of cache accesses and misses.\"\"\" return self . cache . accesses , self . cache . misses","title":"get_stats"},{"location":"wids/#wids.ChunkedSampler","text":"Bases: Sampler A sampler that samples in chunks and then shuffles the samples within each chunk. This preserves locality of reference while still shuffling the data. Source code in wids/wids.py 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 class ChunkedSampler ( Sampler ): \"\"\"A sampler that samples in chunks and then shuffles the samples within each chunk. This preserves locality of reference while still shuffling the data. \"\"\" def __init__ ( self , dataset , * , num_samples = None , chunksize = 2000 , seed = 0 , shuffle = True , shufflefirst = False , ): if isinstance ( num_samples , int ): lo , hi = 0 , num_samples elif num_samples is None : lo , hi = 0 , len ( dataset ) else : lo , hi = num_samples self . ranges = [( i , min ( i + chunksize , hi )) for i in range ( lo , hi , chunksize )] self . seed = seed self . shuffle = shuffle self . shufflefirst = shufflefirst self . epoch = 0 def set_epoch ( self , epoch ): self . epoch = epoch def __iter__ ( self ): self . rng = random . Random ( self . seed + 1289738273 * self . epoch ) shardshuffle = self . shufflefirst or self . epoch > 0 yield from iterate_ranges ( self . ranges , self . rng , indexshuffle = self . shuffle , shardshuffle = ( self . shuffle and shardshuffle ), ) self . epoch += 1","title":"ChunkedSampler"},{"location":"wids/#wids.DistributedChunkedSampler","text":"Return a ChunkedSampler for the current worker in distributed training. Reverts to a simple ChunkedSampler if not running in distributed mode. Since the split among workers takes place before the chunk shuffle, workers end up with a fixed set of shards they need to download. The more workers, the fewer shards are used by each worker. Source code in wids/wids.py 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 def DistributedChunkedSampler ( dataset : Dataset , * , num_replicas : Optional [ int ] = None , num_samples : Optional [ int ] = None , rank : Optional [ int ] = None , shuffle : bool = True , shufflefirst : bool = False , seed : int = 0 , drop_last : bool = None , chunksize : int = 1000000 , ) -> ChunkedSampler : \"\"\"Return a ChunkedSampler for the current worker in distributed training. Reverts to a simple ChunkedSampler if not running in distributed mode. Since the split among workers takes place before the chunk shuffle, workers end up with a fixed set of shards they need to download. The more workers, the fewer shards are used by each worker. \"\"\" if drop_last is not None : warnings . warn ( \"DistributedChunkedSampler does not support drop_last, thus it will be ignored\" ) if not dist . is_initialized (): warnings . warn ( \"DistributedChunkedSampler is called without distributed initialized; assuming single process\" ) num_replicas = 1 rank = 0 else : num_replicas = num_replicas or dist . get_world_size () rank = rank or dist . get_rank () assert rank >= 0 and rank < num_replicas num_samples = num_samples or len ( dataset ) worker_chunk = ( num_samples + num_replicas - 1 ) // num_replicas worker_start = rank * worker_chunk worker_end = min ( worker_start + worker_chunk , num_samples ) return ChunkedSampler ( dataset , num_samples = ( worker_start , worker_end ), chunksize = chunksize , seed = seed , shuffle = shuffle , shufflefirst = shufflefirst , )","title":"DistributedChunkedSampler"},{"location":"wids/#wids.ShardedSampler","text":"","title":"ShardedSampler"}]}