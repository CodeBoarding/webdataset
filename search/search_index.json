{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"%matplotlib inline import matplotlib.pyplot as plt import torch.utils.data import torch.nn from random import randrange import os os.environ[\"WDS_VERBOSE_CACHE\"] = \"1\" os.environ[\"GOPEN_VERBOSE\"] = \"0\" The WebDataset Format WebDataset format files are tar files, with two conventions: within each tar file, files that belong together and make up a training sample share the same basename when stripped of all filename extensions the shards of a tar file are numbered like something-000000.tar to something-012345.tar , usually specified using brace notation something-{000000..012345}.tar You can find a longer, more detailed specification of the WebDataset format in the WebDataset Format Specification WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. WebDataset can also read concatenated MsgPack and CBORs sources. The WebDataset representation allows writing purely sequential I/O pipelines for large scale deep learning. This is important for achieving high I/O rates from local storage (3x-10x for local drives compared to random access) and for using object stores and cloud storage for training. The WebDataset format represents images, movies, audio, etc. in their native file formats, making the creation of WebDataset format data as easy as just creating a tar archive. Because of the way data is aligned, WebDataset works well with block deduplication as well and aligns data on predictable boundaries. Standard tools can be used for accessing and processing WebDataset-format files. bucket = \"https://storage.googleapis.com/webdataset/testdata/\" dataset = \"publaynet-train-{000000..000009}.tar\" url = bucket + dataset !curl -s {url} | tar tf - | sed 10q PMC4991227_00003.json PMC4991227_00003.png PMC4537884_00002.json PMC4537884_00002.png PMC4323233_00003.json PMC4323233_00003.png PMC5429906_00004.json PMC5429906_00004.png PMC5592712_00002.json PMC5592712_00002.png tar: stdout: write error Note that in these .tar files, we have pairs of .json and .png files; each such pair makes up a training sample. WebDataset Libraries There are several libraries supporting the WebDataset format: webdataset for Python3 (includes the wids library), this repository Webdataset.jl a Julia implementation tarp , a Golang implementation and command line tool Ray Data sources and sinks The webdataset library can be used with PyTorch, Tensorflow, and Jax. The webdataset Library The webdataset library is an implementation of PyTorch IterableDataset (or a mock implementation thereof if you aren't using PyTorch). It implements as form of stream processing. Some of its features are: large scale parallel data access through sharding high performance disk I/O due to purely sequential reads latency insensitive due to big fat pipes no local storage required instant startup for training jobs only requires reading from file descriptors/network streams, no special APIs its API encourages high performance I/O pipelines scalable from tiny desktop datasets to petascale datasets provides local caching if desired requires no dataset metadata; any collection of shards can be read and used instantly The main limitations people run into are related to the fact that IterableDataset is less commonly used in PyTorch and some existing code may not support it as well, and that achieving an exactly balanced number of training samples across many compute nodes for a fixed epoch size is tricky; for multinode training, webdataset is usually used with shard resampling. There are two interfaces, the concise \"fluid\" interface and a longer \"pipeline\" interface. We'll show examples using the fluid interface, which is usually what you want. import webdataset as wds pil_dataset = wds.WebDataset(url).shuffle(1000).decode(\"pil\").to_tuple(\"png\", \"json\") The resulting datasets are standard PyTorch IterableDataset instances. isinstance(pil_dataset, torch.utils.data.IterableDataset) True for image, json in pil_dataset: break plt.imshow(image) <matplotlib.image.AxesImage at 0x7f73806db970> We can add onto the existing pipeline for augmentation and data preparation. import torchvision.transforms as transforms from PIL import Image preproc = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), lambda x: 1-x, ]) def preprocess(sample): image, json = sample try: label = json[\"annotations\"][0][\"category_id\"] except: label = 0 return preproc(image), label dataset = pil_dataset.map(preprocess) for image, label in dataset: break plt.imshow(image.numpy().transpose(1, 2, 0)) <matplotlib.image.AxesImage at 0x7f7375fc2230> WebDataset is just an instance of a standard IterableDataset . It's a single-threaded way of iterating over a dataset. Since image decompression and data augmentation can be compute intensive, PyTorch usually uses the DataLoader class to parallelize data loading and preprocessing. WebDataset is fully compatible with the standard DataLoader . Here are a number of notebooks showing how to use WebDataset for image classification and LLM training: train-resnet50-wds -- simple, single GPU training from Imagenet train-resnet50-multiray-wds -- multinode training using webdataset generate-text-dataset -- initial dataset generation tesseract-wds -- shard-to-shard transformations, here for OCR running over large datasets train-ocr-errors-hf -- an example of LLM fine tuning using a dataset in webdataset format The wds-notes notebook contains some additional documentation and information about the library. The webdataset Pipeline API The wds.WebDataset fluid interface is just a convenient shorthand for writing down pipelines. The underlying pipeline is an instance of the wds.DataPipeline class, and you can construct data pipelines explicitly, similar to the way you use nn.Sequential inside models. dataset = wds.DataPipeline( wds.SimpleShardList(url), # at this point we have an iterator over all the shards wds.shuffle(100), # add wds.split_by_node here if you are using multiple nodes wds.split_by_worker, # at this point, we have an iterator over the shards assigned to each worker wds.tarfile_to_samples(), # this shuffles the samples in memory wds.shuffle(1000), # this decodes the images and json wds.decode(\"pil\"), wds.to_tuple(\"png\", \"json\"), wds.map(preprocess), wds.shuffle(1000), wds.batched(16) ) batch = next(iter(dataset)) batch[0].shape, batch[1].shape (torch.Size([16, 3, 224, 224]), (16,)) The wids Library for Indexed WebDatasets Installing the webdataset library installs a second library called wids . This library provides fully indexed/random access to the same datasets that webdataset accesses using iterators/streaming. Like the webdataset library, wids is high scalable and provides efficient access to very large datasets. Being indexed, it is easily backwards compatible with existing data pipelines based on indexed dataset, including precise epochs for multinode training. The library comes with its own ChunkedSampler and DistributedChunkedSampler classes, which provided shuffling accross nodes while still preserving enough locality of reference for efficient training. Internally, the library uses a mmap -based tar file reader implementation; this allows very fast access without precomputed indexes, and it also means that shard and the equivalet of \"shuffle buffers\" are shared in memory between workers on the same machine. This additional power comes at some cost: the library requires a small metadata file that lists all the shards in a dataset and the number of samples contained in each, the library requires local storage for as many shards as there are I/O workers on a node, it uses shared memory and mmap , and the availability of indexing makes it easy to accidentally use inefficient access patterns. Generally, the recommendation is to use webdataset for all data generation, data transformation, and training code, and to use wids only if you need fully random access to datasets (e.g., for browing or sparse sampling), need an indexed-based sampler, or are converting tricky legacy code. import wids train_url = \"https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train.json\" dataset = wids.ShardListDataset(train_url) sample = dataset[1900] print(sample.keys()) print(sample[\".txt\"]) plt.imshow(sample[\".jpg\"]) dict_keys(['.cls', '.jpg', '.txt', '__key__', '__dataset__', '__index__', '__shard__', '__shardindex__']) a high quality color photograph of a dog https://storage.googleapis.com/webdataset/fake-ima base: https://storage.googleapis.com/webdataset/fake-imagenet name: imagenet-train nfiles: 1282 nbytes: 31242280960 samples: 128200 cache: /tmp/_wids_cache <matplotlib.image.AxesImage at 0x7f7373669e70> There are several examples of how to use wids in the examples directory. train-resnet50-wids shows how to train a ResNet-50 model on ImageNet using wids train-resnet50-multiray-wids shows how to train a ResNet-50 model on ImageNet using multiple nodes Note that the APIs between webdataset and wids are not fully consistent: wids keeps the extension's \".\" in the keys, while webdataset removes it (\".txt\" vs \"txt\") wids doesn't have a fully fluid interface, and add_transformation just adds to a list of transformations webdataset currently can't read the wids JSON specifications Installation and Documentation $ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets Dependencies The WebDataset library only requires PyTorch, NumPy, and a small library called braceexpand . WebDataset loads a few additional libraries dynamically only when they are actually needed and only in the decoder: PIL/Pillow for image decoding torchvision , torchvideo , torchaudio for image/video/audio decoding msgpack for MessagePack decoding the curl command line tool for accessing HTTP servers the Google/Amazon/Azure command line tools for accessing cloud storage buckets Loading of one of these libraries is triggered by configuring a decoder that attempts to decode content in the given format and encountering a file in that format during decoding. (Eventually, the torch... dependencies will be refactored into those libraries.)","title":"README"},{"location":"#the-webdataset-format","text":"WebDataset format files are tar files, with two conventions: within each tar file, files that belong together and make up a training sample share the same basename when stripped of all filename extensions the shards of a tar file are numbered like something-000000.tar to something-012345.tar , usually specified using brace notation something-{000000..012345}.tar You can find a longer, more detailed specification of the WebDataset format in the WebDataset Format Specification WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. WebDataset can also read concatenated MsgPack and CBORs sources. The WebDataset representation allows writing purely sequential I/O pipelines for large scale deep learning. This is important for achieving high I/O rates from local storage (3x-10x for local drives compared to random access) and for using object stores and cloud storage for training. The WebDataset format represents images, movies, audio, etc. in their native file formats, making the creation of WebDataset format data as easy as just creating a tar archive. Because of the way data is aligned, WebDataset works well with block deduplication as well and aligns data on predictable boundaries. Standard tools can be used for accessing and processing WebDataset-format files. bucket = \"https://storage.googleapis.com/webdataset/testdata/\" dataset = \"publaynet-train-{000000..000009}.tar\" url = bucket + dataset !curl -s {url} | tar tf - | sed 10q PMC4991227_00003.json PMC4991227_00003.png PMC4537884_00002.json PMC4537884_00002.png PMC4323233_00003.json PMC4323233_00003.png PMC5429906_00004.json PMC5429906_00004.png PMC5592712_00002.json PMC5592712_00002.png tar: stdout: write error Note that in these .tar files, we have pairs of .json and .png files; each such pair makes up a training sample.","title":"The WebDataset Format"},{"location":"#webdataset-libraries","text":"There are several libraries supporting the WebDataset format: webdataset for Python3 (includes the wids library), this repository Webdataset.jl a Julia implementation tarp , a Golang implementation and command line tool Ray Data sources and sinks The webdataset library can be used with PyTorch, Tensorflow, and Jax.","title":"WebDataset Libraries"},{"location":"#the-webdataset-library","text":"The webdataset library is an implementation of PyTorch IterableDataset (or a mock implementation thereof if you aren't using PyTorch). It implements as form of stream processing. Some of its features are: large scale parallel data access through sharding high performance disk I/O due to purely sequential reads latency insensitive due to big fat pipes no local storage required instant startup for training jobs only requires reading from file descriptors/network streams, no special APIs its API encourages high performance I/O pipelines scalable from tiny desktop datasets to petascale datasets provides local caching if desired requires no dataset metadata; any collection of shards can be read and used instantly The main limitations people run into are related to the fact that IterableDataset is less commonly used in PyTorch and some existing code may not support it as well, and that achieving an exactly balanced number of training samples across many compute nodes for a fixed epoch size is tricky; for multinode training, webdataset is usually used with shard resampling. There are two interfaces, the concise \"fluid\" interface and a longer \"pipeline\" interface. We'll show examples using the fluid interface, which is usually what you want. import webdataset as wds pil_dataset = wds.WebDataset(url).shuffle(1000).decode(\"pil\").to_tuple(\"png\", \"json\") The resulting datasets are standard PyTorch IterableDataset instances. isinstance(pil_dataset, torch.utils.data.IterableDataset) True for image, json in pil_dataset: break plt.imshow(image) <matplotlib.image.AxesImage at 0x7f73806db970> We can add onto the existing pipeline for augmentation and data preparation. import torchvision.transforms as transforms from PIL import Image preproc = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), lambda x: 1-x, ]) def preprocess(sample): image, json = sample try: label = json[\"annotations\"][0][\"category_id\"] except: label = 0 return preproc(image), label dataset = pil_dataset.map(preprocess) for image, label in dataset: break plt.imshow(image.numpy().transpose(1, 2, 0)) <matplotlib.image.AxesImage at 0x7f7375fc2230> WebDataset is just an instance of a standard IterableDataset . It's a single-threaded way of iterating over a dataset. Since image decompression and data augmentation can be compute intensive, PyTorch usually uses the DataLoader class to parallelize data loading and preprocessing. WebDataset is fully compatible with the standard DataLoader . Here are a number of notebooks showing how to use WebDataset for image classification and LLM training: train-resnet50-wds -- simple, single GPU training from Imagenet train-resnet50-multiray-wds -- multinode training using webdataset generate-text-dataset -- initial dataset generation tesseract-wds -- shard-to-shard transformations, here for OCR running over large datasets train-ocr-errors-hf -- an example of LLM fine tuning using a dataset in webdataset format The wds-notes notebook contains some additional documentation and information about the library.","title":"The webdataset Library"},{"location":"#the-webdataset-pipeline-api","text":"The wds.WebDataset fluid interface is just a convenient shorthand for writing down pipelines. The underlying pipeline is an instance of the wds.DataPipeline class, and you can construct data pipelines explicitly, similar to the way you use nn.Sequential inside models. dataset = wds.DataPipeline( wds.SimpleShardList(url), # at this point we have an iterator over all the shards wds.shuffle(100), # add wds.split_by_node here if you are using multiple nodes wds.split_by_worker, # at this point, we have an iterator over the shards assigned to each worker wds.tarfile_to_samples(), # this shuffles the samples in memory wds.shuffle(1000), # this decodes the images and json wds.decode(\"pil\"), wds.to_tuple(\"png\", \"json\"), wds.map(preprocess), wds.shuffle(1000), wds.batched(16) ) batch = next(iter(dataset)) batch[0].shape, batch[1].shape (torch.Size([16, 3, 224, 224]), (16,))","title":"The webdataset Pipeline API"},{"location":"#the-wids-library-for-indexed-webdatasets","text":"Installing the webdataset library installs a second library called wids . This library provides fully indexed/random access to the same datasets that webdataset accesses using iterators/streaming. Like the webdataset library, wids is high scalable and provides efficient access to very large datasets. Being indexed, it is easily backwards compatible with existing data pipelines based on indexed dataset, including precise epochs for multinode training. The library comes with its own ChunkedSampler and DistributedChunkedSampler classes, which provided shuffling accross nodes while still preserving enough locality of reference for efficient training. Internally, the library uses a mmap -based tar file reader implementation; this allows very fast access without precomputed indexes, and it also means that shard and the equivalet of \"shuffle buffers\" are shared in memory between workers on the same machine. This additional power comes at some cost: the library requires a small metadata file that lists all the shards in a dataset and the number of samples contained in each, the library requires local storage for as many shards as there are I/O workers on a node, it uses shared memory and mmap , and the availability of indexing makes it easy to accidentally use inefficient access patterns. Generally, the recommendation is to use webdataset for all data generation, data transformation, and training code, and to use wids only if you need fully random access to datasets (e.g., for browing or sparse sampling), need an indexed-based sampler, or are converting tricky legacy code. import wids train_url = \"https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train.json\" dataset = wids.ShardListDataset(train_url) sample = dataset[1900] print(sample.keys()) print(sample[\".txt\"]) plt.imshow(sample[\".jpg\"]) dict_keys(['.cls', '.jpg', '.txt', '__key__', '__dataset__', '__index__', '__shard__', '__shardindex__']) a high quality color photograph of a dog https://storage.googleapis.com/webdataset/fake-ima base: https://storage.googleapis.com/webdataset/fake-imagenet name: imagenet-train nfiles: 1282 nbytes: 31242280960 samples: 128200 cache: /tmp/_wids_cache <matplotlib.image.AxesImage at 0x7f7373669e70> There are several examples of how to use wids in the examples directory. train-resnet50-wids shows how to train a ResNet-50 model on ImageNet using wids train-resnet50-multiray-wids shows how to train a ResNet-50 model on ImageNet using multiple nodes Note that the APIs between webdataset and wids are not fully consistent: wids keeps the extension's \".\" in the keys, while webdataset removes it (\".txt\" vs \"txt\") wids doesn't have a fully fluid interface, and add_transformation just adds to a list of transformations webdataset currently can't read the wids JSON specifications","title":"The wids Library for Indexed WebDatasets"},{"location":"#installation-and-documentation","text":"$ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets","title":"Installation and Documentation"},{"location":"#dependencies","text":"The WebDataset library only requires PyTorch, NumPy, and a small library called braceexpand . WebDataset loads a few additional libraries dynamically only when they are actually needed and only in the decoder: PIL/Pillow for image decoding torchvision , torchvideo , torchaudio for image/video/audio decoding msgpack for MessagePack decoding the curl command line tool for accessing HTTP servers the Google/Amazon/Azure command line tools for accessing cloud storage buckets Loading of one of these libraries is triggered by configuring a decoder that attempts to decode content in the given format and encountering a file in that format during decoding. (Eventually, the torch... dependencies will be refactored into those libraries.)","title":"Dependencies"},{"location":"FAQ/","text":"WebDataset FAQ This is a Frequently Asked Questions file for WebDataset. It is automatically generated from selected WebDataset issues using AI. Since the entries are generated automatically, not all of them may be correct. When in doubt, check the original issue. Issue #367 Q: How can I sample sequences of frames from large video datasets using WebDataset? A: To sample sequences of frames from large video datasets with WebDataset, you can precompute sampled sequences of frames and treat each collection as a batch. Alternatively, you can split your videos into shorter clips with overlapping frames, generate multiple samples from each clip, and shuffle the resulting sequences. Here's a code snippet demonstrating how to generate and shuffle five- frame sequences from 50-frame clips: from webdataset import WebDataset import random ds = WebDataset(\"video-clips-{000000..000999}.tar\").decode() def generate_clips(src): for sample in src: # assume that each video clip sample contains sample.000.jpg to sample.049.jpg images clip = [sample[\"%03d.jpg\" % i] for i in range(50)] starts = random.sample(range(46), 10) # Choose 10 starting points key = sample[\"__key__\"] for i in starts: yield { \"__key__\": f\"{key}-{i}\", \"sequence\": clip[i:i+5], } ds = ds.compose(generate_clips).shuffle(1000) This approach allows you to work with large datasets by handling smaller, manageable sequences, which can be efficiently preprocessed and shuffled to create a diverse training set. Issue #364 Q: How can I ensure that each validation sample is seen exactly once per epoch in a multi-node setup using WebDataset with FSDP? A: When using WebDataset in a multi-node setup with Fully Sharded Data Parallel (FSDP), you can ensure that each validation sample is seen exactly once per epoch by assigning each shard to a specific GPU. Since you have an equal number of shards and GPUs, you can map each shard to a GPU. For the shard that is about half the size, you can either accept that the corresponding GPU will do less work, or you can split another shard to balance the load. To ensure that each sample is loaded exactly once, you can use the wds.ResampledShards function without resampling, and avoid using ddp_equalize since it is designed for training rather than validation. Here's an example of how you might set up your validation dataset: val_dataset = wds.DataPipeline( wds.ResampledShards( os.path.join('path', 'to', 'val_samples_{0000...xxxx}.tar') ), wds.tarfile_to_samples(), wds.decode(), wds.to_tuple(\"input.npy\", \"target.npy\"), wds.batched(1) ).with_length(num_val_samples) To ensure that the validation loop stops after all samples have been loaded, you can use the length of the dataset to control the number of iterations in your validation loop. This way, you can manually iterate over the dataset and stop when you've reached the total number of samples. Issue #331 Q: How can I handle gzipped tar files with WebDataset/WIDS? A: When working with gzipped tar files in WebDataset or WIDS, it's important to understand that random access to compressed files is not straightforward due to the nature of compression. However, Python's tarfile library can handle gzip- compressed streams using tarfile.open(\"filename.tar.gz\", \"r:gz\") . For WIDS, the best practice is to use uncompressed .tar files for the dataset, which allows for efficient random access. If storage is a concern, you can compress individual files within the tar archive (e.g., .json.gz instead of .json ). This approach provides a balance between storage efficiency and compatibility with WIDS. Here's an example of how to compress individual files: import tarfile import gzip # Compress individual files and add them to a tar archive with tarfile.open('archive.tar', 'w') as tar: with open('file.json', 'rb') as f_in: with gzip.open('file.json.gz', 'wb') as f_out: f_out.writelines(f_in) tar.add('file.json.gz', arcname='file.json.gz') Remember that for WebDataset, you can use .tar.gz files directly, as it supports on-the-fly decompression. If you encounter datasets that are not in order, you can repack them using GNU tar with sorting to ensure that corresponding files are adjacent, which is a requirement for WebDataset. Issue #329 Q: How can I create a JSON metafile for random access in a WebDataset? A: To create a JSON metafile for a WebDataset, you can use the widsindex command that comes with the webdataset package. This command generates an index file for a given list of WebDataset shards. The index file is in JSON format and allows for efficient random access to the dataset. Here's a simple example of how to use widsindex : widsindex mydataset-0000.tar mydataset-0001.tar > mydataset-index.json This command will create a JSON file named mydataset-index.json that contains the index for the shards mydataset-0000.tar and mydataset-0001.tar . Issue #319 Q: How can I handle complex hierarchical data structures in WebDataset? A: When dealing with complex hierarchical data structures in WebDataset, it's often more practical to use a flat file naming scheme and express the hierarchy within a JSON metadata file. This approach simplifies the file naming while allowing for detailed structuring of the data. You can sequentially number the files and reference them in the JSON, which contains the structure of your dataset, including frame order, timestamps, and other relevant information. For example, instead of trying to express the hierarchy in the file names, you can name your files like this: sample_0.000.jpg sample_0.001.jpg sample_0.002.jpg sample_0.json And then use a JSON file to define the structure: { \"frames\": [\"000.jpg\", \"001.jpg\", \"002.jpg\"], \"timestamps\": [10001, 10002, 10003], \"duration\": 3 } This method keeps the file naming simple and leverages the JSON file to maintain the hierarchical relationships within the dataset. Issue #316 Q: Why am I getting a ValueError when trying to batch variable-length numpy arrays using webdataset? A: The error you're encountering is due to the attempt to collate numpy arrays with different shapes into a single batch. Since the num_frames dimension varies, you cannot directly convert a list of such arrays into a single numpy array without padding or truncating them to a uniform size. To resolve this, you can specify a custom collation function that handles variable-length sequences appropriately. This function can either pad the sequences to the same length or store them in a data structure that accommodates variable lengths, such as a list or a padded tensor. Here's an example of how to specify a custom collation function: def custom_collate_fn(batch): # Handle variable-length sequences here, e.g., by padding # Return the batch in the desired format return batch pipeline.extend([ # ... other pipeline steps ... wds.batched(args.batch_size, collation_fn=custom_collate_fn, partial=not is_train) ]) By providing a custom collation function, you can ensure that the data is prepared in a way that is compatible with your model's input requirements. Issue #307 Q: Can I skip loading large files in a tar file when using WebDataset? A: When working with WebDataset , it is not possible to skip the reading of files within a tar archive that you do not need. The library operates on a streaming basis, which means that all bytes are read sequentially. However, you can filter out unwanted data after it has been read into memory. If performance is a concern, consider creating a new dataset containing only the necessary files. For indexed access to WebDataset files, you can use the \"wids\" interface, which reads only the data you use from disk when working with local files. Here's a short example of filtering out unwanted data after reading: dataset = wds.WebDataset([\"path/to/dataset.tar\"]) keys_to_keep = [\"__key__\", \"__url__\", \"txt\"] def filter_keys(sample): return {k: sample[k] for k in keys_to_keep if k in sample} filtered_dataset = dataset.map(filter_keys) Issue #303 Q: Why does the number of steps per epoch change when increasing num_workers in DDP training with Webdataset? A: When using multiple workers in a distributed data parallel (DDP) training setup with Webdataset, the number of steps per epoch may change if the epoch size is not properly configured to account for the parallelism introduced by the workers. The with_epoch method should be applied to the WebLoader instead of the WebDataset to ensure that the dataset is correctly divided among the workers. Additionally, to maintain proper shuffling across workers, you may need to add cross-worker shuffling. Here's an example of how to configure the loader: data = wds.WebDataset(self.url, resampled=True).shuffle(1000).map(preprocess_train) loader = wds.WebLoader(data, pin_memory=True, shuffle=False, batch_size=20, num_workers=2).with_epoch(...) For cross-worker shuffling, you can modify the loader like this: loader = loader.unbatched().shuffle(2000).batched(20).with_epoch(200) Issue #291 Q: How can I skip a corrupt image sample when using NVIDIA DALI for data loading? A: When working with NVIDIA DALI for data loading, you can handle corrupt or missing data by using the handler parameter. This parameter allows you to specify a behavior when a decoding error occurs. For example, you can use warn_and_continue to issue a warning and skip the problematic sample, allowing the data pipeline to continue processing the remaining samples. This is particularly useful when dealing with large datasets where some samples may be corrupt or unreadable. Here's a short code example demonstrating how to use the handler parameter: from nvidia.dali.plugin import pytorch import webdataset as wds def warn_and_continue(e): print(\"Warning: skipping a corrupt sample.\", e) ds = ( wds.WebDataset(url, handler=warn_and_continue, shardshuffle=True, verbose=verbose) .map(_mapper, handler=warn_and_continue) .to_tuple(\"jpg\", \"cls\") .map_tuple(transform, identity, handler=warn_and_continue) .batched(batch_size) ) By passing warn_and_continue to the .map , .map_tuple , or .decode methods, you instruct DALI to handle exceptions gracefully and continue with the next sample. Issue #289 Q: Can WebDataset support interleaved datasets such as MMC4, where one example may include a list of texts with several images? A: Yes, WebDataset can support interleaved datasets like MMC4. You can organize your dataset by creating a .json file that contains the hierarchical structure and references to the image files. This .json file acts as a manifest for each sample, detailing the associated text and images. The image files themselves are stored alongside the .json file. Here's a simple example of how you might structure a .json file for an interleaved dataset: { \"text\": [\"This is the first text\", \"This is the second text\"], \"images\": [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"] } And in your dataset, you would have the .json file and the referenced images in the same sample directory or archive. Issue #283 Q: How can I authenticate to read objects from a private bucket with WebDataset? A: To authenticate and read objects from a private bucket using WebDataset, you need to provide the necessary credentials to the underlying command line programs that WebDataset uses for data access. If you are using a storage provider like NetApp, which is not directly supported by WebDataset's built-in protocols, you can use the pipe: protocol to specify a custom command that includes the necessary authentication steps. For example, you can create a shell script that uses your storage provider's CLI tools to authenticate with your access key id and secret access key , and then pass this script to WebDataset: # auth_script.sh # This script authenticates and downloads a shard from a private bucket # Replace <ACCESS_KEY>, <SECRET_KEY>, <BUCKET_NAME>, and <SHARD_NAME> with your actual values netappcli --access-key <ACCESS_KEY> --secret-key <SECRET_KEY> download <BUCKET_NAME>/<SHARD_NAME> Then, use this script with WebDataset: import webdataset as wds # Use the 'pipe:' protocol with your authentication script dataset = wds.WebDataset(\"pipe:./auth_script.sh\") Ensure that your script has the necessary permissions to be executed and that it correctly handles the authentication and data retrieval process. Issue #278 Q: Why is with_epoch(N) needed for multinode training with WebDataset? A: When using WebDataset for training models in PyTorch, the with_epoch(N) function is used to define the end of an epoch when working with an infinite stream of samples. This is particularly important in distributed training scenarios to ensure that all nodes process the same number of batches per epoch, which helps in synchronizing the training process across nodes. Without with_epoch(N) , the training loop would not have a clear indication of when an epoch ends, potentially leading to inconsistent training states among different nodes. WebDataset operates with the IterableDataset interface, which does not support the set_epoch method used by DistributedSampler in PyTorch's DataLoader . Therefore, with_epoch(N) serves as a mechanism to delineate epochs in the absence of set_epoch . # Example of using with_epoch in a training loop for epoch in range(num_epochs): for sample in webdataset_reader.with_epoch(epoch_length): train(sample) Issue #264 Q: How can I include the file name (only the stem, not the extension) in the metadata dictionary when using WebDataset? A: When working with WebDataset, each sample in the dataset contains a special key __key__ that holds the file name without the extension. To include the file name in the metadata dictionary, you can create a custom mapping function that extracts the __key__ and adds it to the metadata . Here's a short code example on how to modify the pipeline to include the file name in the metadata : def add_filename_to_metadata(sample): sample[\"metadata\"][\"filename\"] = sample[\"__key__\"] return sample pipeline = [ # ... (other pipeline steps) wds.map(add_filename_to_metadata), # ... (remaining pipeline steps) ] This function should be added to the pipeline after the wds.decode step and before the wds.to_tuple step. This way, the metadata dictionary will contain the file name for each sample processed by the pipeline. Issue #261 Q: Why is my WebDataset tar file unexpectedly large when saving individual tensors? A: The large file size is due to the fact that each tensor is pointing to a large underlying byte array buffer, which is being saved in its entirety. This results in saving much more data than just the tensor's contents. To fix this, you should clone the tensor before saving it to ensure that only the relevant data is written to the file. Additionally, each file in a tar archive has a 512-byte header, which can add significant overhead when saving many small files. To reduce file size, consider compressing the tar file or batching tensors before saving. Here's a code snippet showing how to clone the tensor before saving: with wds.TarWriter(f\"/tmp/dest.tar\") as sink: for i, d in tqdm(enumerate(tensordict), total=N): obj = {\"__key__\": f\"{i}\"} for k, v in d.items(): buffer = io.BytesIO() torch.save(v.clone(), buffer) # Clone the tensor here obj[f\"{k}.pth\"] = buffer.getvalue() sink.write(obj) To compress the tar file, simply save it with a .tar.gz extension and use a compression library: with wds.TarWriter(f\"/tmp/dest.tar.gz\", compressor=\"gz\") as sink: # ... rest of the code ... Issue #260 Q: What is the purpose of the .with_epoch() method in WebDataset and could it be named more descriptively? A: The .with_epoch() method in WebDataset is used to explicitly set the number of samples that constitute an epoch during distributed training. This is important for ensuring that each worker in a distributed system processes a full epoch's worth of data. The name .with_epoch() might not be immediately clear, but it is intended to indicate that the dataset is being configured with a specific epoch length. A more descriptive name like .set_epoch_size() could potentially convey the purpose more clearly. However, changing the method name would be a breaking change for existing codebases. Improving the documentation with examples can help clarify the usage: # Original method name dataset = dataset.with_epoch(10000) # Hypothetical more descriptive method name dataset = dataset.set_epoch_size(10000) In the meantime, users should refer to the improved documentation for guidance on how to use the .with_epoch() method effectively. Issue #257 Q: How can I efficiently load only the necessary auxiliary images for a sample in my training configuration to save on I/O and decoding time? A: When working with datasets that include a main image and multiple auxiliary images, you can optimize the data loading process by selectively reading only the required files. This can be achieved by using the select_files option in WebDataset or similar tools, which allows you to specify which files to extract from the dataset. By pre-selecting the files during the dataset preparation phase, you ensure that your tar files contain exactly the files needed for training, minimizing unnecessary I/O operations and decoding time for unused images. Here's a short example of how you might use select_files : import webdataset as wds # Define your selection criteria based on the training configuration def select_files(sample): return [sample['main.jpg']] + [sample[f'aux{i}.jpg'] for i in range(number_of_aux_images)] # Create a dataset and apply the selection dataset = wds.WebDataset(\"dataset.tar\").select(select_files) This approach is more efficient than reading all files and discarding the unneeded ones, as it avoids the overhead of reading and decoding data that will not be used in the training process. Issue #256 Q: Why does my training program using WebDataset consume so much memory and crash? A: The memory consumption issue you're experiencing with WebDataset during training is likely due to the shuffle buffer size. WebDataset uses in-memory buffering to shuffle data, and if the buffer size is too large, it can consume a significant amount of memory, especially when dealing with large datasets or when running on systems with limited memory. The parameters _SHARD_SHUFFLE_SIZE and _SAMPLE_SHUFFLE_SIZE control the number of shards and samples kept in memory for shuffling. Reducing these values can help mitigate memory usage issues. For example, you can try setting: _SHARD_SHUFFLE_SIZE = 1000 # Reduced from 2000 _SAMPLE_SHUFFLE_SIZE = 2500 # Reduced from 5000 Adjust these values based on your system's memory capacity and the size of your dataset. Keep in mind that reducing the shuffle buffer size may affect the randomness of your data shuffling and potentially the training results. It's a trade-off between memory usage and shuffle effectiveness. Issue #249 Q: Should I use WebDataset or TorchData for my data loading in PyTorch? A: The choice between WebDataset and TorchData depends on your specific needs and the context of your project. WebDataset is still a good choice if you require backwards compatibility or if you need to work without PyTorch. It is also being integrated with other frameworks like Ray, which may be beneficial for certain use cases. However, it's important to note that as of July 2023, active development on TorchData has been paused to re-evaluate its technical design. This means that while TorchData is still usable, it may not receive updates or new features in the near future. If you are starting a new project or are able to adapt to changes, you might want to consider this factor. Here's a simple example of how you might use WebDataset: import webdataset as wds # Create a dataset dataset = wds.WebDataset(\"path/to/data-{000000..000999}.tar\") # Iterate over the dataset for sample in dataset: image, label = sample[\"image\"], sample[\"label\"] # process image and label And here's how you might use TorchData: from torchdata.datapipes.iter import FileOpener, TarArchiveReader # Create a data pipeline datapipes = FileOpener(\"path/to/data.tar\") \\ .parse(TarArchiveReader()) # Iterate over the data pipeline for file_name, file_stream in datapipes: # process file_stream Given the pause in TorchData development, you should consider the stability and future support of the library when making your decision. Issue #247 Q: How can I load images from nested tar files using webdataset? A: To load images from nested tar files with webdataset, you can create a custom decoder that handles .tar files using Python's tarfile module. This decoder can be applied to your dataset with the .map() method, which allows you to modify each sample in the dataset. The custom decoder will read the nested tar file from the sample, extract its contents, and add them to the sample dictionary. Here's a short example of how you can implement this: import io import tarfile from webdataset import WebDataset def expand_tar_files(sample): stream = tarfile.open(fileobj=io.BytesIO(sample[\"tar\"])) for tarinfo in stream: if tarinfo.isfile(): name = tarinfo.name data = stream.extractfile(tarinfo).read() sample[name] = data return sample ds = WebDataset(\"dataset.tar\").map(expand_tar_files).decode(\"...\") In this example, expand_tar_files is a function that takes a sample from the dataset, opens the nested tar file contained within it, and adds each file from the nested tar to the sample. The WebDataset object is then created with the path to the dataset tar file, and the expand_tar_files function is applied to each sample in the dataset. Issue #246 Q: What is the purpose of .to_tuple() in WebDataset and how does it handle missing files? A: The .to_tuple() method in WebDataset is used to extract specific fields from a dataset where each sample is a dictionary with keys corresponding to file extensions. This method simplifies the process of preparing data for training by converting dictionaries into tuples, which are more convenient to work with in many machine learning frameworks. When you specify multiple file extensions separated by semicolons, .to_tuple() will return the first file that matches any of the given extensions. If a file with a specified extension is not present in a sample, .to_tuple() will raise an error. To handle optional files, you can use a custom function with .map() that uses the get method to return None if a key is missing, thus avoiding errors and allowing for flexible data structures. Here's an example of using .to_tuple() with mandatory and optional files: # Mandatory jpg and txt, optional npy def make_tuple(sample): return sample[\"jpg\"], sample.get(\"npy\"), sample[\"txt\"] ds = WebDataset(...) ... .map(make_tuple) And here's how you might use .to_tuple() directly for mandatory files: ds = WebDataset(...) ... .to_tuple(\"jpg\", \"txt\") Issue #244 Q: How can I combine multiple data sources with a specified frequency for sampling from each? A: To combine multiple data sources with non-integer sampling frequencies, you can use the RandomMix function from the WebDataset library. This function allows you to specify the relative sampling weights as floating-point numbers, which can represent the desired sampling frequency from each dataset. Here's an example of how to use RandomMix to combine two datasets with a specified sampling frequency: from webdataset import WebDataset, RandomMix ds1 = WebDataset('path_to_shards_A/{00..99}.tar') ds2 = WebDataset('path_to_shards_B/{00..99}.tar') mix = RandomMix([ds1, ds2], [1.45, 1.0]) # Sampling from ds1 1.45 times more frequently than ds2 This will create a mixed dataset where samples from ds1 are drawn approximately 1.45 times more often than samples from ds2 . Issue #239 Q: Can I filter a WebDataset to select only a subset of categories? A: Yes, you can filter a WebDataset to select only a subset of categories by using a map function. This is efficient as long as the subset is not too small; otherwise, it can lead to inefficient I/O due to random disk accesses. For very small subsets, it's recommended to create a new WebDataset. Here's a simple example of how to filter categories: def select(sample): if sample[\"cls\"] in [0, 3, 9]: # Replace with desired categories return sample else: return None dataset = wds.WebDataset(...).decode().map(select) This approach works well when the number of classes is much larger than the number of shards, and you're not discarding a significant portion of the data. If you find yourself discarding a large percentage of the data, consider creating a new WebDataset for efficiency. Issue #237 Q: How does WebDataset handle filenames with multiple periods when extracting keys? A: WebDataset uses periods to separate the base filename from the extension, which can lead to unexpected keys when multiple periods are present in the base filename. This is by design to support filenames with multiple extensions, such as .seg.jpg . It's important to follow this convention when creating datasets to avoid issues in downstream processing. If you have filenames with multiple periods, consider renaming them before creating the dataset. For matching files, you can use glob patterns like *.mp3 to ensure you're working with the correct file type. # Example of using a glob pattern to match files with the .mp3 extension dataset = wds.Dataset(\"dataset.tar\").select(lambda x: fnmatch.fnmatch(x, \"*.mp3\")) Issue #236 Q: How does webdataset handle the conversion of tensors to different file formats like .jpg and .npy? A: In webdataset, the conversion of tensors to specific file formats is determined by the file extension you specify in the key when writing the data using ShardWriter . There is no automatic conversion; the tensor is simply saved in the format corresponding to the extension you provide. When reading the data, you can decode the files into tensors using the appropriate arguments. Here's a short example of how to write a tensor as different file formats: from webdataset import ShardWriter writer = ShardWriter(...) sample = {} sample[\"__key__\"] = \"dataset/sample00003\" sample[\"image.jpg\"] = some_tensor # Will be saved as a JPEG file sample[\"image.npy\"] = some_tensor # Will be saved as a NPY file writer.write(sample) When you write a sample with {\"__key__\": \"xyz\", \"image.jpg\": some_tensor} , a JPEG file named xyz.image.jpg is created. Conversely, if you write {\"__key__\": \"xyz\", \"image.npy\": some_tensor} , an NPY file named xyz.image.npy is created. Issue #233 Q: How do I ensure that WebDataset correctly splits shards across multiple nodes and workers? A: When using WebDataset for distributed training across multiple nodes and workers, it's important to use the split_by_node and split_by_worker functions to ensure that each node and worker processes a unique subset of the data. The detshuffle function can be used for deterministic shuffling of shards before splitting. Here's a minimal example of how to set up the dataset pipeline for multi-node training: import webdataset as wds dataset = wds.DataPipeline( wds.SimpleShardList(\"source-{000000..000999}.tar\"), wds.detshuffle(), wds.split_by_node, wds.split_by_worker, ) for idx, item in enumerate(iter(dataset)): if idx < 2: # Just for demonstration print(f\"item: {item}\") Make sure you are using a recent version of WebDataset that supports these features. If you encounter any issues, check the version and consider updating to the latest release. Issue #227 Q: How can I use Apache Beam to write data to a WebDataset tar file for large-scale machine learning datasets? A: Apache Beam is a powerful tool for parallel data processing, which can be used to build large datasets for machine learning. When dealing with datasets larger than 10TB and requiring complex preprocessing, you can use Apache Beam to process and write the data into a WebDataset tar file format. Below is a simplified example of how you might set up your Beam pipeline to write to a WebDataset. This example assumes you have a function preprocess_sample that takes a sample and performs the necessary preprocessing: import apache_beam as beam from webdataset import ShardWriter def write_to_webdataset(sample): # Assuming 'preprocess_sample' is a function that preprocesses your data processed_sample = preprocess_sample(sample) # Write the processed sample to a shard using ShardWriter # This is a simplified example; you'll need to manage shards and temp files with ShardWriter(\"output_shard.tar\", maxcount=1000) as sink: sink.write(processed_sample) # Set up your Apache Beam pipeline with beam.Pipeline() as pipeline: ( pipeline | 'Read Data' >> beam.io.ReadFromSomething(...) # Replace with your data source | 'Process and Write' >> beam.Map(write_to_webdataset) ) Remember to manage the sharding and temporary files appropriately, as the ShardWriter will need to write to different shards based on your dataset's partitioning. The maxcount parameter controls how many items are in each shard. You will also need to handle the copying of the temporary shard files to your destination bucket as needed. Issue #225 Q: How can I ensure that Distributed Data Parallel (DDP) training with WebDataset doesn't hang due to uneven data distribution across nodes? A: When using WebDataset for DDP training, it's important to ensure that all nodes receive the same number of samples to prevent hanging during synchronization. One effective method is to create a number of shards that is divisible by the total number of workers and ensure each shard contains the same number of samples. Assign each worker the same number of shards to achieve exact epochs with no resampling, duplication, or missing samples. If the dataset cannot be evenly divided, you can use resampled=True to generate an infinite stream of samples, and set an epoch length using with_epoch . This approach allows for synchronization across workers even if the dataset size is not divisible by the number of workers. Here's an example of setting an epoch length: from webdataset import WebDataset dataset = WebDataset(urls, resampled=True).with_epoch(epoch_length) For validation, where you want to avoid arbitrary epoch lengths, you can drop samples from the end of the validation set to make its size divisible by the world size. This can be done using TorchData as follows: from torch.utils.data import DataLoader import torch.distributed dataset = dataset.batch(torch.distributed.get_world_size(), drop_last=True) dataset = dataset.unbatch() dataset = dataset.sharding_filter() Remember to use the sharding_filter to ensure that each process only sees its own subset of the data. Issue #219 Q: What should I use instead of ShardList in webdataset v2, and how do I specify a splitter? A: In webdataset v2, the ShardList class has been renamed to SimpleShardList . If you encounter an AttributeError stating that the module webdataset has no attribute ShardList , you should replace it with SimpleShardList . Additionally, the splitter argument has been changed to nodesplitter . Here's how you can update your code to reflect these changes: urls = list(braceexpand.braceexpand(\"dataset-{000000..000999}.tar\")) dataset = wds.SimpleShardList(urls, splitter=wds.split_by_worker, nodesplitter=wds.split_by_node, shuffle=False) dataset = wds.Processor(dataset, wds.url_opener) dataset = wds.Processor(dataset, wds.tar_file_expander) dataset = wds.Processor(dataset, wds.group_by_keys) If you are using WebDataset and encounter a TypeError regarding an unexpected keyword argument splitter , ensure that you are using the correct argument name nodesplitter instead. Issue #216 Q: Can I use ShardWriter to write directly to a cloud storage URL like Google Cloud Storage? A: The ShardWriter from the webdataset library is primarily designed to write shards to a local disk, and then these shards can be copied to cloud storage. Writing directly to cloud storage is not the default behavior because it can be less efficient and more error-prone due to network issues. However, if you have a large dataset that cannot be stored locally, you can modify the ShardWriter code to write directly to a cloud URL by changing the line where the TarWriter is instantiated. Here's a short example of the modification: # Original line in ShardWriter self.tarstream = TarWriter(open(self.fname, \"wb\"), **self.kw) # Modified line to write directly to a cloud URL self.tarstream = TarWriter(self.fname, **self.kw) Please note that this is a workaround and may not be officially supported. It's recommended to test thoroughly to ensure data integrity and handle any potential exceptions related to network issues. Issue #212 Q: Does WebDataset download all shards at once, and how does caching affect the download behavior? A: WebDataset accesses shards individually and handles data in a streaming fashion by default, meaning that shards are not cached locally unless caching is explicitly enabled. When caching is enabled, each shard is downloaded completely before being used, which can block training until the download is finished. This behavior contrasts with the streaming mode, where training can start as soon as the first batch is ready. The caching mechanism does not currently download shards in parallel with training, which can lead to delays when starting the training process. To change the local cache name when using pipe:s3 , you can override the url_to_name argument to map shard names to cache file names as desired. Here's an example of how to override the url_to_name function: import webdataset as wds def custom_url_to_name(url): # Custom logic to convert URL to a cache filename return url.replace(\"http://url/dataset-\", \"\").replace(\".tar\", \".cache\") dataset = wds.WebDataset(\"pipe:s3 http://url/dataset-{001..099}.tar\", url_to_name=custom_url_to_name) Issue #211 Q: How can I write to a remote location using ShardWriter? A: ShardWriter is designed to write to local disk for simplicity and reliability, but it provides a hook for uploading data to a remote location. You can define a function that handles the upload process and then pass this function to the post parameter of ShardWriter. Here's a short example of how to use this feature: def upload_shard(fname): os.system(f\"gsutil cp {fname} gs://mybucket\") os.unlink(fname) with ShardWriter(..., post=upload_shard) as writer: # Your code to add data to the writer ... This approach allows you to have control over the upload process and handle any errors that may occur during the transfer to the remote storage. Issue #210 Q: How does the default_collation_fn work in WebDataset when it seems to expect a list or tuple, but the documentation suggests it should handle a collection of samples as dictionaries? A: The confusion arises from the mismatch between the documentation and the actual implementation of default_collation_fn . The function is designed to take a batch of samples and collate them into a single batch for processing. However, the current implementation of default_collation_fn in WebDataset does not handle dictionaries directly. Instead, it expects each sample in the batch to be a list or tuple. If you have a batch of dictionaries, you would need to convert them into a list or tuple format before using default_collation_fn . Alternatively, you can use torch.utils.data.default_collate from PyTorch 1.11 or later, which can handle dictionaries, or you can provide a custom collate function that handles dictionaries. Here's an example of a custom collate function that could handle a list of dictionaries: def custom_collate_fn(batch): # Assuming each element in batch is a dictionary collated_batch = {} for key in batch[0].keys(): collated_batch[key] = [d[key] for d in batch] return collated_batch You can then pass this custom_collate_fn to your data loader. Issue #209 Q: How can I ensure each batch contains only one description per image when using webdatasets? A: To ensure that each batch contains only one description per image in webdatasets, you can create a custom transformation function that acts as a filter or collate function. This function can be composed with your dataset to enforce the batching rule. You can use buffers or other conditional logic within your transformation to manage the batching process. Here's a simple example of how you might start implementing such a transformation: def unique_image_collate(src): buffer = {} for sample in src: image_id = sample['image_id'] if image_id not in buffer: buffer[image_id] = sample if len(buffer) == batch_size: yield list(buffer.values()) buffer.clear() # Additional logic to handle leftovers, etc. if buffer: yield list(buffer.values()) dataset = dataset.compose(unique_image_collate) This function collects samples in a buffer until it has a batch's worth of unique images, then yields that batch and clears the buffer for the next batch. You'll need to add additional logic to handle cases such as the end of an epoch where the buffer may not be full. Issue #201 Q: How can I efficiently subsample a large dataset without slowing down iteration speed? A: When dealing with large datasets, such as LAION 400M, and needing to subsample based on metadata, there are several strategies to maintain high I/O performance. If the subset is small and static, it's best to create a new dataset ahead of time. This can be done using a WebDataset/TarWriter pipeline or with tarp proc ... | tarp split ... commands, potentially parallelizing the process with tools like ray . If dynamic selection is necessary, consider splitting the dataset into shards by the categories of interest. This approach avoids random file accesses, which can significantly slow down data pipelines. Here's a simple example of creating a subset using tarp : tarp proc mydataset.tar -c 'if sample[\"metadata\"] in metadata_list: yield sample' tarp split -o subset-%06d.tar --size=1e9 Remember to perform filtering before any heavy operations like decoding or augmentation to avoid unnecessary processing. Issue #196 Q: How can I speed up subsampling from a tar file when using WebDataset? A: When working with WebDataset, it's important to remember that it is optimized for streaming data and does not support efficient random access within tar files. To speed up subsampling, you should avoid using very small probabilities with rsample as it requires reading the entire stream. Instead, consider using more shards and applying rsample to the shards rather than individual samples. This approach avoids the overhead of sequential reading. Additionally, some storage servers like AIStore can perform server-side sampling, which can be more efficient as they can use random access. # Example of using rsample with shards dataset = WebDataset(\"dataset-{0000..9999}.tar\").rsample(0.1) Issue #194 Q: How should I balance dataset elements across DDP nodes when using WebDataset? A: When using WebDataset with Distributed Data Parallel (DDP) in PyTorch, you may encounter situations where the dataset is not evenly distributed across the workers. To address this, you can use the .repeat() method in combination with .with_epoch() to ensure that each worker processes the same number of batches. The .repeat(2) method is used to repeat the dataset twice, which should be sufficient for most cases. If the dataset is highly unbalanced, you may need to adjust this number. The .with_epoch(n) method is used to limit the number of samples processed in an epoch to n , where n is typically set to the total number of samples divided by the batch size. This combination ensures that each epoch has a consistent size across workers, while also handling any imbalance in the number of shards or samples per worker. Here's an example of how to use these methods: batch_size = 64 epoch_size = 1281237 # Total number of samples in the dataset loader = wds.WebLoader(dataset, num_workers=4) loader = loader.repeat(2).with_epoch(epoch_size // batch_size) This approach allows for a balanced distribution of data across DDP nodes, with the caveat that some batches may be missing or repeated. It's a trade-off between perfect balance and resource usage. Issue #185 Q: How can I include the original file name in the metadata dictionary when iterating through a WebDataset? A: When working with WebDataset, you can include the original file name in the metadata dictionary by defining a function that extracts the __key__ from the sample and adds it to the metadata. You then apply this function using the .map() method in your pipeline. Here's a short example of how to define and use such a function: def add_filename_to_metadata(sample): sample[\"metadata\"][\"filename\"] = sample[\"__key__\"] return sample # Add this to your pipeline after renaming the keys pipeline.append(wds.map(add_filename_to_metadata)) This function should be added to the pipeline after the renaming step to ensure that the metadata key is already present in the sample dictionary. Issue #177 Q: How can I resume training from a specific step without iterating over unused data when using WebDataset? A: When using WebDataset for training with large datasets, it's common to want to resume training from a specific step without loading all the previous data into memory. WebDataset provides a feature for this scenario through shard resampling. By setting resampled=True or using the wds.resampled pipeline stage, you can ensure that you get the same training statistics when restarting your job without the need to skip samples manually. This approach is recommended over trying to implement \"each sample exactly once per epoch,\" which can be complex and environment-dependent. Here's a short example of how you might use the resampled option: from webdataset import WebDataset dataset = WebDataset(urls).resampled(rng=my_random_state) And here's how you might use the wds.resampled pipeline stage: import webdataset as wds dataset = wds.WebDataset(urls).pipe(wds.resampled) Issue #172 Q: Why does the detshuffle epoch count not increment across epochs when using WebDataset? A: The issue with detshuffle not incrementing the epoch count across epochs is likely due to the interaction between the DataLoader's worker process management and the internal state of the detshuffle . When persistent_workers=False , the DataLoader creates new worker processes each epoch, which do not retain the state of the detshuffle instance. This results in the detshuffle epoch count resetting each time. To maintain the state across epochs, you can set persistent_workers=True in the DataLoader. Alternatively, you can manage the epoch count externally and pass it to detshuffle if needed. Here's a short example of how to set persistent_workers : from torch.utils.data import DataLoader # Assuming 'dataset' is your WebDataset instance loader = DataLoader(dataset, persistent_workers=True) If you need to manage the epoch count externally, you could use an environment variable or another mechanism to pass the epoch count to detshuffle . However, this approach is less clean and should be used with caution, as it may introduce complexity and potential bugs into your code. Issue #171 Q: I'm getting an ImportError when trying to import PytorchShardList from webdataset . What should I do? A: The PytorchShardList class has been removed in recent versions of the webdataset package. If you are using version 0.1 of webdataset , PytorchShardList was available, but in later versions, it has likely been replaced with SimpleShardList . To resolve the ImportError, you should update your import statement to use the new class name. Here's how you can import SimpleShardList : from webdataset import SimpleShardList If SimpleShardList does not meet your requirements, you may need to check the documentation for the version of webdataset you are using to find the appropriate replacement or consider downgrading to the version that contains PytorchShardList . Issue #170 Q: How do I use glob patterns with WebDataset to read data from Google Cloud Storage (GCS)? A: WebDataset does not natively support glob patterns due to the lack of a consistent API for globbing across different object stores. To use glob patterns with files stored in GCS, you need to manually resolve the glob pattern using gsutil and then pass the list of shards to WebDataset. Here's an example of how to do this in Python: import os import webdataset as wds # Use gsutil to resolve the glob pattern and get the list of shard URLs shard_list = [shard.strip() for shard in os.popen(\"gsutil ls gs://BUCKET/PATH/training_*.tar\").readlines()] # Create the WebDataset with the resolved list of shard URLs train_data = wds.WebDataset(shard_list, shardshuffle=True, repeat=True) This approach ensures that you get the expected behavior when reading data from shards that match a glob pattern in GCS. Remember to install gsutil and authenticate with GCS before running the code.","title":"FAQ"},{"location":"FAQ/#webdataset-faq","text":"This is a Frequently Asked Questions file for WebDataset. It is automatically generated from selected WebDataset issues using AI. Since the entries are generated automatically, not all of them may be correct. When in doubt, check the original issue. Issue #367 Q: How can I sample sequences of frames from large video datasets using WebDataset? A: To sample sequences of frames from large video datasets with WebDataset, you can precompute sampled sequences of frames and treat each collection as a batch. Alternatively, you can split your videos into shorter clips with overlapping frames, generate multiple samples from each clip, and shuffle the resulting sequences. Here's a code snippet demonstrating how to generate and shuffle five- frame sequences from 50-frame clips: from webdataset import WebDataset import random ds = WebDataset(\"video-clips-{000000..000999}.tar\").decode() def generate_clips(src): for sample in src: # assume that each video clip sample contains sample.000.jpg to sample.049.jpg images clip = [sample[\"%03d.jpg\" % i] for i in range(50)] starts = random.sample(range(46), 10) # Choose 10 starting points key = sample[\"__key__\"] for i in starts: yield { \"__key__\": f\"{key}-{i}\", \"sequence\": clip[i:i+5], } ds = ds.compose(generate_clips).shuffle(1000) This approach allows you to work with large datasets by handling smaller, manageable sequences, which can be efficiently preprocessed and shuffled to create a diverse training set. Issue #364 Q: How can I ensure that each validation sample is seen exactly once per epoch in a multi-node setup using WebDataset with FSDP? A: When using WebDataset in a multi-node setup with Fully Sharded Data Parallel (FSDP), you can ensure that each validation sample is seen exactly once per epoch by assigning each shard to a specific GPU. Since you have an equal number of shards and GPUs, you can map each shard to a GPU. For the shard that is about half the size, you can either accept that the corresponding GPU will do less work, or you can split another shard to balance the load. To ensure that each sample is loaded exactly once, you can use the wds.ResampledShards function without resampling, and avoid using ddp_equalize since it is designed for training rather than validation. Here's an example of how you might set up your validation dataset: val_dataset = wds.DataPipeline( wds.ResampledShards( os.path.join('path', 'to', 'val_samples_{0000...xxxx}.tar') ), wds.tarfile_to_samples(), wds.decode(), wds.to_tuple(\"input.npy\", \"target.npy\"), wds.batched(1) ).with_length(num_val_samples) To ensure that the validation loop stops after all samples have been loaded, you can use the length of the dataset to control the number of iterations in your validation loop. This way, you can manually iterate over the dataset and stop when you've reached the total number of samples. Issue #331 Q: How can I handle gzipped tar files with WebDataset/WIDS? A: When working with gzipped tar files in WebDataset or WIDS, it's important to understand that random access to compressed files is not straightforward due to the nature of compression. However, Python's tarfile library can handle gzip- compressed streams using tarfile.open(\"filename.tar.gz\", \"r:gz\") . For WIDS, the best practice is to use uncompressed .tar files for the dataset, which allows for efficient random access. If storage is a concern, you can compress individual files within the tar archive (e.g., .json.gz instead of .json ). This approach provides a balance between storage efficiency and compatibility with WIDS. Here's an example of how to compress individual files: import tarfile import gzip # Compress individual files and add them to a tar archive with tarfile.open('archive.tar', 'w') as tar: with open('file.json', 'rb') as f_in: with gzip.open('file.json.gz', 'wb') as f_out: f_out.writelines(f_in) tar.add('file.json.gz', arcname='file.json.gz') Remember that for WebDataset, you can use .tar.gz files directly, as it supports on-the-fly decompression. If you encounter datasets that are not in order, you can repack them using GNU tar with sorting to ensure that corresponding files are adjacent, which is a requirement for WebDataset. Issue #329 Q: How can I create a JSON metafile for random access in a WebDataset? A: To create a JSON metafile for a WebDataset, you can use the widsindex command that comes with the webdataset package. This command generates an index file for a given list of WebDataset shards. The index file is in JSON format and allows for efficient random access to the dataset. Here's a simple example of how to use widsindex : widsindex mydataset-0000.tar mydataset-0001.tar > mydataset-index.json This command will create a JSON file named mydataset-index.json that contains the index for the shards mydataset-0000.tar and mydataset-0001.tar . Issue #319 Q: How can I handle complex hierarchical data structures in WebDataset? A: When dealing with complex hierarchical data structures in WebDataset, it's often more practical to use a flat file naming scheme and express the hierarchy within a JSON metadata file. This approach simplifies the file naming while allowing for detailed structuring of the data. You can sequentially number the files and reference them in the JSON, which contains the structure of your dataset, including frame order, timestamps, and other relevant information. For example, instead of trying to express the hierarchy in the file names, you can name your files like this: sample_0.000.jpg sample_0.001.jpg sample_0.002.jpg sample_0.json And then use a JSON file to define the structure: { \"frames\": [\"000.jpg\", \"001.jpg\", \"002.jpg\"], \"timestamps\": [10001, 10002, 10003], \"duration\": 3 } This method keeps the file naming simple and leverages the JSON file to maintain the hierarchical relationships within the dataset. Issue #316 Q: Why am I getting a ValueError when trying to batch variable-length numpy arrays using webdataset? A: The error you're encountering is due to the attempt to collate numpy arrays with different shapes into a single batch. Since the num_frames dimension varies, you cannot directly convert a list of such arrays into a single numpy array without padding or truncating them to a uniform size. To resolve this, you can specify a custom collation function that handles variable-length sequences appropriately. This function can either pad the sequences to the same length or store them in a data structure that accommodates variable lengths, such as a list or a padded tensor. Here's an example of how to specify a custom collation function: def custom_collate_fn(batch): # Handle variable-length sequences here, e.g., by padding # Return the batch in the desired format return batch pipeline.extend([ # ... other pipeline steps ... wds.batched(args.batch_size, collation_fn=custom_collate_fn, partial=not is_train) ]) By providing a custom collation function, you can ensure that the data is prepared in a way that is compatible with your model's input requirements. Issue #307 Q: Can I skip loading large files in a tar file when using WebDataset? A: When working with WebDataset , it is not possible to skip the reading of files within a tar archive that you do not need. The library operates on a streaming basis, which means that all bytes are read sequentially. However, you can filter out unwanted data after it has been read into memory. If performance is a concern, consider creating a new dataset containing only the necessary files. For indexed access to WebDataset files, you can use the \"wids\" interface, which reads only the data you use from disk when working with local files. Here's a short example of filtering out unwanted data after reading: dataset = wds.WebDataset([\"path/to/dataset.tar\"]) keys_to_keep = [\"__key__\", \"__url__\", \"txt\"] def filter_keys(sample): return {k: sample[k] for k in keys_to_keep if k in sample} filtered_dataset = dataset.map(filter_keys) Issue #303 Q: Why does the number of steps per epoch change when increasing num_workers in DDP training with Webdataset? A: When using multiple workers in a distributed data parallel (DDP) training setup with Webdataset, the number of steps per epoch may change if the epoch size is not properly configured to account for the parallelism introduced by the workers. The with_epoch method should be applied to the WebLoader instead of the WebDataset to ensure that the dataset is correctly divided among the workers. Additionally, to maintain proper shuffling across workers, you may need to add cross-worker shuffling. Here's an example of how to configure the loader: data = wds.WebDataset(self.url, resampled=True).shuffle(1000).map(preprocess_train) loader = wds.WebLoader(data, pin_memory=True, shuffle=False, batch_size=20, num_workers=2).with_epoch(...) For cross-worker shuffling, you can modify the loader like this: loader = loader.unbatched().shuffle(2000).batched(20).with_epoch(200) Issue #291 Q: How can I skip a corrupt image sample when using NVIDIA DALI for data loading? A: When working with NVIDIA DALI for data loading, you can handle corrupt or missing data by using the handler parameter. This parameter allows you to specify a behavior when a decoding error occurs. For example, you can use warn_and_continue to issue a warning and skip the problematic sample, allowing the data pipeline to continue processing the remaining samples. This is particularly useful when dealing with large datasets where some samples may be corrupt or unreadable. Here's a short code example demonstrating how to use the handler parameter: from nvidia.dali.plugin import pytorch import webdataset as wds def warn_and_continue(e): print(\"Warning: skipping a corrupt sample.\", e) ds = ( wds.WebDataset(url, handler=warn_and_continue, shardshuffle=True, verbose=verbose) .map(_mapper, handler=warn_and_continue) .to_tuple(\"jpg\", \"cls\") .map_tuple(transform, identity, handler=warn_and_continue) .batched(batch_size) ) By passing warn_and_continue to the .map , .map_tuple , or .decode methods, you instruct DALI to handle exceptions gracefully and continue with the next sample. Issue #289 Q: Can WebDataset support interleaved datasets such as MMC4, where one example may include a list of texts with several images? A: Yes, WebDataset can support interleaved datasets like MMC4. You can organize your dataset by creating a .json file that contains the hierarchical structure and references to the image files. This .json file acts as a manifest for each sample, detailing the associated text and images. The image files themselves are stored alongside the .json file. Here's a simple example of how you might structure a .json file for an interleaved dataset: { \"text\": [\"This is the first text\", \"This is the second text\"], \"images\": [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"] } And in your dataset, you would have the .json file and the referenced images in the same sample directory or archive. Issue #283 Q: How can I authenticate to read objects from a private bucket with WebDataset? A: To authenticate and read objects from a private bucket using WebDataset, you need to provide the necessary credentials to the underlying command line programs that WebDataset uses for data access. If you are using a storage provider like NetApp, which is not directly supported by WebDataset's built-in protocols, you can use the pipe: protocol to specify a custom command that includes the necessary authentication steps. For example, you can create a shell script that uses your storage provider's CLI tools to authenticate with your access key id and secret access key , and then pass this script to WebDataset: # auth_script.sh # This script authenticates and downloads a shard from a private bucket # Replace <ACCESS_KEY>, <SECRET_KEY>, <BUCKET_NAME>, and <SHARD_NAME> with your actual values netappcli --access-key <ACCESS_KEY> --secret-key <SECRET_KEY> download <BUCKET_NAME>/<SHARD_NAME> Then, use this script with WebDataset: import webdataset as wds # Use the 'pipe:' protocol with your authentication script dataset = wds.WebDataset(\"pipe:./auth_script.sh\") Ensure that your script has the necessary permissions to be executed and that it correctly handles the authentication and data retrieval process. Issue #278 Q: Why is with_epoch(N) needed for multinode training with WebDataset? A: When using WebDataset for training models in PyTorch, the with_epoch(N) function is used to define the end of an epoch when working with an infinite stream of samples. This is particularly important in distributed training scenarios to ensure that all nodes process the same number of batches per epoch, which helps in synchronizing the training process across nodes. Without with_epoch(N) , the training loop would not have a clear indication of when an epoch ends, potentially leading to inconsistent training states among different nodes. WebDataset operates with the IterableDataset interface, which does not support the set_epoch method used by DistributedSampler in PyTorch's DataLoader . Therefore, with_epoch(N) serves as a mechanism to delineate epochs in the absence of set_epoch . # Example of using with_epoch in a training loop for epoch in range(num_epochs): for sample in webdataset_reader.with_epoch(epoch_length): train(sample) Issue #264 Q: How can I include the file name (only the stem, not the extension) in the metadata dictionary when using WebDataset? A: When working with WebDataset, each sample in the dataset contains a special key __key__ that holds the file name without the extension. To include the file name in the metadata dictionary, you can create a custom mapping function that extracts the __key__ and adds it to the metadata . Here's a short code example on how to modify the pipeline to include the file name in the metadata : def add_filename_to_metadata(sample): sample[\"metadata\"][\"filename\"] = sample[\"__key__\"] return sample pipeline = [ # ... (other pipeline steps) wds.map(add_filename_to_metadata), # ... (remaining pipeline steps) ] This function should be added to the pipeline after the wds.decode step and before the wds.to_tuple step. This way, the metadata dictionary will contain the file name for each sample processed by the pipeline. Issue #261 Q: Why is my WebDataset tar file unexpectedly large when saving individual tensors? A: The large file size is due to the fact that each tensor is pointing to a large underlying byte array buffer, which is being saved in its entirety. This results in saving much more data than just the tensor's contents. To fix this, you should clone the tensor before saving it to ensure that only the relevant data is written to the file. Additionally, each file in a tar archive has a 512-byte header, which can add significant overhead when saving many small files. To reduce file size, consider compressing the tar file or batching tensors before saving. Here's a code snippet showing how to clone the tensor before saving: with wds.TarWriter(f\"/tmp/dest.tar\") as sink: for i, d in tqdm(enumerate(tensordict), total=N): obj = {\"__key__\": f\"{i}\"} for k, v in d.items(): buffer = io.BytesIO() torch.save(v.clone(), buffer) # Clone the tensor here obj[f\"{k}.pth\"] = buffer.getvalue() sink.write(obj) To compress the tar file, simply save it with a .tar.gz extension and use a compression library: with wds.TarWriter(f\"/tmp/dest.tar.gz\", compressor=\"gz\") as sink: # ... rest of the code ... Issue #260 Q: What is the purpose of the .with_epoch() method in WebDataset and could it be named more descriptively? A: The .with_epoch() method in WebDataset is used to explicitly set the number of samples that constitute an epoch during distributed training. This is important for ensuring that each worker in a distributed system processes a full epoch's worth of data. The name .with_epoch() might not be immediately clear, but it is intended to indicate that the dataset is being configured with a specific epoch length. A more descriptive name like .set_epoch_size() could potentially convey the purpose more clearly. However, changing the method name would be a breaking change for existing codebases. Improving the documentation with examples can help clarify the usage: # Original method name dataset = dataset.with_epoch(10000) # Hypothetical more descriptive method name dataset = dataset.set_epoch_size(10000) In the meantime, users should refer to the improved documentation for guidance on how to use the .with_epoch() method effectively. Issue #257 Q: How can I efficiently load only the necessary auxiliary images for a sample in my training configuration to save on I/O and decoding time? A: When working with datasets that include a main image and multiple auxiliary images, you can optimize the data loading process by selectively reading only the required files. This can be achieved by using the select_files option in WebDataset or similar tools, which allows you to specify which files to extract from the dataset. By pre-selecting the files during the dataset preparation phase, you ensure that your tar files contain exactly the files needed for training, minimizing unnecessary I/O operations and decoding time for unused images. Here's a short example of how you might use select_files : import webdataset as wds # Define your selection criteria based on the training configuration def select_files(sample): return [sample['main.jpg']] + [sample[f'aux{i}.jpg'] for i in range(number_of_aux_images)] # Create a dataset and apply the selection dataset = wds.WebDataset(\"dataset.tar\").select(select_files) This approach is more efficient than reading all files and discarding the unneeded ones, as it avoids the overhead of reading and decoding data that will not be used in the training process. Issue #256 Q: Why does my training program using WebDataset consume so much memory and crash? A: The memory consumption issue you're experiencing with WebDataset during training is likely due to the shuffle buffer size. WebDataset uses in-memory buffering to shuffle data, and if the buffer size is too large, it can consume a significant amount of memory, especially when dealing with large datasets or when running on systems with limited memory. The parameters _SHARD_SHUFFLE_SIZE and _SAMPLE_SHUFFLE_SIZE control the number of shards and samples kept in memory for shuffling. Reducing these values can help mitigate memory usage issues. For example, you can try setting: _SHARD_SHUFFLE_SIZE = 1000 # Reduced from 2000 _SAMPLE_SHUFFLE_SIZE = 2500 # Reduced from 5000 Adjust these values based on your system's memory capacity and the size of your dataset. Keep in mind that reducing the shuffle buffer size may affect the randomness of your data shuffling and potentially the training results. It's a trade-off between memory usage and shuffle effectiveness. Issue #249 Q: Should I use WebDataset or TorchData for my data loading in PyTorch? A: The choice between WebDataset and TorchData depends on your specific needs and the context of your project. WebDataset is still a good choice if you require backwards compatibility or if you need to work without PyTorch. It is also being integrated with other frameworks like Ray, which may be beneficial for certain use cases. However, it's important to note that as of July 2023, active development on TorchData has been paused to re-evaluate its technical design. This means that while TorchData is still usable, it may not receive updates or new features in the near future. If you are starting a new project or are able to adapt to changes, you might want to consider this factor. Here's a simple example of how you might use WebDataset: import webdataset as wds # Create a dataset dataset = wds.WebDataset(\"path/to/data-{000000..000999}.tar\") # Iterate over the dataset for sample in dataset: image, label = sample[\"image\"], sample[\"label\"] # process image and label And here's how you might use TorchData: from torchdata.datapipes.iter import FileOpener, TarArchiveReader # Create a data pipeline datapipes = FileOpener(\"path/to/data.tar\") \\ .parse(TarArchiveReader()) # Iterate over the data pipeline for file_name, file_stream in datapipes: # process file_stream Given the pause in TorchData development, you should consider the stability and future support of the library when making your decision. Issue #247 Q: How can I load images from nested tar files using webdataset? A: To load images from nested tar files with webdataset, you can create a custom decoder that handles .tar files using Python's tarfile module. This decoder can be applied to your dataset with the .map() method, which allows you to modify each sample in the dataset. The custom decoder will read the nested tar file from the sample, extract its contents, and add them to the sample dictionary. Here's a short example of how you can implement this: import io import tarfile from webdataset import WebDataset def expand_tar_files(sample): stream = tarfile.open(fileobj=io.BytesIO(sample[\"tar\"])) for tarinfo in stream: if tarinfo.isfile(): name = tarinfo.name data = stream.extractfile(tarinfo).read() sample[name] = data return sample ds = WebDataset(\"dataset.tar\").map(expand_tar_files).decode(\"...\") In this example, expand_tar_files is a function that takes a sample from the dataset, opens the nested tar file contained within it, and adds each file from the nested tar to the sample. The WebDataset object is then created with the path to the dataset tar file, and the expand_tar_files function is applied to each sample in the dataset. Issue #246 Q: What is the purpose of .to_tuple() in WebDataset and how does it handle missing files? A: The .to_tuple() method in WebDataset is used to extract specific fields from a dataset where each sample is a dictionary with keys corresponding to file extensions. This method simplifies the process of preparing data for training by converting dictionaries into tuples, which are more convenient to work with in many machine learning frameworks. When you specify multiple file extensions separated by semicolons, .to_tuple() will return the first file that matches any of the given extensions. If a file with a specified extension is not present in a sample, .to_tuple() will raise an error. To handle optional files, you can use a custom function with .map() that uses the get method to return None if a key is missing, thus avoiding errors and allowing for flexible data structures. Here's an example of using .to_tuple() with mandatory and optional files: # Mandatory jpg and txt, optional npy def make_tuple(sample): return sample[\"jpg\"], sample.get(\"npy\"), sample[\"txt\"] ds = WebDataset(...) ... .map(make_tuple) And here's how you might use .to_tuple() directly for mandatory files: ds = WebDataset(...) ... .to_tuple(\"jpg\", \"txt\") Issue #244 Q: How can I combine multiple data sources with a specified frequency for sampling from each? A: To combine multiple data sources with non-integer sampling frequencies, you can use the RandomMix function from the WebDataset library. This function allows you to specify the relative sampling weights as floating-point numbers, which can represent the desired sampling frequency from each dataset. Here's an example of how to use RandomMix to combine two datasets with a specified sampling frequency: from webdataset import WebDataset, RandomMix ds1 = WebDataset('path_to_shards_A/{00..99}.tar') ds2 = WebDataset('path_to_shards_B/{00..99}.tar') mix = RandomMix([ds1, ds2], [1.45, 1.0]) # Sampling from ds1 1.45 times more frequently than ds2 This will create a mixed dataset where samples from ds1 are drawn approximately 1.45 times more often than samples from ds2 . Issue #239 Q: Can I filter a WebDataset to select only a subset of categories? A: Yes, you can filter a WebDataset to select only a subset of categories by using a map function. This is efficient as long as the subset is not too small; otherwise, it can lead to inefficient I/O due to random disk accesses. For very small subsets, it's recommended to create a new WebDataset. Here's a simple example of how to filter categories: def select(sample): if sample[\"cls\"] in [0, 3, 9]: # Replace with desired categories return sample else: return None dataset = wds.WebDataset(...).decode().map(select) This approach works well when the number of classes is much larger than the number of shards, and you're not discarding a significant portion of the data. If you find yourself discarding a large percentage of the data, consider creating a new WebDataset for efficiency. Issue #237 Q: How does WebDataset handle filenames with multiple periods when extracting keys? A: WebDataset uses periods to separate the base filename from the extension, which can lead to unexpected keys when multiple periods are present in the base filename. This is by design to support filenames with multiple extensions, such as .seg.jpg . It's important to follow this convention when creating datasets to avoid issues in downstream processing. If you have filenames with multiple periods, consider renaming them before creating the dataset. For matching files, you can use glob patterns like *.mp3 to ensure you're working with the correct file type. # Example of using a glob pattern to match files with the .mp3 extension dataset = wds.Dataset(\"dataset.tar\").select(lambda x: fnmatch.fnmatch(x, \"*.mp3\")) Issue #236 Q: How does webdataset handle the conversion of tensors to different file formats like .jpg and .npy? A: In webdataset, the conversion of tensors to specific file formats is determined by the file extension you specify in the key when writing the data using ShardWriter . There is no automatic conversion; the tensor is simply saved in the format corresponding to the extension you provide. When reading the data, you can decode the files into tensors using the appropriate arguments. Here's a short example of how to write a tensor as different file formats: from webdataset import ShardWriter writer = ShardWriter(...) sample = {} sample[\"__key__\"] = \"dataset/sample00003\" sample[\"image.jpg\"] = some_tensor # Will be saved as a JPEG file sample[\"image.npy\"] = some_tensor # Will be saved as a NPY file writer.write(sample) When you write a sample with {\"__key__\": \"xyz\", \"image.jpg\": some_tensor} , a JPEG file named xyz.image.jpg is created. Conversely, if you write {\"__key__\": \"xyz\", \"image.npy\": some_tensor} , an NPY file named xyz.image.npy is created. Issue #233 Q: How do I ensure that WebDataset correctly splits shards across multiple nodes and workers? A: When using WebDataset for distributed training across multiple nodes and workers, it's important to use the split_by_node and split_by_worker functions to ensure that each node and worker processes a unique subset of the data. The detshuffle function can be used for deterministic shuffling of shards before splitting. Here's a minimal example of how to set up the dataset pipeline for multi-node training: import webdataset as wds dataset = wds.DataPipeline( wds.SimpleShardList(\"source-{000000..000999}.tar\"), wds.detshuffle(), wds.split_by_node, wds.split_by_worker, ) for idx, item in enumerate(iter(dataset)): if idx < 2: # Just for demonstration print(f\"item: {item}\") Make sure you are using a recent version of WebDataset that supports these features. If you encounter any issues, check the version and consider updating to the latest release. Issue #227 Q: How can I use Apache Beam to write data to a WebDataset tar file for large-scale machine learning datasets? A: Apache Beam is a powerful tool for parallel data processing, which can be used to build large datasets for machine learning. When dealing with datasets larger than 10TB and requiring complex preprocessing, you can use Apache Beam to process and write the data into a WebDataset tar file format. Below is a simplified example of how you might set up your Beam pipeline to write to a WebDataset. This example assumes you have a function preprocess_sample that takes a sample and performs the necessary preprocessing: import apache_beam as beam from webdataset import ShardWriter def write_to_webdataset(sample): # Assuming 'preprocess_sample' is a function that preprocesses your data processed_sample = preprocess_sample(sample) # Write the processed sample to a shard using ShardWriter # This is a simplified example; you'll need to manage shards and temp files with ShardWriter(\"output_shard.tar\", maxcount=1000) as sink: sink.write(processed_sample) # Set up your Apache Beam pipeline with beam.Pipeline() as pipeline: ( pipeline | 'Read Data' >> beam.io.ReadFromSomething(...) # Replace with your data source | 'Process and Write' >> beam.Map(write_to_webdataset) ) Remember to manage the sharding and temporary files appropriately, as the ShardWriter will need to write to different shards based on your dataset's partitioning. The maxcount parameter controls how many items are in each shard. You will also need to handle the copying of the temporary shard files to your destination bucket as needed. Issue #225 Q: How can I ensure that Distributed Data Parallel (DDP) training with WebDataset doesn't hang due to uneven data distribution across nodes? A: When using WebDataset for DDP training, it's important to ensure that all nodes receive the same number of samples to prevent hanging during synchronization. One effective method is to create a number of shards that is divisible by the total number of workers and ensure each shard contains the same number of samples. Assign each worker the same number of shards to achieve exact epochs with no resampling, duplication, or missing samples. If the dataset cannot be evenly divided, you can use resampled=True to generate an infinite stream of samples, and set an epoch length using with_epoch . This approach allows for synchronization across workers even if the dataset size is not divisible by the number of workers. Here's an example of setting an epoch length: from webdataset import WebDataset dataset = WebDataset(urls, resampled=True).with_epoch(epoch_length) For validation, where you want to avoid arbitrary epoch lengths, you can drop samples from the end of the validation set to make its size divisible by the world size. This can be done using TorchData as follows: from torch.utils.data import DataLoader import torch.distributed dataset = dataset.batch(torch.distributed.get_world_size(), drop_last=True) dataset = dataset.unbatch() dataset = dataset.sharding_filter() Remember to use the sharding_filter to ensure that each process only sees its own subset of the data. Issue #219 Q: What should I use instead of ShardList in webdataset v2, and how do I specify a splitter? A: In webdataset v2, the ShardList class has been renamed to SimpleShardList . If you encounter an AttributeError stating that the module webdataset has no attribute ShardList , you should replace it with SimpleShardList . Additionally, the splitter argument has been changed to nodesplitter . Here's how you can update your code to reflect these changes: urls = list(braceexpand.braceexpand(\"dataset-{000000..000999}.tar\")) dataset = wds.SimpleShardList(urls, splitter=wds.split_by_worker, nodesplitter=wds.split_by_node, shuffle=False) dataset = wds.Processor(dataset, wds.url_opener) dataset = wds.Processor(dataset, wds.tar_file_expander) dataset = wds.Processor(dataset, wds.group_by_keys) If you are using WebDataset and encounter a TypeError regarding an unexpected keyword argument splitter , ensure that you are using the correct argument name nodesplitter instead. Issue #216 Q: Can I use ShardWriter to write directly to a cloud storage URL like Google Cloud Storage? A: The ShardWriter from the webdataset library is primarily designed to write shards to a local disk, and then these shards can be copied to cloud storage. Writing directly to cloud storage is not the default behavior because it can be less efficient and more error-prone due to network issues. However, if you have a large dataset that cannot be stored locally, you can modify the ShardWriter code to write directly to a cloud URL by changing the line where the TarWriter is instantiated. Here's a short example of the modification: # Original line in ShardWriter self.tarstream = TarWriter(open(self.fname, \"wb\"), **self.kw) # Modified line to write directly to a cloud URL self.tarstream = TarWriter(self.fname, **self.kw) Please note that this is a workaround and may not be officially supported. It's recommended to test thoroughly to ensure data integrity and handle any potential exceptions related to network issues. Issue #212 Q: Does WebDataset download all shards at once, and how does caching affect the download behavior? A: WebDataset accesses shards individually and handles data in a streaming fashion by default, meaning that shards are not cached locally unless caching is explicitly enabled. When caching is enabled, each shard is downloaded completely before being used, which can block training until the download is finished. This behavior contrasts with the streaming mode, where training can start as soon as the first batch is ready. The caching mechanism does not currently download shards in parallel with training, which can lead to delays when starting the training process. To change the local cache name when using pipe:s3 , you can override the url_to_name argument to map shard names to cache file names as desired. Here's an example of how to override the url_to_name function: import webdataset as wds def custom_url_to_name(url): # Custom logic to convert URL to a cache filename return url.replace(\"http://url/dataset-\", \"\").replace(\".tar\", \".cache\") dataset = wds.WebDataset(\"pipe:s3 http://url/dataset-{001..099}.tar\", url_to_name=custom_url_to_name) Issue #211 Q: How can I write to a remote location using ShardWriter? A: ShardWriter is designed to write to local disk for simplicity and reliability, but it provides a hook for uploading data to a remote location. You can define a function that handles the upload process and then pass this function to the post parameter of ShardWriter. Here's a short example of how to use this feature: def upload_shard(fname): os.system(f\"gsutil cp {fname} gs://mybucket\") os.unlink(fname) with ShardWriter(..., post=upload_shard) as writer: # Your code to add data to the writer ... This approach allows you to have control over the upload process and handle any errors that may occur during the transfer to the remote storage. Issue #210 Q: How does the default_collation_fn work in WebDataset when it seems to expect a list or tuple, but the documentation suggests it should handle a collection of samples as dictionaries? A: The confusion arises from the mismatch between the documentation and the actual implementation of default_collation_fn . The function is designed to take a batch of samples and collate them into a single batch for processing. However, the current implementation of default_collation_fn in WebDataset does not handle dictionaries directly. Instead, it expects each sample in the batch to be a list or tuple. If you have a batch of dictionaries, you would need to convert them into a list or tuple format before using default_collation_fn . Alternatively, you can use torch.utils.data.default_collate from PyTorch 1.11 or later, which can handle dictionaries, or you can provide a custom collate function that handles dictionaries. Here's an example of a custom collate function that could handle a list of dictionaries: def custom_collate_fn(batch): # Assuming each element in batch is a dictionary collated_batch = {} for key in batch[0].keys(): collated_batch[key] = [d[key] for d in batch] return collated_batch You can then pass this custom_collate_fn to your data loader. Issue #209 Q: How can I ensure each batch contains only one description per image when using webdatasets? A: To ensure that each batch contains only one description per image in webdatasets, you can create a custom transformation function that acts as a filter or collate function. This function can be composed with your dataset to enforce the batching rule. You can use buffers or other conditional logic within your transformation to manage the batching process. Here's a simple example of how you might start implementing such a transformation: def unique_image_collate(src): buffer = {} for sample in src: image_id = sample['image_id'] if image_id not in buffer: buffer[image_id] = sample if len(buffer) == batch_size: yield list(buffer.values()) buffer.clear() # Additional logic to handle leftovers, etc. if buffer: yield list(buffer.values()) dataset = dataset.compose(unique_image_collate) This function collects samples in a buffer until it has a batch's worth of unique images, then yields that batch and clears the buffer for the next batch. You'll need to add additional logic to handle cases such as the end of an epoch where the buffer may not be full. Issue #201 Q: How can I efficiently subsample a large dataset without slowing down iteration speed? A: When dealing with large datasets, such as LAION 400M, and needing to subsample based on metadata, there are several strategies to maintain high I/O performance. If the subset is small and static, it's best to create a new dataset ahead of time. This can be done using a WebDataset/TarWriter pipeline or with tarp proc ... | tarp split ... commands, potentially parallelizing the process with tools like ray . If dynamic selection is necessary, consider splitting the dataset into shards by the categories of interest. This approach avoids random file accesses, which can significantly slow down data pipelines. Here's a simple example of creating a subset using tarp : tarp proc mydataset.tar -c 'if sample[\"metadata\"] in metadata_list: yield sample' tarp split -o subset-%06d.tar --size=1e9 Remember to perform filtering before any heavy operations like decoding or augmentation to avoid unnecessary processing. Issue #196 Q: How can I speed up subsampling from a tar file when using WebDataset? A: When working with WebDataset, it's important to remember that it is optimized for streaming data and does not support efficient random access within tar files. To speed up subsampling, you should avoid using very small probabilities with rsample as it requires reading the entire stream. Instead, consider using more shards and applying rsample to the shards rather than individual samples. This approach avoids the overhead of sequential reading. Additionally, some storage servers like AIStore can perform server-side sampling, which can be more efficient as they can use random access. # Example of using rsample with shards dataset = WebDataset(\"dataset-{0000..9999}.tar\").rsample(0.1) Issue #194 Q: How should I balance dataset elements across DDP nodes when using WebDataset? A: When using WebDataset with Distributed Data Parallel (DDP) in PyTorch, you may encounter situations where the dataset is not evenly distributed across the workers. To address this, you can use the .repeat() method in combination with .with_epoch() to ensure that each worker processes the same number of batches. The .repeat(2) method is used to repeat the dataset twice, which should be sufficient for most cases. If the dataset is highly unbalanced, you may need to adjust this number. The .with_epoch(n) method is used to limit the number of samples processed in an epoch to n , where n is typically set to the total number of samples divided by the batch size. This combination ensures that each epoch has a consistent size across workers, while also handling any imbalance in the number of shards or samples per worker. Here's an example of how to use these methods: batch_size = 64 epoch_size = 1281237 # Total number of samples in the dataset loader = wds.WebLoader(dataset, num_workers=4) loader = loader.repeat(2).with_epoch(epoch_size // batch_size) This approach allows for a balanced distribution of data across DDP nodes, with the caveat that some batches may be missing or repeated. It's a trade-off between perfect balance and resource usage. Issue #185 Q: How can I include the original file name in the metadata dictionary when iterating through a WebDataset? A: When working with WebDataset, you can include the original file name in the metadata dictionary by defining a function that extracts the __key__ from the sample and adds it to the metadata. You then apply this function using the .map() method in your pipeline. Here's a short example of how to define and use such a function: def add_filename_to_metadata(sample): sample[\"metadata\"][\"filename\"] = sample[\"__key__\"] return sample # Add this to your pipeline after renaming the keys pipeline.append(wds.map(add_filename_to_metadata)) This function should be added to the pipeline after the renaming step to ensure that the metadata key is already present in the sample dictionary. Issue #177 Q: How can I resume training from a specific step without iterating over unused data when using WebDataset? A: When using WebDataset for training with large datasets, it's common to want to resume training from a specific step without loading all the previous data into memory. WebDataset provides a feature for this scenario through shard resampling. By setting resampled=True or using the wds.resampled pipeline stage, you can ensure that you get the same training statistics when restarting your job without the need to skip samples manually. This approach is recommended over trying to implement \"each sample exactly once per epoch,\" which can be complex and environment-dependent. Here's a short example of how you might use the resampled option: from webdataset import WebDataset dataset = WebDataset(urls).resampled(rng=my_random_state) And here's how you might use the wds.resampled pipeline stage: import webdataset as wds dataset = wds.WebDataset(urls).pipe(wds.resampled) Issue #172 Q: Why does the detshuffle epoch count not increment across epochs when using WebDataset? A: The issue with detshuffle not incrementing the epoch count across epochs is likely due to the interaction between the DataLoader's worker process management and the internal state of the detshuffle . When persistent_workers=False , the DataLoader creates new worker processes each epoch, which do not retain the state of the detshuffle instance. This results in the detshuffle epoch count resetting each time. To maintain the state across epochs, you can set persistent_workers=True in the DataLoader. Alternatively, you can manage the epoch count externally and pass it to detshuffle if needed. Here's a short example of how to set persistent_workers : from torch.utils.data import DataLoader # Assuming 'dataset' is your WebDataset instance loader = DataLoader(dataset, persistent_workers=True) If you need to manage the epoch count externally, you could use an environment variable or another mechanism to pass the epoch count to detshuffle . However, this approach is less clean and should be used with caution, as it may introduce complexity and potential bugs into your code. Issue #171 Q: I'm getting an ImportError when trying to import PytorchShardList from webdataset . What should I do? A: The PytorchShardList class has been removed in recent versions of the webdataset package. If you are using version 0.1 of webdataset , PytorchShardList was available, but in later versions, it has likely been replaced with SimpleShardList . To resolve the ImportError, you should update your import statement to use the new class name. Here's how you can import SimpleShardList : from webdataset import SimpleShardList If SimpleShardList does not meet your requirements, you may need to check the documentation for the version of webdataset you are using to find the appropriate replacement or consider downgrading to the version that contains PytorchShardList . Issue #170 Q: How do I use glob patterns with WebDataset to read data from Google Cloud Storage (GCS)? A: WebDataset does not natively support glob patterns due to the lack of a consistent API for globbing across different object stores. To use glob patterns with files stored in GCS, you need to manually resolve the glob pattern using gsutil and then pass the list of shards to WebDataset. Here's an example of how to do this in Python: import os import webdataset as wds # Use gsutil to resolve the glob pattern and get the list of shard URLs shard_list = [shard.strip() for shard in os.popen(\"gsutil ls gs://BUCKET/PATH/training_*.tar\").readlines()] # Create the WebDataset with the resolved list of shard URLs train_data = wds.WebDataset(shard_list, shardshuffle=True, repeat=True) This approach ensures that you get the expected behavior when reading data from shards that match a glob pattern in GCS. Remember to install gsutil and authenticate with GCS before running the code.","title":"WebDataset FAQ"},{"location":"column-store/","text":"Using WebDataset as a Column Store Sometimes it is desirable to break up a dataset not just by rows but also by columns. This is quite easy in WebDataset, although there is no explicit API for it (one will likely be added). The idea is to just use the __url__ field in a sample to load additional columns as necessary. # We usually abbreviate webdataset as wds import webdataset as wds batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\" # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset(training_urls, resampled=True, shardshuffle=True) This function computes the URL for an additional column from a base URL. This is then used by the add_column function to add data from that additional URL to the data already loaded from the base URL. def find_column_url(url): # In this function, given the main URL for a shard, find the corresponding # extra column URL. # For the demo, we just return the same URL, which means that we simply # add the same values to the samples twice. return url # .replace(\"-train\", \"-train-more\") def add_column(src, find_column_url=find_column_url): \"\"\"Given an iterator over a dataset, add an extra column from a separate dataset.\"\"\" last_url = None column_src = None for sample in src: # We use the __url__ field to keep track of which shard we are working on. # We then open the corresponding URL for the extra column data if necessary. if last_url != sample[\"__url__\"]: column_url = find_column_url(sample[\"__url__\"]) print(\"*** opening column_url\", column_url) column_src = iter(wds.WebDataset(column_url, shardshuffle=False)) last_url = sample[\"__url__\"] # Read the next sample from the extra column data. extra = next(column_src) # Check that the keys match. assert extra[\"__key__\"] == sample[\"__key__\"] # Update the sample with the extra data. for k, v in extra.items(): if k[0] != \"_\": sample[k] = v yield sample trainset = trainset.compose(add_column) # NB: any shuffling, decoding, etc. needs to happen after the `add_column` call Let's see all of it in action. Actually, nothing particularly interesting happens here because we are just loading the same data for the base URL and the additional column. Really, the only feedback you get from this code is the message about opening the column_url. for k, v in next(iter(trainset)).items(): print(k, repr(v)[:60]) *** opening column_url https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-001010.tar __key__ '001010-000002' __url__ 'https://storage.googleapis.com/webdataset/fake-imagenet/ima cls b'9' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x txt b'a high quality color photograph of a frog' Some comments: The code above assumes an exact correspondence between the samples in the different columnn shards; this is really what you ought to aim for. But you can add code to skip data. For small amounts of data (like class labels), you probably just want to store the data in a dbm-style database and use .associate(data) . You could also use wids to retrieve additional samples in add_column . If you want to do the same thing in wids , the code becomes even simpler: class CombinedDataset: def __init__(self, ds1, ds2): self.ds1 = wids.ShardListDataset(ds1) self.ds2 = wids.ShardListDataset(ds2) assert len(self.ds1) == len(self.ds2) def getitem(self, index): return self.ds1[index].update(self.ds2[index]) def __len__(self): return len(self.ds1)","title":"Using WebDataset as a Column Store"},{"location":"column-store/#using-webdataset-as-a-column-store","text":"Sometimes it is desirable to break up a dataset not just by rows but also by columns. This is quite easy in WebDataset, although there is no explicit API for it (one will likely be added). The idea is to just use the __url__ field in a sample to load additional columns as necessary. # We usually abbreviate webdataset as wds import webdataset as wds batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\" # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset(training_urls, resampled=True, shardshuffle=True) This function computes the URL for an additional column from a base URL. This is then used by the add_column function to add data from that additional URL to the data already loaded from the base URL. def find_column_url(url): # In this function, given the main URL for a shard, find the corresponding # extra column URL. # For the demo, we just return the same URL, which means that we simply # add the same values to the samples twice. return url # .replace(\"-train\", \"-train-more\") def add_column(src, find_column_url=find_column_url): \"\"\"Given an iterator over a dataset, add an extra column from a separate dataset.\"\"\" last_url = None column_src = None for sample in src: # We use the __url__ field to keep track of which shard we are working on. # We then open the corresponding URL for the extra column data if necessary. if last_url != sample[\"__url__\"]: column_url = find_column_url(sample[\"__url__\"]) print(\"*** opening column_url\", column_url) column_src = iter(wds.WebDataset(column_url, shardshuffle=False)) last_url = sample[\"__url__\"] # Read the next sample from the extra column data. extra = next(column_src) # Check that the keys match. assert extra[\"__key__\"] == sample[\"__key__\"] # Update the sample with the extra data. for k, v in extra.items(): if k[0] != \"_\": sample[k] = v yield sample trainset = trainset.compose(add_column) # NB: any shuffling, decoding, etc. needs to happen after the `add_column` call Let's see all of it in action. Actually, nothing particularly interesting happens here because we are just loading the same data for the base URL and the additional column. Really, the only feedback you get from this code is the message about opening the column_url. for k, v in next(iter(trainset)).items(): print(k, repr(v)[:60]) *** opening column_url https://storage.googleapis.com/webdataset/fake-imagenet/imagenet-train-001010.tar __key__ '001010-000002' __url__ 'https://storage.googleapis.com/webdataset/fake-imagenet/ima cls b'9' jpg b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x txt b'a high quality color photograph of a frog' Some comments: The code above assumes an exact correspondence between the samples in the different columnn shards; this is really what you ought to aim for. But you can add code to skip data. For small amounts of data (like class labels), you probably just want to store the data in a dbm-style database and use .associate(data) . You could also use wids to retrieve additional samples in add_column . If you want to do the same thing in wids , the code becomes even simpler: class CombinedDataset: def __init__(self, ds1, ds2): self.ds1 = wids.ShardListDataset(ds1) self.ds2 = wids.ShardListDataset(ds2) assert len(self.ds1) == len(self.ds2) def getitem(self, index): return self.ds1[index].update(self.ds2[index]) def __len__(self): return len(self.ds1)","title":"Using WebDataset as a Column Store"},{"location":"generate-text-dataset/","text":"Dataset Generation This is a simple example of dataset generation using WebDataset TarWriter . Shard are uploaded to a server or to the cloud as they are generated. Parallel dataset generation with Ray is illustrated at the very end. This particular notebook generates short text samples using GPT-2. These can be used to generate OCR training data. # package installs for colab import sys if \"google.colab\" in sys.modules: !pip install --quiet webdataset !pip install --quiet adapter-transformers !pip install --quiet sentencepiece !pip install --quiet datasets import uuid import webdataset as wds import os from transformers import GPT2LMHeadModel, GPT2Tokenizer from transformers import pipeline import textwrap # Parameters nsamples = 10 ntokens = 100 nshards = 3 # text generation with Huggingface and GPT2 tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\") tokenizer.pad_token = tokenizer.eos_token model = GPT2LMHeadModel.from_pretrained(\"gpt2\") generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) def generate(n, prompt=\"\"): \"\"\"Generate n words of text, starting with prompt.\"\"\" global tokenizer, model, generator output = generator( prompt, max_length=n + len(tokenizer.encode(prompt)), do_sample=True, temperature=0.99, top_k=50, top_p=0.99, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, )[0] return output[\"generated_text\"] text = generate(100).strip() print() print(textwrap.fill(text, 64)) # function generating an entire shard using TarWriter def generate_shard(oname, nsamples=10000, ntokens=500, prefix=\"\"): \"\"\"Generate a shard of samples with text. Each sample has a \"__key__\" field and a \"txt.gz\" field. That is, the individual text files are compressed automatically on write. They will be automatically decompressed when read. \"\"\" with wds.TarWriter(oname) as output: for i in range(nsamples): text = generate(100).strip() key = uuid.uuid4().hex text = generate(ntokens) sample = {\"__key__\": key, \"txt.gz\": text} output.write(sample) if i % 10 == 0: print(f\"{i:6d} {prefix}:\", repr(text)[:60]) generate_shard(\"temp.tar\", nsamples=10, ntokens=10) !ls -l temp.tar !tar tf temp.tar | head -5 # We need a couple of simple functions to upload to the cloud. def cloud_exists(oname): \"\"\"Check whether a file exists in the cloud.\"\"\" # return os.system(f\"gsutil stat gs://mybucket/500tokens/{oname}\") == 0 return True def cloud_upload(oname): \"\"\"Upload a file to the cloud.\"\"\" # assert os.system(f\"gsutil cp {oname} gs://mybucket/500tokens/{oname}\") == 0 pass # We can now generate a shard and upload it to the cloud. # We skip the generation if the file already exists in the cloud. def generate_and_upload(i): \"\"\"Generate a shard and upload it to the cloud.\"\"\" oname = f\"text-{i:06d}.tar\" if cloud_exists(oname): print(f\"{oname} already exists, skipping\") return False generate_shard(oname, nsamples=nsamples, ntokens=ntokens, prefix=f\"{i:6d} {oname}\") cloud_upload(oname) os.remove(oname) return True # For sequential generation, use this for i in range(nshards): generate_and_upload(i) %%script true # For parallel generation, use this import ray @ray.remote(num_cpus=1, num_gpus=1) def ray_generate_and_upload(i): \"\"\"A Ray remote function that generates a shard and uploads it to the cloud.\"\"\" return generate_and_upload(i) def generate_shards(nshards=10): \"\"\"Generate a number of shards and upload them to the cloud. Runs in parallel on a Ray cluster. \"\"\" ray.init(address='auto') # Connect to the Ray cluster tasks = [ray_generate_and_upload.remote(i) for i in range(nshards)] ray.shutdown() return shard_names","title":"Dataset Generation"},{"location":"generate-text-dataset/#dataset-generation","text":"This is a simple example of dataset generation using WebDataset TarWriter . Shard are uploaded to a server or to the cloud as they are generated. Parallel dataset generation with Ray is illustrated at the very end. This particular notebook generates short text samples using GPT-2. These can be used to generate OCR training data. # package installs for colab import sys if \"google.colab\" in sys.modules: !pip install --quiet webdataset !pip install --quiet adapter-transformers !pip install --quiet sentencepiece !pip install --quiet datasets import uuid import webdataset as wds import os from transformers import GPT2LMHeadModel, GPT2Tokenizer from transformers import pipeline import textwrap # Parameters nsamples = 10 ntokens = 100 nshards = 3 # text generation with Huggingface and GPT2 tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\") tokenizer.pad_token = tokenizer.eos_token model = GPT2LMHeadModel.from_pretrained(\"gpt2\") generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) def generate(n, prompt=\"\"): \"\"\"Generate n words of text, starting with prompt.\"\"\" global tokenizer, model, generator output = generator( prompt, max_length=n + len(tokenizer.encode(prompt)), do_sample=True, temperature=0.99, top_k=50, top_p=0.99, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, )[0] return output[\"generated_text\"] text = generate(100).strip() print() print(textwrap.fill(text, 64)) # function generating an entire shard using TarWriter def generate_shard(oname, nsamples=10000, ntokens=500, prefix=\"\"): \"\"\"Generate a shard of samples with text. Each sample has a \"__key__\" field and a \"txt.gz\" field. That is, the individual text files are compressed automatically on write. They will be automatically decompressed when read. \"\"\" with wds.TarWriter(oname) as output: for i in range(nsamples): text = generate(100).strip() key = uuid.uuid4().hex text = generate(ntokens) sample = {\"__key__\": key, \"txt.gz\": text} output.write(sample) if i % 10 == 0: print(f\"{i:6d} {prefix}:\", repr(text)[:60]) generate_shard(\"temp.tar\", nsamples=10, ntokens=10) !ls -l temp.tar !tar tf temp.tar | head -5 # We need a couple of simple functions to upload to the cloud. def cloud_exists(oname): \"\"\"Check whether a file exists in the cloud.\"\"\" # return os.system(f\"gsutil stat gs://mybucket/500tokens/{oname}\") == 0 return True def cloud_upload(oname): \"\"\"Upload a file to the cloud.\"\"\" # assert os.system(f\"gsutil cp {oname} gs://mybucket/500tokens/{oname}\") == 0 pass # We can now generate a shard and upload it to the cloud. # We skip the generation if the file already exists in the cloud. def generate_and_upload(i): \"\"\"Generate a shard and upload it to the cloud.\"\"\" oname = f\"text-{i:06d}.tar\" if cloud_exists(oname): print(f\"{oname} already exists, skipping\") return False generate_shard(oname, nsamples=nsamples, ntokens=ntokens, prefix=f\"{i:6d} {oname}\") cloud_upload(oname) os.remove(oname) return True # For sequential generation, use this for i in range(nshards): generate_and_upload(i) %%script true # For parallel generation, use this import ray @ray.remote(num_cpus=1, num_gpus=1) def ray_generate_and_upload(i): \"\"\"A Ray remote function that generates a shard and uploads it to the cloud.\"\"\" return generate_and_upload(i) def generate_shards(nshards=10): \"\"\"Generate a number of shards and upload them to the cloud. Runs in parallel on a Ray cluster. \"\"\" ray.init(address='auto') # Connect to the Ray cluster tasks = [ray_generate_and_upload.remote(i) for i in range(nshards)] ray.shutdown() return shard_names","title":"Dataset Generation"},{"location":"webdataset/","text":"WebDataset API Fluid Interfaces The FluidInterface class provides a way to create fluent interfaces for chaining operations on datasets. Most operations are contained in the FluidInterface mixin class. with_epoch sets the epoch size (number of samples per epoch), effectively an itertools.islice over the dataset. webdataset.WebDataset Bases: DataPipeline , FluidInterface Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Parameters: urls \u2013 The source URLs or specifications for the dataset. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. mode \u2013 The mode of operation. Defaults to None. resampled \u2013 Whether to use resampled mode. Defaults to False. repeat \u2013 Whether to repeat the dataset. Defaults to False. shardshuffle \u2013 The number of shards to shuffle, or None. Defaults to None. cache_size \u2013 The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir \u2013 The directory to use for caching. Defaults to None. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle \u2013 Whether to use deterministic shuffling. Defaults to False. nodesplitter \u2013 Function to split data by node. Defaults to single_node_only. workersplitter \u2013 Function to split data by worker. Defaults to split_by_worker. select_files \u2013 Function to select files from tar archives. Defaults to None. rename_files \u2013 Function to rename files from tar archives. Defaults to None. empty_check \u2013 Whether to check for empty datasets. Defaults to True. verbose \u2013 Whether to print verbose output. Defaults to False. seed \u2013 Random seed for shuffling. Defaults to None. Raises: ValueError \u2013 If the cache directory does not exist or if the URL type is not supported. Source code in webdataset/compat.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 class WebDataset ( DataPipeline , FluidInterface ): \"\"\"Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Args: urls: The source URLs or specifications for the dataset. handler: Function to handle exceptions. Defaults to reraise_exception. mode: The mode of operation. Defaults to None. resampled: Whether to use resampled mode. Defaults to False. repeat: Whether to repeat the dataset. Defaults to False. shardshuffle: The number of shards to shuffle, or None. Defaults to None. cache_size: The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir: The directory to use for caching. Defaults to None. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle: Whether to use deterministic shuffling. Defaults to False. nodesplitter: Function to split data by node. Defaults to single_node_only. workersplitter: Function to split data by worker. Defaults to split_by_worker. select_files: Function to select files from tar archives. Defaults to None. rename_files: Function to rename files from tar archives. Defaults to None. empty_check: Whether to check for empty datasets. Defaults to True. verbose: Whether to print verbose output. Defaults to False. seed: Random seed for shuffling. Defaults to None. Raises: ValueError: If the cache directory does not exist or if the URL type is not supported. \"\"\" def __init__ ( self , urls , handler = reraise_exception , mode = None , resampled = False , repeat = False , shardshuffle = None , cache_size =- 1 , cache_dir = None , url_to_name = cache . pipe_cleaner , detshuffle = False , nodesplitter = shardlists . single_node_only , workersplitter = shardlists . split_by_worker , select_files = None , rename_files = None , empty_check = True , verbose = False , seed = None , ): super () . __init__ () if resampled : mode = \"resampled\" if mode == \"resampled\" and shardshuffle not in ( False , None ): warnings . warn ( \"WebDataset(shardshuffle=...) is ignored for resampled datasets\" ) elif shardshuffle is None : warnings . warn ( \"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\" ) if shardshuffle is True : warnings . warn ( \"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\" ) shardshuffle = 100 args = SimpleNamespace ( ** locals ()) self . seed = ( os . environ . get ( \"WDS_SEED\" , random . randint ( 0 , 1000000 )) if seed is None else seed ) self . update_cache_info ( args ) # first, we add a generator for the urls to used # this generates a stream of dict(url=...) self . create_url_iterator ( args ) # split by node (for distributed processing) if nodesplitter is not None : self . append ( nodesplitter ) # split by worker (for DataLoader) if workersplitter : self . append ( workersplitter ) # add a shard shuffler if args . shardshuffle is not None : if args . detshuffle : self . append ( filters . detshuffle ( args . shardshuffle , seed = self . seed )) else : self . append ( filters . shuffle ( args . shardshuffle , seed = self . seed )) # next, we select a URL opener, either with or without caching # this generates a stream of dict(url=..., stream=...) if cache_dir is None or cache_size == 0 : opener = cache . StreamingOpen ( handler = handler ) else : opener = cache . FileCache ( cache_dir = cache_dir , cache_size = cache_size , handler = handler ) self . append ( opener ) # now we need to open each stream and read the tar files contained in it # this generates a stream of dict(fname=..., data=...) objects expander = pipelinefilter ( tar_file_expander ) self . append ( expander ( handler = handler , select_files = select_files , rename_files = rename_files ) ) # finally, the files need to be groups into samples # this generates a stream of dict(__key__=..., ...=...) objects grouper = pipelinefilter ( group_by_keys ) self . append ( grouper ( handler = handler )) # check for empty datasets if empty_check : self . append ( check_empty ) def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" ) def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardList ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" ) def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close () __enter__ () Enter the runtime context for the WebDataset. Returns: self \u2013 The WebDataset instance. Source code in webdataset/compat.py 515 516 517 518 519 520 521 def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self __exit__ ( * args ) Exit the runtime context for the WebDataset. Parameters: *args \u2013 Exception type, value, and traceback if an exception occurred. Source code in webdataset/compat.py 523 524 525 526 527 528 529 def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close () create_url_iterator ( args ) Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the URL type is not supported or implemented. Source code in webdataset/compat.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardList ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" ) update_cache_info ( args ) Update cache information based on arguments and environment variables. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the specified cache directory does not exist. Source code in webdataset/compat.py 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" ) webdataset.WebLoader Bases: DataPipeline , FluidInterface A wrapper for DataLoader that adds a fluid interface. Source code in webdataset/compat.py 540 541 542 543 544 class WebLoader ( DataPipeline , FluidInterface ): \"\"\"A wrapper for DataLoader that adds a fluid interface.\"\"\" def __init__ ( self , * args , ** kw ): super () . __init__ ( DataLoader ( * args , ** kw )) webdataset.FluidInterface Source code in webdataset/compat.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 class FluidInterface : def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) ) def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ()) def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None )) def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ()) def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile )) def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw )) def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler )) def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler ) def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw )) def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw )) def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw )) def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler )) def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args )) def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw )) def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p )) def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw )) def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw )) def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw )) def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ()) def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw )) batched ( batchsize , collation_fn = filters . default_collation_fn , partial = True ) Create batches of the given size. This method forwards to the filters.batched function. Parameters: batchsize ( int ) \u2013 Target batch size. collation_fn ( callable , default: default_collation_fn ) \u2013 Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial ( bool , default: True ) \u2013 Whether to return partial batches. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with batched filter. Source code in webdataset/compat.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) ) decode ( * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception ) Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Parameters: *args \u2013 Decoding functions or strings representing image handlers. pre ( callable , default: None ) \u2013 Pre-processing function. post ( callable , default: None ) \u2013 Post-processing function. only ( list , default: None ) \u2013 List of keys to decode. partial ( bool , default: False ) \u2013 Whether to allow partial decoding. Defaults to False. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with decode filter. Source code in webdataset/compat.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler ) extract_keys ( * args , ** kw ) Extract specific keys from samples. This method forwards to the filters.extract_keys function. Parameters: *args \u2013 Keys or patterns to extract. **kw \u2013 Additional keyword arguments for filters.extract_keys. Returns: FluidInterface \u2013 Updated pipeline with extract_keys filter. Source code in webdataset/compat.py 256 257 258 259 260 261 262 263 264 265 266 267 268 def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw )) listed ( batchsize , partial = True ) Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Parameters: batchsize ( int ) \u2013 Target list size. partial ( bool , default: True ) \u2013 Whether to return partial lists. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with listed filter. Source code in webdataset/compat.py 47 48 49 50 51 52 53 54 55 56 57 58 59 def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None )) lmdb_cached ( * args , ** kw ) Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Parameters: *args \u2013 Positional arguments for filters.LMDBCached. **kw \u2013 Keyword arguments for filters.LMDBCached. Returns: FluidInterface \u2013 Updated pipeline with LMDB caching. Source code in webdataset/compat.py 294 295 296 297 298 299 300 301 302 303 304 305 306 def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw )) log_keys ( logfile = None ) Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Parameters: logfile ( str , default: None ) \u2013 Path to the log file. If None, logging is disabled. Returns: FluidInterface \u2013 Updated pipeline with log_keys filter. Source code in webdataset/compat.py 71 72 73 74 75 76 77 78 79 80 81 82 def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile )) map ( f , handler = reraise_exception ) Apply a function to each sample in the stream. This method forwards to the filters.map function. Parameters: f ( callable ) \u2013 Function to apply to each sample. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map filter. Source code in webdataset/compat.py 101 102 103 104 105 106 107 108 109 110 111 112 113 def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler )) map_dict ( handler = reraise_exception , ** kw ) Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Parameters: handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. **kw \u2013 Mapping of keys to functions to apply. Returns: FluidInterface \u2013 Updated pipeline with map_dict filter. Source code in webdataset/compat.py 147 148 149 150 151 152 153 154 155 156 157 158 159 def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw )) map_tuple ( * args , handler = reraise_exception ) Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Parameters: *args \u2013 Functions to apply to each element of the tuple. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map_tuple filter. Source code in webdataset/compat.py 189 190 191 192 193 194 195 196 197 198 199 200 201 def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler )) mcached () Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface \u2013 Updated pipeline with memory caching. Source code in webdataset/compat.py 284 285 286 287 288 289 290 291 292 def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ()) rename ( ** kw ) Rename samples based on keyword arguments. This method forwards to the filters.rename function. Parameters: **kw \u2013 Mapping of old names to new names. Returns: FluidInterface \u2013 Updated pipeline with rename filter. Source code in webdataset/compat.py 216 217 218 219 220 221 222 223 224 225 226 227 def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw )) rename_keys ( * args , ** kw ) Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Parameters: *args \u2013 Positional arguments for filters.rename_keys. **kw \u2013 Keyword arguments for filters.rename_keys. Returns: FluidInterface \u2013 Updated pipeline with rename_keys filter. Source code in webdataset/compat.py 242 243 244 245 246 247 248 249 250 251 252 253 254 def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw )) rsample ( p = 0.5 ) Randomly subsample a stream of data. This method forwards to the filters.rsample function. Parameters: p ( float , default: 0.5 ) \u2013 Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface \u2013 Updated pipeline with rsample filter. Source code in webdataset/compat.py 229 230 231 232 233 234 235 236 237 238 239 240 def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p )) select ( predicate , ** kw ) Select samples based on a predicate. This method forwards to the filters.select function. Parameters: predicate ( callable ) \u2013 Function that returns True for samples to keep. **kw \u2013 Additional keyword arguments for filters.select. Returns: FluidInterface \u2013 Updated pipeline with select filter. Source code in webdataset/compat.py 161 162 163 164 165 166 167 168 169 170 171 172 173 def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw )) shuffle ( size , ** kw ) Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Parameters: size ( int ) \u2013 Buffer size for shuffling. **kw \u2013 Additional keyword arguments for filters.shuffle. Returns: FluidInterface \u2013 Updated pipeline with shuffle filter, or self if size < 1. Source code in webdataset/compat.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw )) slice ( * args ) Slice the data stream. This method forwards to the filters.slice function. Parameters: *args \u2013 Arguments for slicing (start, stop, step). Returns: FluidInterface \u2013 Updated pipeline with slice filter. Source code in webdataset/compat.py 203 204 205 206 207 208 209 210 211 212 213 214 def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args )) to_tuple ( * args , ** kw ) Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Parameters: *args \u2013 Keys to extract from the dict. **kw \u2013 Additional keyword arguments for filters.to_tuple. Returns: FluidInterface \u2013 Updated pipeline with to_tuple filter. Source code in webdataset/compat.py 175 176 177 178 179 180 181 182 183 184 185 186 187 def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw )) unbatched () Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface \u2013 Updated pipeline with unbatched filter. Source code in webdataset/compat.py 37 38 39 40 41 42 43 44 45 def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ()) unlisted () Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface \u2013 Updated pipeline with unlisted filter. Source code in webdataset/compat.py 61 62 63 64 65 66 67 68 69 def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ()) xdecode ( * args , ** kw ) Decode data based on file extensions. This method forwards to the filters.xdecode function. Parameters: *args \u2013 Positional arguments for filters.xdecode. **kw \u2013 Keyword arguments for filters.xdecode. Returns: FluidInterface \u2013 Updated pipeline with xdecode filter. Source code in webdataset/compat.py 270 271 272 273 274 275 276 277 278 279 280 281 282 def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw )) webdataset.with_epoch Bases: IterableDataset Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Parameters: dataset \u2013 The source IterableDataset. length ( int ) \u2013 Declared length of the dataset. Source code in webdataset/extradatasets.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class with_epoch ( IterableDataset ): \"\"\"Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Args: dataset: The source IterableDataset. length (int): Declared length of the dataset. \"\"\" def __init__ ( self , dataset , length ): super () . __init__ () self . length = length self . source = None def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None __getstate__ () Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict \u2013 A dictionary representing the pickled state of the dataset. Source code in webdataset/extradatasets.py 91 92 93 94 95 96 97 98 99 100 101 def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result invoke ( dataset ) Return an iterator over the dataset. This iterator returns as many samples as given by the length parameter. Parameters: dataset \u2013 The source dataset to iterate over. Yields: Sample \u2013 The next sample from the dataset. Source code in webdataset/extradatasets.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None Writing WebDatasets webdataset.ShardWriter Like TarWriter but splits into multiple shards. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. Defaults to 100000. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. Defaults to 3e9. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. Defaults to None. start_shard ( int , default: 0 ) \u2013 Starting shard number. Defaults to 0. verbose ( int , default: 1 ) \u2013 Verbosity level. Defaults to 1. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. Defaults to None. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 class ShardWriter : \"\"\"Like TarWriter but splits into multiple shards. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. Defaults to 100000. maxsize: Maximum size of each shard. Defaults to 3e9. post: Optional callable to be executed after each shard is written. Defaults to None. start_shard: Starting shard number. Defaults to 0. verbose: Verbosity level. Defaults to 1. opener: Optional callable to open output files. Defaults to None. **kw: Other options passed to TarWriter. \"\"\" def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream () def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( self . opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close () __enter__ () Enter context. Returns: self \u2013 The ShardWriter object. Source code in webdataset/writer.py 609 610 611 612 613 614 615 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self __exit__ ( * args , ** kw ) Exit context. Source code in webdataset/writer.py 617 618 619 def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close () __init__ ( pattern , maxcount = 100000 , maxsize = 3000000000.0 , post = None , start_shard = 0 , verbose = 1 , opener = None , ** kw ) Create a ShardWriter. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. start_shard ( int , default: 0 ) \u2013 Starting shard number. verbose ( int , default: 1 ) \u2013 Verbosity level. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream () close () Close the stream. Source code in webdataset/writer.py 601 602 603 604 605 606 607 def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size finish () Finish all writing (use close instead). Source code in webdataset/writer.py 592 593 594 595 596 597 598 599 def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None next_stream () Close the current stream and move to the next. Source code in webdataset/writer.py 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( self . opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0 write ( obj ) Write a sample. Parameters: obj \u2013 Sample to be written. Source code in webdataset/writer.py 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size webdataset.TarWriter A class for writing dictionaries to tar files. Parameters: fileobj \u2013 File name for tar file (.tgz/.tar) or open file descriptor. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Sample encoding. Defaults to True. compress ( Optional [ Union [ bool , str ]] , default: None ) \u2013 Compression flag. Defaults to None. user ( str , default: 'bigdata' ) \u2013 User for tar files. Defaults to \"bigdata\". group ( str , default: 'bigdata' ) \u2013 Group for tar files. Defaults to \"bigdata\". mode ( int , default: 292 ) \u2013 Mode for tar files. Defaults to 0o0444. keep_meta ( bool , default: False ) \u2013 Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime ( Optional [ float ] , default: None ) \u2013 Modification time. Defaults to None. format ( Any , default: None ) \u2013 Tar format. Defaults to None. Returns: \u2013 TarWriter object. Raises: ValueError \u2013 If the encoder doesn't yield bytes for a key. True will use an encoder that behaves similar to the automatic decoder for Dataset . False disables encoding and expects byte strings (except for metadata, which must be strings). The encoder argument can also be a callable , or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: a/b.png and a/b.output.png . tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) Source code in webdataset/writer.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 class TarWriter : \"\"\"A class for writing dictionaries to tar files. Args: fileobj: File name for tar file (.tgz/.tar) or open file descriptor. encoder: Sample encoding. Defaults to True. compress: Compression flag. Defaults to None. user: User for tar files. Defaults to \"bigdata\". group: Group for tar files. Defaults to \"bigdata\". mode: Mode for tar files. Defaults to 0o0444. keep_meta: Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime: Modification time. Defaults to None. format: Tar format. Defaults to None. Returns: TarWriter object. Raises: ValueError: If the encoder doesn't yield bytes for a key. `True` will use an encoder that behaves similar to the automatic decoder for `Dataset`. `False` disables encoding and expects byte strings (except for metadata, which must be strings). The `encoder` argument can also be a `callable`, or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: `a/b.png` and `a/b.output.png`. tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) \"\"\" def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ Union [ bool , str ]] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime tarmode = self . tarmode ( fileobj , compress ) if isinstance ( fileobj , str ): fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close () def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total @staticmethod def tarmode ( fileobj , compress : Optional [ Union [ bool , str ]] = None ): if compress is False : return \"w|\" elif ( compress is True or compress == \"gz\" or ( isinstance ( fileobj , str ) and fileobj . endswith ( \"gz\" )) ): return \"w|gz\" elif compress == \"bz2\" or ( isinstance ( fileobj , str ) and fileobj . endswith ( \"bz2\" ) ): return \"w|bz2\" elif compress == \"xz\" or ( isinstance ( fileobj , str ) and fileobj . endswith ( \"xz\" )): return \"w|xz\" else : return \"w|\" __enter__ () Enter context. Returns: self \u2013 The TarWriter object. Source code in webdataset/writer.py 416 417 418 419 420 421 422 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self __exit__ ( exc_type , exc_val , exc_tb ) Exit context. Source code in webdataset/writer.py 424 425 426 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close () __init__ ( fileobj , user = 'bigdata' , group = 'bigdata' , mode = 292 , compress = None , encoder = True , keep_meta = False , mtime = None , format = None ) Create a tar writer. Parameters: fileobj \u2013 Stream to write data to. user ( str , default: 'bigdata' ) \u2013 User for tar files. group ( str , default: 'bigdata' ) \u2013 Group for tar files. mode ( int , default: 292 ) \u2013 Mode for tar files. compress ( Optional [ Union [ bool , str ]] , default: None ) \u2013 Desired compression. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Encoder function. keep_meta ( bool , default: False ) \u2013 Keep metadata (entries starting with \"_\"). mtime ( Optional [ float ] , default: None ) \u2013 Modification time (set this to some fixed value to get reproducible tar files). format ( Any , default: None ) \u2013 Tar format. Source code in webdataset/writer.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ Union [ bool , str ]] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime tarmode = self . tarmode ( fileobj , compress ) if isinstance ( fileobj , str ): fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress close () Close the tar file. Source code in webdataset/writer.py 428 429 430 431 432 433 def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None write ( obj ) Write a dictionary to the tar file. Parameters: obj \u2013 Dictionary of objects to be stored. Returns: int \u2013 Size of the entry. Raises: ValueError \u2013 If the object doesn't contain a key or if a key doesn't map to bytes after encoding. Source code in webdataset/writer.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total Low Level I/O webdataset . gopen . gopen ( url , mode = 'rb' , bufsize = 8192 , ** kw ) Open the URL using various schemes and protocols. This function provides a unified interface for opening resources specified by URLs, supporting multiple schemes and protocols. It uses the gopen_schemes dispatch table to handle different URL schemes. Built-in support is provided for the following schemes: - pipe: for opening named pipes - file: for local file system access - http, https: for web resources - sftp, ftps: for secure file transfer - scp: for secure copy protocol When no scheme is specified in the URL, it is treated as a local file path. Environment Variables: - GOPEN_VERBOSE: Set to a non-zero value to enable verbose logging of file operations. Format: GOPEN_VERBOSE=1 - USE_AIS_FOR: Specifies which cloud storage services should use AIS (and its cache) for access. Format: USE_AIS_FOR=aws:gs:s3 - GOPEN_BUFFER: Sets the buffer size for file operations (in bytes). Format: GOPEN_BUFFER=8192 Parameters: url ( str ) \u2013 The source URL or file path to open. mode ( str , default: 'rb' ) \u2013 The mode for opening the resource. Only \"rb\" (read binary) and \"wb\" (write binary) are supported. bufsize ( int , default: 8192 ) \u2013 The buffer size for file operations. Default is 8192 bytes. **kw \u2013 Additional keyword arguments to pass to the underlying open function. Returns: \u2013 file-like object: An opened file-like object for the specified resource. Raises: ValueError \u2013 If an unsupported mode is specified. Note: - For stdin/stdout operations, use \"-\" as the URL. - The function applies URL rewriting based on the GOPEN_REWRITE environment variable before processing. Source code in webdataset/gopen.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def gopen ( url , mode = \"rb\" , bufsize = 8192 , ** kw ): \"\"\"Open the URL using various schemes and protocols. This function provides a unified interface for opening resources specified by URLs, supporting multiple schemes and protocols. It uses the `gopen_schemes` dispatch table to handle different URL schemes. Built-in support is provided for the following schemes: - pipe: for opening named pipes - file: for local file system access - http, https: for web resources - sftp, ftps: for secure file transfer - scp: for secure copy protocol When no scheme is specified in the URL, it is treated as a local file path. Environment Variables: - GOPEN_VERBOSE: Set to a non-zero value to enable verbose logging of file operations. Format: GOPEN_VERBOSE=1 - USE_AIS_FOR: Specifies which cloud storage services should use AIS (and its cache) for access. Format: USE_AIS_FOR=aws:gs:s3 - GOPEN_BUFFER: Sets the buffer size for file operations (in bytes). Format: GOPEN_BUFFER=8192 Args: url (str): The source URL or file path to open. mode (str): The mode for opening the resource. Only \"rb\" (read binary) and \"wb\" (write binary) are supported. bufsize (int): The buffer size for file operations. Default is 8192 bytes. **kw: Additional keyword arguments to pass to the underlying open function. Returns: file-like object: An opened file-like object for the specified resource. Raises: ValueError: If an unsupported mode is specified. Other exceptions may be raised depending on the specific handler used for the URL scheme. Note: - For stdin/stdout operations, use \"-\" as the URL. - The function applies URL rewriting based on the GOPEN_REWRITE environment variable before processing. \"\"\" global fallback_gopen verbose = int ( os . environ . get ( \"GOPEN_VERBOSE\" , 0 )) if verbose : print ( \"GOPEN\" , url , info , file = sys . stderr ) assert mode in [ \"rb\" , \"wb\" ], mode if url == \"-\" : if mode == \"rb\" : return sys . stdin . buffer elif mode == \"wb\" : return sys . stdout . buffer else : raise ValueError ( f \"unknown mode { mode } \" ) url = rewrite_url ( url ) pr = urlparse ( url ) if pr . scheme == \"\" : bufsize = int ( os . environ . get ( \"GOPEN_BUFFER\" , - 1 )) return open ( url , mode , buffering = bufsize ) if pr . scheme == \"file\" : bufsize = int ( os . environ . get ( \"GOPEN_BUFFER\" , - 1 )) return open ( url2pathname ( pr . path ), mode , buffering = bufsize ) handler = gopen_schemes [ \"__default__\" ] handler = gopen_schemes . get ( pr . scheme , handler ) return handler ( url , mode , bufsize , ** kw ) Error Handling webdataset . ignore_and_continue ( exn ) Ignore the exception and continue processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 34 35 36 37 38 39 40 41 42 43 def ignore_and_continue ( exn ): \"\"\"Ignore the exception and continue processing. Args: exn: The exception to be ignored. Returns: bool: Always returns True to indicate continuation. \"\"\" return True webdataset . ignore_and_stop ( exn ) Ignore the exception and stop further processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 60 61 62 63 64 65 66 67 68 69 def ignore_and_stop ( exn ): \"\"\"Ignore the exception and stop further processing. Args: exn: The exception to be ignored. Returns: bool: Always returns False to indicate stopping. \"\"\" return False webdataset . reraise_exception ( exn ) Re-raise the given exception. Parameters: exn \u2013 The exception to be re-raised. Source code in webdataset/handlers.py 22 23 24 25 26 27 28 29 30 31 def reraise_exception ( exn ): \"\"\"Re-raise the given exception. Args: exn: The exception to be re-raised. Raises: The input exception. \"\"\" raise exn webdataset . warn_and_continue ( exn ) Issue a warning for the exception and continue processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 46 47 48 49 50 51 52 53 54 55 56 57 def warn_and_continue ( exn ): \"\"\"Issue a warning for the exception and continue processing. Args: exn: The exception to be warned about. Returns: bool: Always returns True to indicate continuation. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return True webdataset . warn_and_stop ( exn ) Issue a warning for the exception and stop further processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 72 73 74 75 76 77 78 79 80 81 82 83 def warn_and_stop ( exn ): \"\"\"Issue a warning for the exception and stop further processing. Args: exn: The exception to be warned about. Returns: bool: Always returns False to indicate stopping. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return False","title":"WebDataset"},{"location":"webdataset/#webdataset-api","text":"","title":"WebDataset API"},{"location":"webdataset/#fluid-interfaces","text":"The FluidInterface class provides a way to create fluent interfaces for chaining operations on datasets. Most operations are contained in the FluidInterface mixin class. with_epoch sets the epoch size (number of samples per epoch), effectively an itertools.islice over the dataset.","title":"Fluid Interfaces"},{"location":"webdataset/#webdataset.WebDataset","text":"Bases: DataPipeline , FluidInterface Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Parameters: urls \u2013 The source URLs or specifications for the dataset. handler \u2013 Function to handle exceptions. Defaults to reraise_exception. mode \u2013 The mode of operation. Defaults to None. resampled \u2013 Whether to use resampled mode. Defaults to False. repeat \u2013 Whether to repeat the dataset. Defaults to False. shardshuffle \u2013 The number of shards to shuffle, or None. Defaults to None. cache_size \u2013 The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir \u2013 The directory to use for caching. Defaults to None. url_to_name \u2013 Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle \u2013 Whether to use deterministic shuffling. Defaults to False. nodesplitter \u2013 Function to split data by node. Defaults to single_node_only. workersplitter \u2013 Function to split data by worker. Defaults to split_by_worker. select_files \u2013 Function to select files from tar archives. Defaults to None. rename_files \u2013 Function to rename files from tar archives. Defaults to None. empty_check \u2013 Whether to check for empty datasets. Defaults to True. verbose \u2013 Whether to print verbose output. Defaults to False. seed \u2013 Random seed for shuffling. Defaults to None. Raises: ValueError \u2013 If the cache directory does not exist or if the URL type is not supported. Source code in webdataset/compat.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 class WebDataset ( DataPipeline , FluidInterface ): \"\"\"Create a WebDataset pipeline for efficient data loading. This class sets up a data pipeline for loading and processing WebDataset-format data. It handles URL generation, shard shuffling, caching, and sample grouping. Args: urls: The source URLs or specifications for the dataset. handler: Function to handle exceptions. Defaults to reraise_exception. mode: The mode of operation. Defaults to None. resampled: Whether to use resampled mode. Defaults to False. repeat: Whether to repeat the dataset. Defaults to False. shardshuffle: The number of shards to shuffle, or None. Defaults to None. cache_size: The size of the cache in bytes. Defaults to -1 (unlimited). cache_dir: The directory to use for caching. Defaults to None. url_to_name: Function to convert URLs to cache names. Defaults to pipe_cleaner. detshuffle: Whether to use deterministic shuffling. Defaults to False. nodesplitter: Function to split data by node. Defaults to single_node_only. workersplitter: Function to split data by worker. Defaults to split_by_worker. select_files: Function to select files from tar archives. Defaults to None. rename_files: Function to rename files from tar archives. Defaults to None. empty_check: Whether to check for empty datasets. Defaults to True. verbose: Whether to print verbose output. Defaults to False. seed: Random seed for shuffling. Defaults to None. Raises: ValueError: If the cache directory does not exist or if the URL type is not supported. \"\"\" def __init__ ( self , urls , handler = reraise_exception , mode = None , resampled = False , repeat = False , shardshuffle = None , cache_size =- 1 , cache_dir = None , url_to_name = cache . pipe_cleaner , detshuffle = False , nodesplitter = shardlists . single_node_only , workersplitter = shardlists . split_by_worker , select_files = None , rename_files = None , empty_check = True , verbose = False , seed = None , ): super () . __init__ () if resampled : mode = \"resampled\" if mode == \"resampled\" and shardshuffle not in ( False , None ): warnings . warn ( \"WebDataset(shardshuffle=...) is ignored for resampled datasets\" ) elif shardshuffle is None : warnings . warn ( \"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\" ) if shardshuffle is True : warnings . warn ( \"set WebDataset(shardshuffle=...) to a positive integer or 0 or False\" ) shardshuffle = 100 args = SimpleNamespace ( ** locals ()) self . seed = ( os . environ . get ( \"WDS_SEED\" , random . randint ( 0 , 1000000 )) if seed is None else seed ) self . update_cache_info ( args ) # first, we add a generator for the urls to used # this generates a stream of dict(url=...) self . create_url_iterator ( args ) # split by node (for distributed processing) if nodesplitter is not None : self . append ( nodesplitter ) # split by worker (for DataLoader) if workersplitter : self . append ( workersplitter ) # add a shard shuffler if args . shardshuffle is not None : if args . detshuffle : self . append ( filters . detshuffle ( args . shardshuffle , seed = self . seed )) else : self . append ( filters . shuffle ( args . shardshuffle , seed = self . seed )) # next, we select a URL opener, either with or without caching # this generates a stream of dict(url=..., stream=...) if cache_dir is None or cache_size == 0 : opener = cache . StreamingOpen ( handler = handler ) else : opener = cache . FileCache ( cache_dir = cache_dir , cache_size = cache_size , handler = handler ) self . append ( opener ) # now we need to open each stream and read the tar files contained in it # this generates a stream of dict(fname=..., data=...) objects expander = pipelinefilter ( tar_file_expander ) self . append ( expander ( handler = handler , select_files = select_files , rename_files = rename_files ) ) # finally, the files need to be groups into samples # this generates a stream of dict(__key__=..., ...=...) objects grouper = pipelinefilter ( group_by_keys ) self . append ( grouper ( handler = handler )) # check for empty datasets if empty_check : self . append ( check_empty ) def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" ) def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardList ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" ) def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close ()","title":"WebDataset"},{"location":"webdataset/#webdataset.WebDataset.__enter__","text":"Enter the runtime context for the WebDataset. Returns: self \u2013 The WebDataset instance. Source code in webdataset/compat.py 515 516 517 518 519 520 521 def __enter__ ( self ): \"\"\"Enter the runtime context for the WebDataset. Returns: self: The WebDataset instance. \"\"\" return self","title":"__enter__"},{"location":"webdataset/#webdataset.WebDataset.__exit__","text":"Exit the runtime context for the WebDataset. Parameters: *args \u2013 Exception type, value, and traceback if an exception occurred. Source code in webdataset/compat.py 523 524 525 526 527 528 529 def __exit__ ( self , * args ): \"\"\"Exit the runtime context for the WebDataset. Args: *args: Exception type, value, and traceback if an exception occurred. \"\"\" self . close ()","title":"__exit__"},{"location":"webdataset/#webdataset.WebDataset.create_url_iterator","text":"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the URL type is not supported or implemented. Source code in webdataset/compat.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 def create_url_iterator ( self , args ): \"\"\"Create an appropriate URL iterator based on the input type. This method determines the type of URL input and creates the corresponding iterator for the dataset. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the URL type is not supported or implemented. \"\"\" urls = args . urls # .yaml specification files if isinstance ( urls , str ) and ( urls . endswith ( \".yaml\" ) or urls . endswith ( \".yml\" )): with open ( args . urls ) as stream : spec = yaml . safe_load ( stream ) assert \"datasets\" in spec self . append ( shardlists . MultiShardSample ( spec )) return # .yaml specifications already loaded as dictionaries if isinstance ( args . urls , dict ): assert \"datasets\" in args . urls self . append ( shardlists . MultiShardSample ( args . urls )) return # .json specification files (from wids) if isinstance ( urls , str ) and urls . endswith ( \".json\" ): raise ValueError ( \"unimplemented\" ) # any URL ending in \"/\" is assumed to be a directory if isinstance ( urls , str ) and urlparse ( urls ) . path . endswith ( \"/\" ): self . append ( shardlists . DirectoryShardList ( urls , mode = args . mode )) return # the rest is either a shard list or a resampled shard list if isinstance ( args . urls , str ) or utils . is_iterable ( args . urls ): if args . mode == \"resampled\" : self . append ( shardlists . ResampledShardList ( args . urls )) else : self . append ( shardlists . SimpleShardList ( args . urls )) return raise ValueError ( f \"cannot handle urls of type { type ( args . urls ) } \" )","title":"create_url_iterator"},{"location":"webdataset/#webdataset.WebDataset.update_cache_info","text":"Update cache information based on arguments and environment variables. Parameters: args \u2013 A SimpleNamespace object containing the arguments. Raises: ValueError \u2013 If the specified cache directory does not exist. Source code in webdataset/compat.py 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 def update_cache_info ( self , args ): \"\"\"Update cache information based on arguments and environment variables. Args: args: A SimpleNamespace object containing the arguments. Raises: ValueError: If the specified cache directory does not exist. \"\"\" args . cache_size = int ( os . environ . get ( \"WDS_CACHE_SIZE\" , args . cache_size )) args . cache_dir = os . environ . get ( \"WDS_CACHE\" , args . cache_dir ) if args . cache_dir is not None : args . cache_dir = os . path . expanduser ( args . cache_dir ) if not os . path . exists ( args . cache_dir ): raise ValueError ( f \"cache directory { args . cache_dir } does not exist\" )","title":"update_cache_info"},{"location":"webdataset/#webdataset.WebLoader","text":"Bases: DataPipeline , FluidInterface A wrapper for DataLoader that adds a fluid interface. Source code in webdataset/compat.py 540 541 542 543 544 class WebLoader ( DataPipeline , FluidInterface ): \"\"\"A wrapper for DataLoader that adds a fluid interface.\"\"\" def __init__ ( self , * args , ** kw ): super () . __init__ ( DataLoader ( * args , ** kw ))","title":"WebLoader"},{"location":"webdataset/#webdataset.FluidInterface","text":"Source code in webdataset/compat.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 class FluidInterface : def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) ) def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ()) def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None )) def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ()) def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile )) def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw )) def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler )) def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler ) def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw )) def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw )) def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw )) def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler )) def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args )) def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw )) def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p )) def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw )) def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw )) def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw )) def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ()) def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw ))","title":"FluidInterface"},{"location":"webdataset/#webdataset.FluidInterface.batched","text":"Create batches of the given size. This method forwards to the filters.batched function. Parameters: batchsize ( int ) \u2013 Target batch size. collation_fn ( callable , default: default_collation_fn ) \u2013 Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial ( bool , default: True ) \u2013 Whether to return partial batches. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with batched filter. Source code in webdataset/compat.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def batched ( self , batchsize , collation_fn = filters . default_collation_fn , partial = True ): \"\"\"Create batches of the given size. This method forwards to the filters.batched function. Args: batchsize (int): Target batch size. collation_fn (callable, optional): Function to collate samples into a batch. Defaults to filters.default_collation_fn. partial (bool, optional): Whether to return partial batches. Defaults to True. Returns: FluidInterface: Updated pipeline with batched filter. \"\"\" return self . compose ( filters . batched ( batchsize , collation_fn = collation_fn , partial = partial ) )","title":"batched"},{"location":"webdataset/#webdataset.FluidInterface.decode","text":"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Parameters: *args \u2013 Decoding functions or strings representing image handlers. pre ( callable , default: None ) \u2013 Pre-processing function. post ( callable , default: None ) \u2013 Post-processing function. only ( list , default: None ) \u2013 List of keys to decode. partial ( bool , default: False ) \u2013 Whether to allow partial decoding. Defaults to False. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with decode filter. Source code in webdataset/compat.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def decode ( self , * args , pre = None , post = None , only = None , partial = False , handler = reraise_exception , ): \"\"\"Decode data based on the decoding functions given as arguments. This method creates a decoder using autodecode.Decoder and applies it using filters.map. Args: *args: Decoding functions or strings representing image handlers. pre (callable, optional): Pre-processing function. post (callable, optional): Post-processing function. only (list, optional): List of keys to decode. partial (bool, optional): Whether to allow partial decoding. Defaults to False. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with decode filter. \"\"\" handlers = [ autodecode . ImageHandler ( x ) if isinstance ( x , str ) else x for x in args ] decoder = autodecode . Decoder ( handlers , pre = pre , post = post , only = only , partial = partial ) return self . map ( decoder , handler = handler )","title":"decode"},{"location":"webdataset/#webdataset.FluidInterface.extract_keys","text":"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Parameters: *args \u2013 Keys or patterns to extract. **kw \u2013 Additional keyword arguments for filters.extract_keys. Returns: FluidInterface \u2013 Updated pipeline with extract_keys filter. Source code in webdataset/compat.py 256 257 258 259 260 261 262 263 264 265 266 267 268 def extract_keys ( self , * args , ** kw ): \"\"\"Extract specific keys from samples. This method forwards to the filters.extract_keys function. Args: *args: Keys or patterns to extract. **kw: Additional keyword arguments for filters.extract_keys. Returns: FluidInterface: Updated pipeline with extract_keys filter. \"\"\" return self . compose ( filters . extract_keys ( * args , ** kw ))","title":"extract_keys"},{"location":"webdataset/#webdataset.FluidInterface.listed","text":"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Parameters: batchsize ( int ) \u2013 Target list size. partial ( bool , default: True ) \u2013 Whether to return partial lists. Defaults to True. Returns: FluidInterface \u2013 Updated pipeline with listed filter. Source code in webdataset/compat.py 47 48 49 50 51 52 53 54 55 56 57 58 59 def listed ( self , batchsize , partial = True ): \"\"\"Create lists of samples without collation. This method forwards to the filters.batched function with collation_fn set to None. Args: batchsize (int): Target list size. partial (bool, optional): Whether to return partial lists. Defaults to True. Returns: FluidInterface: Updated pipeline with listed filter. \"\"\" return self . compose ( filters . batched ( batchsize = batchsize , collation_fn = None ))","title":"listed"},{"location":"webdataset/#webdataset.FluidInterface.lmdb_cached","text":"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Parameters: *args \u2013 Positional arguments for filters.LMDBCached. **kw \u2013 Keyword arguments for filters.LMDBCached. Returns: FluidInterface \u2013 Updated pipeline with LMDB caching. Source code in webdataset/compat.py 294 295 296 297 298 299 300 301 302 303 304 305 306 def lmdb_cached ( self , * args , ** kw ): \"\"\"Cache samples using LMDB. This method forwards to the filters.LMDBCached class. Args: *args: Positional arguments for filters.LMDBCached. **kw: Keyword arguments for filters.LMDBCached. Returns: FluidInterface: Updated pipeline with LMDB caching. \"\"\" return self . compose ( filters . LMDBCached ( * args , ** kw ))","title":"lmdb_cached"},{"location":"webdataset/#webdataset.FluidInterface.log_keys","text":"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Parameters: logfile ( str , default: None ) \u2013 Path to the log file. If None, logging is disabled. Returns: FluidInterface \u2013 Updated pipeline with log_keys filter. Source code in webdataset/compat.py 71 72 73 74 75 76 77 78 79 80 81 82 def log_keys ( self , logfile = None ): \"\"\"Log keys of samples passing through the pipeline. This method forwards to the filters.log_keys function. Args: logfile (str, optional): Path to the log file. If None, logging is disabled. Returns: FluidInterface: Updated pipeline with log_keys filter. \"\"\" return self . compose ( filters . log_keys ( logfile ))","title":"log_keys"},{"location":"webdataset/#webdataset.FluidInterface.map","text":"Apply a function to each sample in the stream. This method forwards to the filters.map function. Parameters: f ( callable ) \u2013 Function to apply to each sample. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map filter. Source code in webdataset/compat.py 101 102 103 104 105 106 107 108 109 110 111 112 113 def map ( self , f , handler = reraise_exception ): \"\"\"Apply a function to each sample in the stream. This method forwards to the filters.map function. Args: f (callable): Function to apply to each sample. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map filter. \"\"\" return self . compose ( filters . map ( f , handler = handler ))","title":"map"},{"location":"webdataset/#webdataset.FluidInterface.map_dict","text":"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Parameters: handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. **kw \u2013 Mapping of keys to functions to apply. Returns: FluidInterface \u2013 Updated pipeline with map_dict filter. Source code in webdataset/compat.py 147 148 149 150 151 152 153 154 155 156 157 158 159 def map_dict ( self , handler = reraise_exception , ** kw ): \"\"\"Map the entries in a dict sample with individual functions. This method forwards to the filters.map_dict function. Args: handler (callable, optional): Exception handler. Defaults to reraise_exception. **kw: Mapping of keys to functions to apply. Returns: FluidInterface: Updated pipeline with map_dict filter. \"\"\" return self . compose ( filters . map_dict ( handler = handler , ** kw ))","title":"map_dict"},{"location":"webdataset/#webdataset.FluidInterface.map_tuple","text":"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Parameters: *args \u2013 Functions to apply to each element of the tuple. handler ( callable , default: reraise_exception ) \u2013 Exception handler. Defaults to reraise_exception. Returns: FluidInterface \u2013 Updated pipeline with map_tuple filter. Source code in webdataset/compat.py 189 190 191 192 193 194 195 196 197 198 199 200 201 def map_tuple ( self , * args , handler = reraise_exception ): \"\"\"Map the entries of a tuple with individual functions. This method forwards to the filters.map_tuple function. Args: *args: Functions to apply to each element of the tuple. handler (callable, optional): Exception handler. Defaults to reraise_exception. Returns: FluidInterface: Updated pipeline with map_tuple filter. \"\"\" return self . compose ( filters . map_tuple ( * args , handler = handler ))","title":"map_tuple"},{"location":"webdataset/#webdataset.FluidInterface.mcached","text":"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface \u2013 Updated pipeline with memory caching. Source code in webdataset/compat.py 284 285 286 287 288 289 290 291 292 def mcached ( self ): \"\"\"Cache samples in memory. This method forwards to the filters.Cached class. Returns: FluidInterface: Updated pipeline with memory caching. \"\"\" return self . compose ( filters . Cached ())","title":"mcached"},{"location":"webdataset/#webdataset.FluidInterface.rename","text":"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Parameters: **kw \u2013 Mapping of old names to new names. Returns: FluidInterface \u2013 Updated pipeline with rename filter. Source code in webdataset/compat.py 216 217 218 219 220 221 222 223 224 225 226 227 def rename ( self , ** kw ): \"\"\"Rename samples based on keyword arguments. This method forwards to the filters.rename function. Args: **kw: Mapping of old names to new names. Returns: FluidInterface: Updated pipeline with rename filter. \"\"\" return self . compose ( filters . rename ( ** kw ))","title":"rename"},{"location":"webdataset/#webdataset.FluidInterface.rename_keys","text":"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Parameters: *args \u2013 Positional arguments for filters.rename_keys. **kw \u2013 Keyword arguments for filters.rename_keys. Returns: FluidInterface \u2013 Updated pipeline with rename_keys filter. Source code in webdataset/compat.py 242 243 244 245 246 247 248 249 250 251 252 253 254 def rename_keys ( self , * args , ** kw ): \"\"\"Rename keys in samples based on patterns. This method forwards to the filters.rename_keys function. Args: *args: Positional arguments for filters.rename_keys. **kw: Keyword arguments for filters.rename_keys. Returns: FluidInterface: Updated pipeline with rename_keys filter. \"\"\" return self . compose ( filters . rename_keys ( * args , ** kw ))","title":"rename_keys"},{"location":"webdataset/#webdataset.FluidInterface.rsample","text":"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Parameters: p ( float , default: 0.5 ) \u2013 Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface \u2013 Updated pipeline with rsample filter. Source code in webdataset/compat.py 229 230 231 232 233 234 235 236 237 238 239 240 def rsample ( self , p = 0.5 ): \"\"\"Randomly subsample a stream of data. This method forwards to the filters.rsample function. Args: p (float, optional): Probability of keeping each sample. Defaults to 0.5. Returns: FluidInterface: Updated pipeline with rsample filter. \"\"\" return self . compose ( filters . rsample ( p ))","title":"rsample"},{"location":"webdataset/#webdataset.FluidInterface.select","text":"Select samples based on a predicate. This method forwards to the filters.select function. Parameters: predicate ( callable ) \u2013 Function that returns True for samples to keep. **kw \u2013 Additional keyword arguments for filters.select. Returns: FluidInterface \u2013 Updated pipeline with select filter. Source code in webdataset/compat.py 161 162 163 164 165 166 167 168 169 170 171 172 173 def select ( self , predicate , ** kw ): \"\"\"Select samples based on a predicate. This method forwards to the filters.select function. Args: predicate (callable): Function that returns True for samples to keep. **kw: Additional keyword arguments for filters.select. Returns: FluidInterface: Updated pipeline with select filter. \"\"\" return self . compose ( filters . select ( predicate , ** kw ))","title":"select"},{"location":"webdataset/#webdataset.FluidInterface.shuffle","text":"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Parameters: size ( int ) \u2013 Buffer size for shuffling. **kw \u2013 Additional keyword arguments for filters.shuffle. Returns: FluidInterface \u2013 Updated pipeline with shuffle filter, or self if size < 1. Source code in webdataset/compat.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def shuffle ( self , size , ** kw ): \"\"\"Shuffle the data in the stream. This method forwards to the filters.shuffle function if size > 0. Args: size (int): Buffer size for shuffling. **kw: Additional keyword arguments for filters.shuffle. Returns: FluidInterface: Updated pipeline with shuffle filter, or self if size < 1. \"\"\" if size < 1 : return self else : return self . compose ( filters . shuffle ( size , ** kw ))","title":"shuffle"},{"location":"webdataset/#webdataset.FluidInterface.slice","text":"Slice the data stream. This method forwards to the filters.slice function. Parameters: *args \u2013 Arguments for slicing (start, stop, step). Returns: FluidInterface \u2013 Updated pipeline with slice filter. Source code in webdataset/compat.py 203 204 205 206 207 208 209 210 211 212 213 214 def slice ( self , * args ): \"\"\"Slice the data stream. This method forwards to the filters.slice function. Args: *args: Arguments for slicing (start, stop, step). Returns: FluidInterface: Updated pipeline with slice filter. \"\"\" return self . compose ( filters . slice ( * args ))","title":"slice"},{"location":"webdataset/#webdataset.FluidInterface.to_tuple","text":"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Parameters: *args \u2013 Keys to extract from the dict. **kw \u2013 Additional keyword arguments for filters.to_tuple. Returns: FluidInterface \u2013 Updated pipeline with to_tuple filter. Source code in webdataset/compat.py 175 176 177 178 179 180 181 182 183 184 185 186 187 def to_tuple ( self , * args , ** kw ): \"\"\"Convert dict samples to tuples. This method forwards to the filters.to_tuple function. Args: *args: Keys to extract from the dict. **kw: Additional keyword arguments for filters.to_tuple. Returns: FluidInterface: Updated pipeline with to_tuple filter. \"\"\" return self . compose ( filters . to_tuple ( * args , ** kw ))","title":"to_tuple"},{"location":"webdataset/#webdataset.FluidInterface.unbatched","text":"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface \u2013 Updated pipeline with unbatched filter. Source code in webdataset/compat.py 37 38 39 40 41 42 43 44 45 def unbatched ( self ): \"\"\"Turn batched data back into unbatched data. This method forwards to the filters.unbatched function. Returns: FluidInterface: Updated pipeline with unbatched filter. \"\"\" return self . compose ( filters . unbatched ())","title":"unbatched"},{"location":"webdataset/#webdataset.FluidInterface.unlisted","text":"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface \u2013 Updated pipeline with unlisted filter. Source code in webdataset/compat.py 61 62 63 64 65 66 67 68 69 def unlisted ( self ): \"\"\"Turn listed data back into individual samples. This method forwards to the filters.unlisted function. Returns: FluidInterface: Updated pipeline with unlisted filter. \"\"\" return self . compose ( filters . unlisted ())","title":"unlisted"},{"location":"webdataset/#webdataset.FluidInterface.xdecode","text":"Decode data based on file extensions. This method forwards to the filters.xdecode function. Parameters: *args \u2013 Positional arguments for filters.xdecode. **kw \u2013 Keyword arguments for filters.xdecode. Returns: FluidInterface \u2013 Updated pipeline with xdecode filter. Source code in webdataset/compat.py 270 271 272 273 274 275 276 277 278 279 280 281 282 def xdecode ( self , * args , ** kw ): \"\"\"Decode data based on file extensions. This method forwards to the filters.xdecode function. Args: *args: Positional arguments for filters.xdecode. **kw: Keyword arguments for filters.xdecode. Returns: FluidInterface: Updated pipeline with xdecode filter. \"\"\" return self . compose ( filters . xdecode ( * args , ** kw ))","title":"xdecode"},{"location":"webdataset/#webdataset.with_epoch","text":"Bases: IterableDataset Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Parameters: dataset \u2013 The source IterableDataset. length ( int ) \u2013 Declared length of the dataset. Source code in webdataset/extradatasets.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 class with_epoch ( IterableDataset ): \"\"\"Change the actual and nominal length of an IterableDataset. This will continuously iterate through the original dataset, but impose new epoch boundaries at the given length/nominal. This exists mainly as a workaround for the odd logic in DataLoader. It is also useful for choosing smaller nominal epoch sizes with very large datasets. Args: dataset: The source IterableDataset. length (int): Declared length of the dataset. \"\"\" def __init__ ( self , dataset , length ): super () . __init__ () self . length = length self . source = None def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None","title":"with_epoch"},{"location":"webdataset/#webdataset.with_epoch.__getstate__","text":"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict \u2013 A dictionary representing the pickled state of the dataset. Source code in webdataset/extradatasets.py 91 92 93 94 95 96 97 98 99 100 101 def __getstate__ ( self ): \"\"\"Return the pickled state of the dataset. This resets the dataset iterator, since that can't be pickled. Returns: dict: A dictionary representing the pickled state of the dataset. \"\"\" result = dict ( self . __dict__ ) result [ \"source\" ] = None return result","title":"__getstate__"},{"location":"webdataset/#webdataset.with_epoch.invoke","text":"Return an iterator over the dataset. This iterator returns as many samples as given by the length parameter. Parameters: dataset \u2013 The source dataset to iterate over. Yields: Sample \u2013 The next sample from the dataset. Source code in webdataset/extradatasets.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def invoke ( self , dataset ): \"\"\"Return an iterator over the dataset. This iterator returns as many samples as given by the `length` parameter. Args: dataset: The source dataset to iterate over. Yields: Sample: The next sample from the dataset. \"\"\" if self . source is None : self . source = iter ( dataset ) for _ in range ( self . length ): try : sample = next ( self . source ) except StopIteration : self . source = iter ( dataset ) try : sample = next ( self . source ) except StopIteration : return yield sample self . source = None","title":"invoke"},{"location":"webdataset/#writing-webdatasets","text":"","title":"Writing WebDatasets"},{"location":"webdataset/#webdataset.ShardWriter","text":"Like TarWriter but splits into multiple shards. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. Defaults to 100000. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. Defaults to 3e9. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. Defaults to None. start_shard ( int , default: 0 ) \u2013 Starting shard number. Defaults to 0. verbose ( int , default: 1 ) \u2013 Verbosity level. Defaults to 1. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. Defaults to None. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 class ShardWriter : \"\"\"Like TarWriter but splits into multiple shards. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. Defaults to 100000. maxsize: Maximum size of each shard. Defaults to 3e9. post: Optional callable to be executed after each shard is written. Defaults to None. start_shard: Starting shard number. Defaults to 0. verbose: Verbosity level. Defaults to 1. opener: Optional callable to open output files. Defaults to None. **kw: Other options passed to TarWriter. \"\"\" def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream () def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( self . opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close ()","title":"ShardWriter"},{"location":"webdataset/#webdataset.ShardWriter.__enter__","text":"Enter context. Returns: self \u2013 The ShardWriter object. Source code in webdataset/writer.py 609 610 611 612 613 614 615 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The ShardWriter object. \"\"\" return self","title":"__enter__"},{"location":"webdataset/#webdataset.ShardWriter.__exit__","text":"Exit context. Source code in webdataset/writer.py 617 618 619 def __exit__ ( self , * args , ** kw ): \"\"\"Exit context.\"\"\" self . close ()","title":"__exit__"},{"location":"webdataset/#webdataset.ShardWriter.__init__","text":"Create a ShardWriter. Parameters: pattern ( str ) \u2013 Output file pattern. maxcount ( int , default: 100000 ) \u2013 Maximum number of records per shard. maxsize ( float , default: 3000000000.0 ) \u2013 Maximum size of each shard. post ( Optional [ Callable ] , default: None ) \u2013 Optional callable to be executed after each shard is written. start_shard ( int , default: 0 ) \u2013 Starting shard number. verbose ( int , default: 1 ) \u2013 Verbosity level. opener ( Optional [ Callable ] , default: None ) \u2013 Optional callable to open output files. **kw \u2013 Other options passed to TarWriter. Source code in webdataset/writer.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def __init__ ( self , pattern : str , maxcount : int = 100000 , maxsize : float = 3e9 , post : Optional [ Callable ] = None , start_shard : int = 0 , verbose : int = 1 , opener : Optional [ Callable ] = None , ** kw , ): \"\"\"Create a ShardWriter. Args: pattern: Output file pattern. maxcount: Maximum number of records per shard. maxsize: Maximum size of each shard. post: Optional callable to be executed after each shard is written. start_shard: Starting shard number. verbose: Verbosity level. opener: Optional callable to open output files. **kw: Other options passed to TarWriter. \"\"\" self . verbose = verbose self . kw = kw self . maxcount = maxcount self . maxsize = maxsize self . post = post self . tarstream = None self . shard = start_shard self . pattern = pattern self . total = 0 self . count = 0 self . size = 0 self . fname = None self . opener = opener self . next_stream ()","title":"__init__"},{"location":"webdataset/#webdataset.ShardWriter.close","text":"Close the stream. Source code in webdataset/writer.py 601 602 603 604 605 606 607 def close ( self ): \"\"\"Close the stream.\"\"\" self . finish () del self . tarstream del self . shard del self . count del self . size","title":"close"},{"location":"webdataset/#webdataset.ShardWriter.finish","text":"Finish all writing (use close instead). Source code in webdataset/writer.py 592 593 594 595 596 597 598 599 def finish ( self ): \"\"\"Finish all writing (use close instead).\"\"\" if self . tarstream is not None : self . tarstream . close () assert self . fname is not None if callable ( self . post ): self . post ( self . fname ) self . tarstream = None","title":"finish"},{"location":"webdataset/#webdataset.ShardWriter.next_stream","text":"Close the current stream and move to the next. Source code in webdataset/writer.py 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 def next_stream ( self ): \"\"\"Close the current stream and move to the next.\"\"\" self . finish () self . fname = self . pattern % self . shard if self . verbose : print ( \"# writing\" , self . fname , self . count , \" %.1f GB\" % ( self . size / 1e9 ), self . total , ) self . shard += 1 if self . opener : self . tarstream = TarWriter ( self . opener ( self . fname ), ** self . kw ) else : self . tarstream = TarWriter ( self . fname , ** self . kw ) self . count = 0 self . size = 0","title":"next_stream"},{"location":"webdataset/#webdataset.ShardWriter.write","text":"Write a sample. Parameters: obj \u2013 Sample to be written. Source code in webdataset/writer.py 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 def write ( self , obj ): \"\"\"Write a sample. Args: obj: Sample to be written. \"\"\" if ( self . tarstream is None or self . count >= self . maxcount or self . size >= self . maxsize ): self . next_stream () size = self . tarstream . write ( obj ) self . count += 1 self . total += 1 self . size += size","title":"write"},{"location":"webdataset/#webdataset.TarWriter","text":"A class for writing dictionaries to tar files. Parameters: fileobj \u2013 File name for tar file (.tgz/.tar) or open file descriptor. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Sample encoding. Defaults to True. compress ( Optional [ Union [ bool , str ]] , default: None ) \u2013 Compression flag. Defaults to None. user ( str , default: 'bigdata' ) \u2013 User for tar files. Defaults to \"bigdata\". group ( str , default: 'bigdata' ) \u2013 Group for tar files. Defaults to \"bigdata\". mode ( int , default: 292 ) \u2013 Mode for tar files. Defaults to 0o0444. keep_meta ( bool , default: False ) \u2013 Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime ( Optional [ float ] , default: None ) \u2013 Modification time. Defaults to None. format ( Any , default: None ) \u2013 Tar format. Defaults to None. Returns: \u2013 TarWriter object. Raises: ValueError \u2013 If the encoder doesn't yield bytes for a key. True will use an encoder that behaves similar to the automatic decoder for Dataset . False disables encoding and expects byte strings (except for metadata, which must be strings). The encoder argument can also be a callable , or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: a/b.png and a/b.output.png . tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) Source code in webdataset/writer.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 class TarWriter : \"\"\"A class for writing dictionaries to tar files. Args: fileobj: File name for tar file (.tgz/.tar) or open file descriptor. encoder: Sample encoding. Defaults to True. compress: Compression flag. Defaults to None. user: User for tar files. Defaults to \"bigdata\". group: Group for tar files. Defaults to \"bigdata\". mode: Mode for tar files. Defaults to 0o0444. keep_meta: Flag to keep metadata (entries starting with \"_\"). Defaults to False. mtime: Modification time. Defaults to None. format: Tar format. Defaults to None. Returns: TarWriter object. Raises: ValueError: If the encoder doesn't yield bytes for a key. `True` will use an encoder that behaves similar to the automatic decoder for `Dataset`. `False` disables encoding and expects byte strings (except for metadata, which must be strings). The `encoder` argument can also be a `callable`, or a dictionary mapping extensions to encoders. The following code will add two file to the tar archive: `a/b.png` and `a/b.output.png`. tarwriter = TarWriter(stream) image = imread(\"b.jpg\") image2 = imread(\"b.out.jpg\") sample = {\"__key__\": \"a/b\", \"png\": image, \"output.png\": image2} tarwriter.write(sample) \"\"\" def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ Union [ bool , str ]] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime tarmode = self . tarmode ( fileobj , compress ) if isinstance ( fileobj , str ): fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close () def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total @staticmethod def tarmode ( fileobj , compress : Optional [ Union [ bool , str ]] = None ): if compress is False : return \"w|\" elif ( compress is True or compress == \"gz\" or ( isinstance ( fileobj , str ) and fileobj . endswith ( \"gz\" )) ): return \"w|gz\" elif compress == \"bz2\" or ( isinstance ( fileobj , str ) and fileobj . endswith ( \"bz2\" ) ): return \"w|bz2\" elif compress == \"xz\" or ( isinstance ( fileobj , str ) and fileobj . endswith ( \"xz\" )): return \"w|xz\" else : return \"w|\"","title":"TarWriter"},{"location":"webdataset/#webdataset.TarWriter.__enter__","text":"Enter context. Returns: self \u2013 The TarWriter object. Source code in webdataset/writer.py 416 417 418 419 420 421 422 def __enter__ ( self ): \"\"\"Enter context. Returns: self: The TarWriter object. \"\"\" return self","title":"__enter__"},{"location":"webdataset/#webdataset.TarWriter.__exit__","text":"Exit context. Source code in webdataset/writer.py 424 425 426 def __exit__ ( self , exc_type , exc_val , exc_tb ): \"\"\"Exit context.\"\"\" self . close ()","title":"__exit__"},{"location":"webdataset/#webdataset.TarWriter.__init__","text":"Create a tar writer. Parameters: fileobj \u2013 Stream to write data to. user ( str , default: 'bigdata' ) \u2013 User for tar files. group ( str , default: 'bigdata' ) \u2013 Group for tar files. mode ( int , default: 292 ) \u2013 Mode for tar files. compress ( Optional [ Union [ bool , str ]] , default: None ) \u2013 Desired compression. encoder ( Union [None, bool , Callable ] , default: True ) \u2013 Encoder function. keep_meta ( bool , default: False ) \u2013 Keep metadata (entries starting with \"_\"). mtime ( Optional [ float ] , default: None ) \u2013 Modification time (set this to some fixed value to get reproducible tar files). format ( Any , default: None ) \u2013 Tar format. Source code in webdataset/writer.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def __init__ ( self , fileobj , user : str = \"bigdata\" , group : str = \"bigdata\" , mode : int = 0o0444 , compress : Optional [ Union [ bool , str ]] = None , encoder : Union [ None , bool , Callable ] = True , keep_meta : bool = False , mtime : Optional [ float ] = None , format : Any = None , ): # sourcery skip: avoid-builtin-shadow \"\"\"Create a tar writer. Args: fileobj: Stream to write data to. user: User for tar files. group: Group for tar files. mode: Mode for tar files. compress: Desired compression. encoder: Encoder function. keep_meta: Keep metadata (entries starting with \"_\"). mtime: Modification time (set this to some fixed value to get reproducible tar files). format: Tar format. \"\"\" format = getattr ( tarfile , format , format ) if format else tarfile . USTAR_FORMAT self . mtime = mtime tarmode = self . tarmode ( fileobj , compress ) if isinstance ( fileobj , str ): fileobj = gopen ( fileobj , \"wb\" ) self . own_fileobj = fileobj else : self . own_fileobj = None self . encoder = make_encoder ( encoder ) self . keep_meta = keep_meta self . stream = fileobj self . tarstream = tarfile . open ( fileobj = fileobj , mode = tarmode ) self . user = user self . group = group self . mode = mode self . compress = compress","title":"__init__"},{"location":"webdataset/#webdataset.TarWriter.close","text":"Close the tar file. Source code in webdataset/writer.py 428 429 430 431 432 433 def close ( self ): \"\"\"Close the tar file.\"\"\" self . tarstream . close () if self . own_fileobj is not None : self . own_fileobj . close () self . own_fileobj = None","title":"close"},{"location":"webdataset/#webdataset.TarWriter.write","text":"Write a dictionary to the tar file. Parameters: obj \u2013 Dictionary of objects to be stored. Returns: int \u2013 Size of the entry. Raises: ValueError \u2013 If the object doesn't contain a key or if a key doesn't map to bytes after encoding. Source code in webdataset/writer.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def write ( self , obj ): \"\"\"Write a dictionary to the tar file. Args: obj: Dictionary of objects to be stored. Returns: int: Size of the entry. Raises: ValueError: If the object doesn't contain a __key__ or if a key doesn't map to bytes after encoding. \"\"\" total = 0 obj = self . encoder ( obj ) if \"__key__\" not in obj : raise ValueError ( \"object must contain a __key__\" ) for k , v in list ( obj . items ()): if k [ 0 ] == \"_\" : continue if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \" { k } doesn't map to a bytes after encoding ( { type ( v ) } )\" ) key = obj [ \"__key__\" ] for k in sorted ( obj . keys ()): if k == \"__key__\" : continue if not self . keep_meta and k [ 0 ] == \"_\" : continue v = obj [ k ] if isinstance ( v , str ): v = v . encode ( \"utf-8\" ) now = time . time () ti = tarfile . TarInfo ( key + \".\" + k ) ti . size = len ( v ) ti . mtime = self . mtime if self . mtime is not None else now ti . mode = self . mode ti . uname = self . user ti . gname = self . group if not isinstance ( v , ( bytes , bytearray , memoryview )): raise ValueError ( f \"converter didn't yield bytes: { k } , { type ( v ) } \" ) stream = io . BytesIO ( v ) self . tarstream . addfile ( ti , stream ) total += ti . size return total","title":"write"},{"location":"webdataset/#low-level-io","text":"","title":"Low Level I/O"},{"location":"webdataset/#webdataset.gopen.gopen","text":"Open the URL using various schemes and protocols. This function provides a unified interface for opening resources specified by URLs, supporting multiple schemes and protocols. It uses the gopen_schemes dispatch table to handle different URL schemes. Built-in support is provided for the following schemes: - pipe: for opening named pipes - file: for local file system access - http, https: for web resources - sftp, ftps: for secure file transfer - scp: for secure copy protocol When no scheme is specified in the URL, it is treated as a local file path. Environment Variables: - GOPEN_VERBOSE: Set to a non-zero value to enable verbose logging of file operations. Format: GOPEN_VERBOSE=1 - USE_AIS_FOR: Specifies which cloud storage services should use AIS (and its cache) for access. Format: USE_AIS_FOR=aws:gs:s3 - GOPEN_BUFFER: Sets the buffer size for file operations (in bytes). Format: GOPEN_BUFFER=8192 Parameters: url ( str ) \u2013 The source URL or file path to open. mode ( str , default: 'rb' ) \u2013 The mode for opening the resource. Only \"rb\" (read binary) and \"wb\" (write binary) are supported. bufsize ( int , default: 8192 ) \u2013 The buffer size for file operations. Default is 8192 bytes. **kw \u2013 Additional keyword arguments to pass to the underlying open function. Returns: \u2013 file-like object: An opened file-like object for the specified resource. Raises: ValueError \u2013 If an unsupported mode is specified. Note: - For stdin/stdout operations, use \"-\" as the URL. - The function applies URL rewriting based on the GOPEN_REWRITE environment variable before processing. Source code in webdataset/gopen.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def gopen ( url , mode = \"rb\" , bufsize = 8192 , ** kw ): \"\"\"Open the URL using various schemes and protocols. This function provides a unified interface for opening resources specified by URLs, supporting multiple schemes and protocols. It uses the `gopen_schemes` dispatch table to handle different URL schemes. Built-in support is provided for the following schemes: - pipe: for opening named pipes - file: for local file system access - http, https: for web resources - sftp, ftps: for secure file transfer - scp: for secure copy protocol When no scheme is specified in the URL, it is treated as a local file path. Environment Variables: - GOPEN_VERBOSE: Set to a non-zero value to enable verbose logging of file operations. Format: GOPEN_VERBOSE=1 - USE_AIS_FOR: Specifies which cloud storage services should use AIS (and its cache) for access. Format: USE_AIS_FOR=aws:gs:s3 - GOPEN_BUFFER: Sets the buffer size for file operations (in bytes). Format: GOPEN_BUFFER=8192 Args: url (str): The source URL or file path to open. mode (str): The mode for opening the resource. Only \"rb\" (read binary) and \"wb\" (write binary) are supported. bufsize (int): The buffer size for file operations. Default is 8192 bytes. **kw: Additional keyword arguments to pass to the underlying open function. Returns: file-like object: An opened file-like object for the specified resource. Raises: ValueError: If an unsupported mode is specified. Other exceptions may be raised depending on the specific handler used for the URL scheme. Note: - For stdin/stdout operations, use \"-\" as the URL. - The function applies URL rewriting based on the GOPEN_REWRITE environment variable before processing. \"\"\" global fallback_gopen verbose = int ( os . environ . get ( \"GOPEN_VERBOSE\" , 0 )) if verbose : print ( \"GOPEN\" , url , info , file = sys . stderr ) assert mode in [ \"rb\" , \"wb\" ], mode if url == \"-\" : if mode == \"rb\" : return sys . stdin . buffer elif mode == \"wb\" : return sys . stdout . buffer else : raise ValueError ( f \"unknown mode { mode } \" ) url = rewrite_url ( url ) pr = urlparse ( url ) if pr . scheme == \"\" : bufsize = int ( os . environ . get ( \"GOPEN_BUFFER\" , - 1 )) return open ( url , mode , buffering = bufsize ) if pr . scheme == \"file\" : bufsize = int ( os . environ . get ( \"GOPEN_BUFFER\" , - 1 )) return open ( url2pathname ( pr . path ), mode , buffering = bufsize ) handler = gopen_schemes [ \"__default__\" ] handler = gopen_schemes . get ( pr . scheme , handler ) return handler ( url , mode , bufsize , ** kw )","title":"gopen"},{"location":"webdataset/#error-handling","text":"","title":"Error Handling"},{"location":"webdataset/#webdataset.ignore_and_continue","text":"Ignore the exception and continue processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 34 35 36 37 38 39 40 41 42 43 def ignore_and_continue ( exn ): \"\"\"Ignore the exception and continue processing. Args: exn: The exception to be ignored. Returns: bool: Always returns True to indicate continuation. \"\"\" return True","title":"ignore_and_continue"},{"location":"webdataset/#webdataset.ignore_and_stop","text":"Ignore the exception and stop further processing. Parameters: exn \u2013 The exception to be ignored. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 60 61 62 63 64 65 66 67 68 69 def ignore_and_stop ( exn ): \"\"\"Ignore the exception and stop further processing. Args: exn: The exception to be ignored. Returns: bool: Always returns False to indicate stopping. \"\"\" return False","title":"ignore_and_stop"},{"location":"webdataset/#webdataset.reraise_exception","text":"Re-raise the given exception. Parameters: exn \u2013 The exception to be re-raised. Source code in webdataset/handlers.py 22 23 24 25 26 27 28 29 30 31 def reraise_exception ( exn ): \"\"\"Re-raise the given exception. Args: exn: The exception to be re-raised. Raises: The input exception. \"\"\" raise exn","title":"reraise_exception"},{"location":"webdataset/#webdataset.warn_and_continue","text":"Issue a warning for the exception and continue processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns True to indicate continuation. Source code in webdataset/handlers.py 46 47 48 49 50 51 52 53 54 55 56 57 def warn_and_continue ( exn ): \"\"\"Issue a warning for the exception and continue processing. Args: exn: The exception to be warned about. Returns: bool: Always returns True to indicate continuation. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return True","title":"warn_and_continue"},{"location":"webdataset/#webdataset.warn_and_stop","text":"Issue a warning for the exception and stop further processing. Parameters: exn \u2013 The exception to be warned about. Returns: bool \u2013 Always returns False to indicate stopping. Source code in webdataset/handlers.py 72 73 74 75 76 77 78 79 80 81 82 83 def warn_and_stop ( exn ): \"\"\"Issue a warning for the exception and stop further processing. Args: exn: The exception to be warned about. Returns: bool: Always returns False to indicate stopping. \"\"\" warnings . warn ( repr ( exn )) time . sleep ( 0.5 ) return False","title":"warn_and_stop"},{"location":"wids/","text":"WIDS API wids.ShardListDataset Bases: Dataset [ T ] An indexable dataset based on a list of shards. The dataset is either given as a list of shards with optional options and name, or as a URL pointing to a JSON descriptor file. Datasets can reference other datasets via source_url . Shard references within a dataset are resolve relative to an explicitly given base property, or relative to the URL from which the dataset descriptor was loaded. Source code in wids/wids.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 class ShardListDataset ( Dataset [ T ]): \"\"\"An indexable dataset based on a list of shards. The dataset is either given as a list of shards with optional options and name, or as a URL pointing to a JSON descriptor file. Datasets can reference other datasets via `source_url`. Shard references within a dataset are resolve relative to an explicitly given `base` property, or relative to the URL from which the dataset descriptor was loaded. \"\"\" def __init__ ( self , shards , * , cache_size = int ( 1e12 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = \"PIL\" , keep = False , base = None , options = None , ): \"\"\"Create a ShardListDataset. Args: shards: a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size: the number of shards to keep in the cache lru_size: the number of shards to keep in the LRU cache localname: a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. \"\"\" if options is None : options = {} super ( ShardListDataset , self ) . __init__ () # shards is a list of (filename, length) pairs. We'll need to # keep track of the lengths and cumulative lengths to know how # to map indices to shards and indices within shards. if isinstance ( shards , ( str , io . IOBase )): if base is None and isinstance ( shards , str ): base = urldir ( shards ) self . base = base self . spec = load_dsdesc_and_resolve ( shards , options = options , base = base ) self . shards = self . spec . get ( \"shardlist\" , []) self . dataset_name = self . spec . get ( \"name\" ) or hash_dataset_name ( str ( shards )) else : self . base = None self . spec = options self . shards = shards self . dataset_name = dataset_name or hash_dataset_name ( str ( shards )) self . lengths = [ shard [ \"nsamples\" ] for shard in self . shards ] self . cum_lengths = np . cumsum ( self . lengths ) self . total_length = self . cum_lengths [ - 1 ] if cache_dir is not None : # when a cache dir is explicitly given, we download files into # that directory without any changes self . cache_dir = cache_dir self . localname = cache_localname ( cache_dir ) elif localname is not None : # when a localname function is given, we use that self . cache_dir = None self . localname = localname else : # when no cache dir or localname are given, use the cache from the environment self . cache_dir = os . environ . get ( \"WIDS_CACHE\" , \"/tmp/_wids_cache\" ) self . localname = default_localname ( self . cache_dir ) if True or int ( os . environ . get ( \"WIDS_VERBOSE\" , 0 )): nbytes = sum ( shard . get ( \"filesize\" , 0 ) for shard in self . shards ) nsamples = sum ( shard [ \"nsamples\" ] for shard in self . shards ) print ( str ( shards )[: 50 ], \"base:\" , self . base , \"name:\" , self . spec . get ( \"name\" ), \"nfiles:\" , len ( self . shards ), \"nbytes:\" , nbytes , \"samples:\" , nsamples , \"cache:\" , self . cache_dir , file = sys . stderr , ) self . transformations = interpret_transformations ( transformations ) if lru_size > 200 : warnings . warn ( \"LRU size is very large; consider reducing it to avoid running out of file descriptors\" ) self . cache = LRUShards ( lru_size , localname = self . localname , keep = keep ) def add_transform ( self , transform ): \"\"\"Add a transformation to the dataset.\"\"\" self . transformations . append ( transform ) return self def __len__ ( self ): \"\"\"Return the total number of samples in the dataset.\"\"\" return self . total_length def get_stats ( self ): \"\"\"Return the number of cache accesses and misses.\"\"\" return self . cache . accesses , self . cache . misses def check_cache_misses ( self ): \"\"\"Check if the cache miss rate is too high.\"\"\" accesses , misses = self . get_stats () if accesses > 100 and misses / accesses > 0.3 : # output a warning only once self . check_cache_misses = lambda : None print ( \"Warning: ShardListDataset has a cache miss rate of {:.1%} %\" . format ( misses * 100.0 / accesses ) ) def get_shard ( self , index ): \"\"\"Get the shard and index within the shard corresponding to the given index.\"\"\" # Find the shard corresponding to the given index. shard_idx = np . searchsorted ( self . cum_lengths , index , side = \"right\" ) # Figure out which index within the shard corresponds to the # given index. if shard_idx == 0 : inner_idx = index else : inner_idx = index - self . cum_lengths [ shard_idx - 1 ] # Get the shard and return the corresponding element. desc = self . shards [ shard_idx ] url = desc [ \"url\" ] shard = self . cache . get_shard ( url ) return shard , inner_idx , desc def __getitem__ ( self , index ): \"\"\"Return the sample corresponding to the given index.\"\"\" shard , inner_idx , desc = self . get_shard ( index ) sample = shard [ inner_idx ] # Check if we're missing the cache too often. self . check_cache_misses () sample [ \"__dataset__\" ] = desc . get ( \"dataset\" ) sample [ \"__index__\" ] = index sample [ \"__shard__\" ] = desc [ \"url\" ] sample [ \"__shardindex__\" ] = inner_idx # Apply transformations for transform in self . transformations : sample = transform ( sample ) return sample def close ( self ): \"\"\"Close the dataset.\"\"\" self . cache . clear () __getitem__ ( index ) Return the sample corresponding to the given index. Source code in wids/wids.py 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 def __getitem__ ( self , index ): \"\"\"Return the sample corresponding to the given index.\"\"\" shard , inner_idx , desc = self . get_shard ( index ) sample = shard [ inner_idx ] # Check if we're missing the cache too often. self . check_cache_misses () sample [ \"__dataset__\" ] = desc . get ( \"dataset\" ) sample [ \"__index__\" ] = index sample [ \"__shard__\" ] = desc [ \"url\" ] sample [ \"__shardindex__\" ] = inner_idx # Apply transformations for transform in self . transformations : sample = transform ( sample ) return sample __init__ ( shards , * , cache_size = int ( 1000000000000.0 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = 'PIL' , keep = False , base = None , options = None ) Create a ShardListDataset. Parameters: shards \u2013 a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size \u2013 the number of shards to keep in the cache lru_size \u2013 the number of shards to keep in the LRU cache localname \u2013 a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. Source code in wids/wids.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def __init__ ( self , shards , * , cache_size = int ( 1e12 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = \"PIL\" , keep = False , base = None , options = None , ): \"\"\"Create a ShardListDataset. Args: shards: a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size: the number of shards to keep in the cache lru_size: the number of shards to keep in the LRU cache localname: a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. \"\"\" if options is None : options = {} super ( ShardListDataset , self ) . __init__ () # shards is a list of (filename, length) pairs. We'll need to # keep track of the lengths and cumulative lengths to know how # to map indices to shards and indices within shards. if isinstance ( shards , ( str , io . IOBase )): if base is None and isinstance ( shards , str ): base = urldir ( shards ) self . base = base self . spec = load_dsdesc_and_resolve ( shards , options = options , base = base ) self . shards = self . spec . get ( \"shardlist\" , []) self . dataset_name = self . spec . get ( \"name\" ) or hash_dataset_name ( str ( shards )) else : self . base = None self . spec = options self . shards = shards self . dataset_name = dataset_name or hash_dataset_name ( str ( shards )) self . lengths = [ shard [ \"nsamples\" ] for shard in self . shards ] self . cum_lengths = np . cumsum ( self . lengths ) self . total_length = self . cum_lengths [ - 1 ] if cache_dir is not None : # when a cache dir is explicitly given, we download files into # that directory without any changes self . cache_dir = cache_dir self . localname = cache_localname ( cache_dir ) elif localname is not None : # when a localname function is given, we use that self . cache_dir = None self . localname = localname else : # when no cache dir or localname are given, use the cache from the environment self . cache_dir = os . environ . get ( \"WIDS_CACHE\" , \"/tmp/_wids_cache\" ) self . localname = default_localname ( self . cache_dir ) if True or int ( os . environ . get ( \"WIDS_VERBOSE\" , 0 )): nbytes = sum ( shard . get ( \"filesize\" , 0 ) for shard in self . shards ) nsamples = sum ( shard [ \"nsamples\" ] for shard in self . shards ) print ( str ( shards )[: 50 ], \"base:\" , self . base , \"name:\" , self . spec . get ( \"name\" ), \"nfiles:\" , len ( self . shards ), \"nbytes:\" , nbytes , \"samples:\" , nsamples , \"cache:\" , self . cache_dir , file = sys . stderr , ) self . transformations = interpret_transformations ( transformations ) if lru_size > 200 : warnings . warn ( \"LRU size is very large; consider reducing it to avoid running out of file descriptors\" ) self . cache = LRUShards ( lru_size , localname = self . localname , keep = keep ) __len__ () Return the total number of samples in the dataset. Source code in wids/wids.py 460 461 462 def __len__ ( self ): \"\"\"Return the total number of samples in the dataset.\"\"\" return self . total_length add_transform ( transform ) Add a transformation to the dataset. Source code in wids/wids.py 455 456 457 458 def add_transform ( self , transform ): \"\"\"Add a transformation to the dataset.\"\"\" self . transformations . append ( transform ) return self check_cache_misses () Check if the cache miss rate is too high. Source code in wids/wids.py 468 469 470 471 472 473 474 475 476 477 478 def check_cache_misses ( self ): \"\"\"Check if the cache miss rate is too high.\"\"\" accesses , misses = self . get_stats () if accesses > 100 and misses / accesses > 0.3 : # output a warning only once self . check_cache_misses = lambda : None print ( \"Warning: ShardListDataset has a cache miss rate of {:.1%} %\" . format ( misses * 100.0 / accesses ) ) close () Close the dataset. Source code in wids/wids.py 517 518 519 def close ( self ): \"\"\"Close the dataset.\"\"\" self . cache . clear () get_shard ( index ) Get the shard and index within the shard corresponding to the given index. Source code in wids/wids.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 def get_shard ( self , index ): \"\"\"Get the shard and index within the shard corresponding to the given index.\"\"\" # Find the shard corresponding to the given index. shard_idx = np . searchsorted ( self . cum_lengths , index , side = \"right\" ) # Figure out which index within the shard corresponds to the # given index. if shard_idx == 0 : inner_idx = index else : inner_idx = index - self . cum_lengths [ shard_idx - 1 ] # Get the shard and return the corresponding element. desc = self . shards [ shard_idx ] url = desc [ \"url\" ] shard = self . cache . get_shard ( url ) return shard , inner_idx , desc get_stats () Return the number of cache accesses and misses. Source code in wids/wids.py 464 465 466 def get_stats ( self ): \"\"\"Return the number of cache accesses and misses.\"\"\" return self . cache . accesses , self . cache . misses wids.ChunkedSampler Bases: Sampler A sampler that samples in chunks and then shuffles the samples within each chunk. This preserves locality of reference while still shuffling the data. Source code in wids/wids.py 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 class ChunkedSampler ( Sampler ): \"\"\"A sampler that samples in chunks and then shuffles the samples within each chunk. This preserves locality of reference while still shuffling the data. \"\"\" def __init__ ( self , dataset , * , dslength_per_replica =- 1 , num_samples = None , chunksize = 2000 , seed = 0 , shuffle = True , shufflefirst = False , ): if isinstance ( num_samples , int ): lo , hi = 0 , num_samples elif num_samples is None : lo , hi = 0 , len ( dataset ) else : lo , hi = num_samples self . dslength_per_replica = ( dslength_per_replica if dslength_per_replica > 0 else ( hi - lo ) ) self . ranges = [( i , min ( i + chunksize , hi )) for i in range ( lo , hi , chunksize )] self . seed = seed self . shuffle = shuffle self . shufflefirst = shufflefirst self . epoch = 0 def set_epoch ( self , epoch ): self . epoch = epoch def __iter__ ( self ): self . rng = random . Random ( self . seed + 1289738273 * self . epoch ) shardshuffle = self . shufflefirst or self . epoch > 0 yield from iterate_ranges ( self . ranges , self . rng , indexshuffle = self . shuffle , shardshuffle = ( self . shuffle and shardshuffle ), ) self . epoch += 1 def __len__ ( self ) -> int : return self . dslength_per_replica wids . DistributedChunkedSampler ( dataset , * , num_replicas = None , num_samples = None , rank = None , shuffle = True , shufflefirst = False , seed = 0 , drop_last = None , chunksize = 1000000 ) Return a ChunkedSampler for the current worker in distributed training. Reverts to a simple ChunkedSampler if not running in distributed mode. Since the split among workers takes place before the chunk shuffle, workers end up with a fixed set of shards they need to download. The more workers, the fewer shards are used by each worker. Source code in wids/wids.py 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 def DistributedChunkedSampler ( dataset : Dataset , * , num_replicas : Optional [ int ] = None , num_samples : Optional [ int ] = None , rank : Optional [ int ] = None , shuffle : bool = True , shufflefirst : bool = False , seed : int = 0 , drop_last : bool = None , chunksize : int = 1000000 , ) -> ChunkedSampler : \"\"\"Return a ChunkedSampler for the current worker in distributed training. Reverts to a simple ChunkedSampler if not running in distributed mode. Since the split among workers takes place before the chunk shuffle, workers end up with a fixed set of shards they need to download. The more workers, the fewer shards are used by each worker. \"\"\" if drop_last is not None : warnings . warn ( \"DistributedChunkedSampler does not support drop_last, thus it will be ignored\" ) if not dist . is_initialized (): warnings . warn ( \"DistributedChunkedSampler is called without distributed initialized; assuming single process\" ) num_replicas = 1 rank = 0 else : num_replicas = num_replicas or dist . get_world_size () rank = rank or dist . get_rank () assert rank >= 0 and rank < num_replicas # From https://github.com/pytorch/pytorch/blob/13fa59580e4dd695817ccf2f24922fd211667fc8/torch/utils/data/distributed.py#L93 dslength_per_replica = ( math . ceil ( len ( dataset ) / num_replicas ) if num_replicas > 1 else len ( dataset ) ) num_samples = num_samples or len ( dataset ) worker_chunk = ( num_samples + num_replicas - 1 ) // num_replicas worker_start = rank * worker_chunk worker_end = min ( worker_start + worker_chunk , num_samples ) return ChunkedSampler ( dataset , dslength_per_replica = dslength_per_replica , num_samples = ( worker_start , worker_end ), chunksize = chunksize , seed = seed , shuffle = shuffle , shufflefirst = shufflefirst , ) wids . ShardedSampler = ShardListSampler module-attribute","title":"WIDS"},{"location":"wids/#wids-api","text":"","title":"WIDS API"},{"location":"wids/#wids.ShardListDataset","text":"Bases: Dataset [ T ] An indexable dataset based on a list of shards. The dataset is either given as a list of shards with optional options and name, or as a URL pointing to a JSON descriptor file. Datasets can reference other datasets via source_url . Shard references within a dataset are resolve relative to an explicitly given base property, or relative to the URL from which the dataset descriptor was loaded. Source code in wids/wids.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 class ShardListDataset ( Dataset [ T ]): \"\"\"An indexable dataset based on a list of shards. The dataset is either given as a list of shards with optional options and name, or as a URL pointing to a JSON descriptor file. Datasets can reference other datasets via `source_url`. Shard references within a dataset are resolve relative to an explicitly given `base` property, or relative to the URL from which the dataset descriptor was loaded. \"\"\" def __init__ ( self , shards , * , cache_size = int ( 1e12 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = \"PIL\" , keep = False , base = None , options = None , ): \"\"\"Create a ShardListDataset. Args: shards: a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size: the number of shards to keep in the cache lru_size: the number of shards to keep in the LRU cache localname: a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. \"\"\" if options is None : options = {} super ( ShardListDataset , self ) . __init__ () # shards is a list of (filename, length) pairs. We'll need to # keep track of the lengths and cumulative lengths to know how # to map indices to shards and indices within shards. if isinstance ( shards , ( str , io . IOBase )): if base is None and isinstance ( shards , str ): base = urldir ( shards ) self . base = base self . spec = load_dsdesc_and_resolve ( shards , options = options , base = base ) self . shards = self . spec . get ( \"shardlist\" , []) self . dataset_name = self . spec . get ( \"name\" ) or hash_dataset_name ( str ( shards )) else : self . base = None self . spec = options self . shards = shards self . dataset_name = dataset_name or hash_dataset_name ( str ( shards )) self . lengths = [ shard [ \"nsamples\" ] for shard in self . shards ] self . cum_lengths = np . cumsum ( self . lengths ) self . total_length = self . cum_lengths [ - 1 ] if cache_dir is not None : # when a cache dir is explicitly given, we download files into # that directory without any changes self . cache_dir = cache_dir self . localname = cache_localname ( cache_dir ) elif localname is not None : # when a localname function is given, we use that self . cache_dir = None self . localname = localname else : # when no cache dir or localname are given, use the cache from the environment self . cache_dir = os . environ . get ( \"WIDS_CACHE\" , \"/tmp/_wids_cache\" ) self . localname = default_localname ( self . cache_dir ) if True or int ( os . environ . get ( \"WIDS_VERBOSE\" , 0 )): nbytes = sum ( shard . get ( \"filesize\" , 0 ) for shard in self . shards ) nsamples = sum ( shard [ \"nsamples\" ] for shard in self . shards ) print ( str ( shards )[: 50 ], \"base:\" , self . base , \"name:\" , self . spec . get ( \"name\" ), \"nfiles:\" , len ( self . shards ), \"nbytes:\" , nbytes , \"samples:\" , nsamples , \"cache:\" , self . cache_dir , file = sys . stderr , ) self . transformations = interpret_transformations ( transformations ) if lru_size > 200 : warnings . warn ( \"LRU size is very large; consider reducing it to avoid running out of file descriptors\" ) self . cache = LRUShards ( lru_size , localname = self . localname , keep = keep ) def add_transform ( self , transform ): \"\"\"Add a transformation to the dataset.\"\"\" self . transformations . append ( transform ) return self def __len__ ( self ): \"\"\"Return the total number of samples in the dataset.\"\"\" return self . total_length def get_stats ( self ): \"\"\"Return the number of cache accesses and misses.\"\"\" return self . cache . accesses , self . cache . misses def check_cache_misses ( self ): \"\"\"Check if the cache miss rate is too high.\"\"\" accesses , misses = self . get_stats () if accesses > 100 and misses / accesses > 0.3 : # output a warning only once self . check_cache_misses = lambda : None print ( \"Warning: ShardListDataset has a cache miss rate of {:.1%} %\" . format ( misses * 100.0 / accesses ) ) def get_shard ( self , index ): \"\"\"Get the shard and index within the shard corresponding to the given index.\"\"\" # Find the shard corresponding to the given index. shard_idx = np . searchsorted ( self . cum_lengths , index , side = \"right\" ) # Figure out which index within the shard corresponds to the # given index. if shard_idx == 0 : inner_idx = index else : inner_idx = index - self . cum_lengths [ shard_idx - 1 ] # Get the shard and return the corresponding element. desc = self . shards [ shard_idx ] url = desc [ \"url\" ] shard = self . cache . get_shard ( url ) return shard , inner_idx , desc def __getitem__ ( self , index ): \"\"\"Return the sample corresponding to the given index.\"\"\" shard , inner_idx , desc = self . get_shard ( index ) sample = shard [ inner_idx ] # Check if we're missing the cache too often. self . check_cache_misses () sample [ \"__dataset__\" ] = desc . get ( \"dataset\" ) sample [ \"__index__\" ] = index sample [ \"__shard__\" ] = desc [ \"url\" ] sample [ \"__shardindex__\" ] = inner_idx # Apply transformations for transform in self . transformations : sample = transform ( sample ) return sample def close ( self ): \"\"\"Close the dataset.\"\"\" self . cache . clear ()","title":"ShardListDataset"},{"location":"wids/#wids.ShardListDataset.__getitem__","text":"Return the sample corresponding to the given index. Source code in wids/wids.py 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 def __getitem__ ( self , index ): \"\"\"Return the sample corresponding to the given index.\"\"\" shard , inner_idx , desc = self . get_shard ( index ) sample = shard [ inner_idx ] # Check if we're missing the cache too often. self . check_cache_misses () sample [ \"__dataset__\" ] = desc . get ( \"dataset\" ) sample [ \"__index__\" ] = index sample [ \"__shard__\" ] = desc [ \"url\" ] sample [ \"__shardindex__\" ] = inner_idx # Apply transformations for transform in self . transformations : sample = transform ( sample ) return sample","title":"__getitem__"},{"location":"wids/#wids.ShardListDataset.__init__","text":"Create a ShardListDataset. Parameters: shards \u2013 a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size \u2013 the number of shards to keep in the cache lru_size \u2013 the number of shards to keep in the LRU cache localname \u2013 a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. Source code in wids/wids.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def __init__ ( self , shards , * , cache_size = int ( 1e12 ), cache_dir = None , lru_size = 10 , dataset_name = None , localname = None , transformations = \"PIL\" , keep = False , base = None , options = None , ): \"\"\"Create a ShardListDataset. Args: shards: a list of (filename, length) pairs or a URL pointing to a JSON descriptor file cache_size: the number of shards to keep in the cache lru_size: the number of shards to keep in the LRU cache localname: a function that maps URLs to local filenames Note that there are two caches: an on-disk directory, and an in-memory LRU cache. \"\"\" if options is None : options = {} super ( ShardListDataset , self ) . __init__ () # shards is a list of (filename, length) pairs. We'll need to # keep track of the lengths and cumulative lengths to know how # to map indices to shards and indices within shards. if isinstance ( shards , ( str , io . IOBase )): if base is None and isinstance ( shards , str ): base = urldir ( shards ) self . base = base self . spec = load_dsdesc_and_resolve ( shards , options = options , base = base ) self . shards = self . spec . get ( \"shardlist\" , []) self . dataset_name = self . spec . get ( \"name\" ) or hash_dataset_name ( str ( shards )) else : self . base = None self . spec = options self . shards = shards self . dataset_name = dataset_name or hash_dataset_name ( str ( shards )) self . lengths = [ shard [ \"nsamples\" ] for shard in self . shards ] self . cum_lengths = np . cumsum ( self . lengths ) self . total_length = self . cum_lengths [ - 1 ] if cache_dir is not None : # when a cache dir is explicitly given, we download files into # that directory without any changes self . cache_dir = cache_dir self . localname = cache_localname ( cache_dir ) elif localname is not None : # when a localname function is given, we use that self . cache_dir = None self . localname = localname else : # when no cache dir or localname are given, use the cache from the environment self . cache_dir = os . environ . get ( \"WIDS_CACHE\" , \"/tmp/_wids_cache\" ) self . localname = default_localname ( self . cache_dir ) if True or int ( os . environ . get ( \"WIDS_VERBOSE\" , 0 )): nbytes = sum ( shard . get ( \"filesize\" , 0 ) for shard in self . shards ) nsamples = sum ( shard [ \"nsamples\" ] for shard in self . shards ) print ( str ( shards )[: 50 ], \"base:\" , self . base , \"name:\" , self . spec . get ( \"name\" ), \"nfiles:\" , len ( self . shards ), \"nbytes:\" , nbytes , \"samples:\" , nsamples , \"cache:\" , self . cache_dir , file = sys . stderr , ) self . transformations = interpret_transformations ( transformations ) if lru_size > 200 : warnings . warn ( \"LRU size is very large; consider reducing it to avoid running out of file descriptors\" ) self . cache = LRUShards ( lru_size , localname = self . localname , keep = keep )","title":"__init__"},{"location":"wids/#wids.ShardListDataset.__len__","text":"Return the total number of samples in the dataset. Source code in wids/wids.py 460 461 462 def __len__ ( self ): \"\"\"Return the total number of samples in the dataset.\"\"\" return self . total_length","title":"__len__"},{"location":"wids/#wids.ShardListDataset.add_transform","text":"Add a transformation to the dataset. Source code in wids/wids.py 455 456 457 458 def add_transform ( self , transform ): \"\"\"Add a transformation to the dataset.\"\"\" self . transformations . append ( transform ) return self","title":"add_transform"},{"location":"wids/#wids.ShardListDataset.check_cache_misses","text":"Check if the cache miss rate is too high. Source code in wids/wids.py 468 469 470 471 472 473 474 475 476 477 478 def check_cache_misses ( self ): \"\"\"Check if the cache miss rate is too high.\"\"\" accesses , misses = self . get_stats () if accesses > 100 and misses / accesses > 0.3 : # output a warning only once self . check_cache_misses = lambda : None print ( \"Warning: ShardListDataset has a cache miss rate of {:.1%} %\" . format ( misses * 100.0 / accesses ) )","title":"check_cache_misses"},{"location":"wids/#wids.ShardListDataset.close","text":"Close the dataset. Source code in wids/wids.py 517 518 519 def close ( self ): \"\"\"Close the dataset.\"\"\" self . cache . clear ()","title":"close"},{"location":"wids/#wids.ShardListDataset.get_shard","text":"Get the shard and index within the shard corresponding to the given index. Source code in wids/wids.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 def get_shard ( self , index ): \"\"\"Get the shard and index within the shard corresponding to the given index.\"\"\" # Find the shard corresponding to the given index. shard_idx = np . searchsorted ( self . cum_lengths , index , side = \"right\" ) # Figure out which index within the shard corresponds to the # given index. if shard_idx == 0 : inner_idx = index else : inner_idx = index - self . cum_lengths [ shard_idx - 1 ] # Get the shard and return the corresponding element. desc = self . shards [ shard_idx ] url = desc [ \"url\" ] shard = self . cache . get_shard ( url ) return shard , inner_idx , desc","title":"get_shard"},{"location":"wids/#wids.ShardListDataset.get_stats","text":"Return the number of cache accesses and misses. Source code in wids/wids.py 464 465 466 def get_stats ( self ): \"\"\"Return the number of cache accesses and misses.\"\"\" return self . cache . accesses , self . cache . misses","title":"get_stats"},{"location":"wids/#wids.ChunkedSampler","text":"Bases: Sampler A sampler that samples in chunks and then shuffles the samples within each chunk. This preserves locality of reference while still shuffling the data. Source code in wids/wids.py 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 class ChunkedSampler ( Sampler ): \"\"\"A sampler that samples in chunks and then shuffles the samples within each chunk. This preserves locality of reference while still shuffling the data. \"\"\" def __init__ ( self , dataset , * , dslength_per_replica =- 1 , num_samples = None , chunksize = 2000 , seed = 0 , shuffle = True , shufflefirst = False , ): if isinstance ( num_samples , int ): lo , hi = 0 , num_samples elif num_samples is None : lo , hi = 0 , len ( dataset ) else : lo , hi = num_samples self . dslength_per_replica = ( dslength_per_replica if dslength_per_replica > 0 else ( hi - lo ) ) self . ranges = [( i , min ( i + chunksize , hi )) for i in range ( lo , hi , chunksize )] self . seed = seed self . shuffle = shuffle self . shufflefirst = shufflefirst self . epoch = 0 def set_epoch ( self , epoch ): self . epoch = epoch def __iter__ ( self ): self . rng = random . Random ( self . seed + 1289738273 * self . epoch ) shardshuffle = self . shufflefirst or self . epoch > 0 yield from iterate_ranges ( self . ranges , self . rng , indexshuffle = self . shuffle , shardshuffle = ( self . shuffle and shardshuffle ), ) self . epoch += 1 def __len__ ( self ) -> int : return self . dslength_per_replica","title":"ChunkedSampler"},{"location":"wids/#wids.DistributedChunkedSampler","text":"Return a ChunkedSampler for the current worker in distributed training. Reverts to a simple ChunkedSampler if not running in distributed mode. Since the split among workers takes place before the chunk shuffle, workers end up with a fixed set of shards they need to download. The more workers, the fewer shards are used by each worker. Source code in wids/wids.py 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 def DistributedChunkedSampler ( dataset : Dataset , * , num_replicas : Optional [ int ] = None , num_samples : Optional [ int ] = None , rank : Optional [ int ] = None , shuffle : bool = True , shufflefirst : bool = False , seed : int = 0 , drop_last : bool = None , chunksize : int = 1000000 , ) -> ChunkedSampler : \"\"\"Return a ChunkedSampler for the current worker in distributed training. Reverts to a simple ChunkedSampler if not running in distributed mode. Since the split among workers takes place before the chunk shuffle, workers end up with a fixed set of shards they need to download. The more workers, the fewer shards are used by each worker. \"\"\" if drop_last is not None : warnings . warn ( \"DistributedChunkedSampler does not support drop_last, thus it will be ignored\" ) if not dist . is_initialized (): warnings . warn ( \"DistributedChunkedSampler is called without distributed initialized; assuming single process\" ) num_replicas = 1 rank = 0 else : num_replicas = num_replicas or dist . get_world_size () rank = rank or dist . get_rank () assert rank >= 0 and rank < num_replicas # From https://github.com/pytorch/pytorch/blob/13fa59580e4dd695817ccf2f24922fd211667fc8/torch/utils/data/distributed.py#L93 dslength_per_replica = ( math . ceil ( len ( dataset ) / num_replicas ) if num_replicas > 1 else len ( dataset ) ) num_samples = num_samples or len ( dataset ) worker_chunk = ( num_samples + num_replicas - 1 ) // num_replicas worker_start = rank * worker_chunk worker_end = min ( worker_start + worker_chunk , num_samples ) return ChunkedSampler ( dataset , dslength_per_replica = dslength_per_replica , num_samples = ( worker_start , worker_end ), chunksize = chunksize , seed = seed , shuffle = shuffle , shufflefirst = shufflefirst , )","title":"DistributedChunkedSampler"},{"location":"wids/#wids.ShardedSampler","text":"","title":"ShardedSampler"},{"location":"examples/webdataset/","text":"Tesseract Wds Tesseract Wds The notebook demonstrates how to process a large dataset of PDF files stored in shards using the webdataset library. It illustrates the conversion of PDFs to JPEG images and the subsequent OCR processing with Tesseract, showcasing the use of WebDataset for reading and TarWriter for writing datasets. The notebook also exemplifies parallel processing of shards with Ray, highlighting the library's capability to handle large-scale OCR tasks efficiently. Column Store Column Store The notebook demonstrates how to use the WebDataset library to handle datasets that are split not only by rows but also by columns, by adding an additional column from a separate dataset to the main dataset using a custom add_column function. It shows how to use the __url__ field to load additional data and ensure the keys match between the main and additional data. The notebook also introduces a simpler approach using the wids library with a CombinedDataset class to combine two datasets, ensuring they have the same length and merging their contents by index. Generate Text Dataset Generate Text Dataset The notebook demonstrates the use of the webdataset library for generating and storing synthetic text data in a sharded TAR format, which is suitable for large-scale machine learning tasks. It uses the TarWriter class to create shards containing text samples generated by GPT-2, and includes functions for checking the existence of and uploading these shards to cloud storage. The notebook also shows how to parallelize the shard generation and upload process using Ray. Mi Prompts Mi Prompts The notebook outlines the process of generating a Mini ImageNet dataset using a text generation model ( TextGenerationModel ) and a caption generator ( CaptionGenerator ). It utilizes the webdataset library to handle the dataset creation and shuffling, specifically using classes like TarWriter for writing shards and read_webdataset and write_webdataset functions for reading and shuffling the dataset shards. The notebook demonstrates parallelization with Ray to scale up the generation process across multiple GPUs and the use of webdataset for managing large-scale image-caption datasets. Train Resnet50 Multiray Wds Train Resnet50 Multiray Wds The notebook demonstrates how to use the WebDataset library for distributed training with PyTorch's DistributedDataParallel . It focuses on creating data loaders for streaming data directly from cloud storage for training deep learning models, specifically using the WebDataset , WebLoader , and IterableDataset classes from the webdataset library. The notebook also shows how to integrate this setup with Ray for easy distributed training across multiple GPUs. Train Resnet50 Wds Train Resnet50 Wds The notebook demonstrates how to use the webdataset library for training a ResNet50 model on a large-scale image dataset, specifically a fake version of ImageNet, by streaming data directly from cloud storage. It showcases the construction of a data loader using WebDataset , WebLoader , and various transformations and shuffling methods provided by the library to prepare the data for training. The notebook also includes a typical PyTorch training loop, illustrating how the webdataset library integrates with PyTorch's training pipeline. Wds Notes Wds Notes The notebook provides an overview and tutorial on how to use the webdataset library, which is a PyTorch IterableDataset for handling large-scale datasets stored in web-friendly formats like tar files. It demonstrates how to read and process data from such datasets, including shuffling, decoding, and preprocessing for deep learning applications. The notebook showcases the use of classes like WebDataset , DataPipeline , and TarWriter from the webdataset library, illustrating how to create efficient I/O pipelines for training models with large datasets that can be read from local disk or cloud storage. Mi Images Mi Images The notebook demonstrates the use of the webdataset library to create a mini ImageNet-like dataset by generating images from textual prompts using a stable diffusion model. It showcases the WebDataset class for reading and writing dataset shards and the TarWriter class for outputting transformed data. The notebook also illustrates parallel processing with Ray to distribute the image rendering workload across multiple GPUs, utilizing the ActorPool class for managing a pool of actors. Train Ocr Errors Hf Train Ocr Errors Hf The notebook demonstrates the use of the WebDataset library, specifically the ShardListDataset and ShardedSampler classes, for efficient remote data access and loading in the context of fine-tuning a Huggingface large language model. It highlights how training data can be incrementally downloaded and cached locally, and how a custom sampler can be employed to optimize the data retrieval process from shards. The notebook also showcases the integration of WebDataset with Huggingface's training loop, including the use of adapters for model fine-tuning.","title":"Index"},{"location":"examples/webdataset/#tesseract-wds","text":"Tesseract Wds The notebook demonstrates how to process a large dataset of PDF files stored in shards using the webdataset library. It illustrates the conversion of PDFs to JPEG images and the subsequent OCR processing with Tesseract, showcasing the use of WebDataset for reading and TarWriter for writing datasets. The notebook also exemplifies parallel processing of shards with Ray, highlighting the library's capability to handle large-scale OCR tasks efficiently.","title":"Tesseract Wds"},{"location":"examples/webdataset/#column-store","text":"Column Store The notebook demonstrates how to use the WebDataset library to handle datasets that are split not only by rows but also by columns, by adding an additional column from a separate dataset to the main dataset using a custom add_column function. It shows how to use the __url__ field to load additional data and ensure the keys match between the main and additional data. The notebook also introduces a simpler approach using the wids library with a CombinedDataset class to combine two datasets, ensuring they have the same length and merging their contents by index.","title":"Column Store"},{"location":"examples/webdataset/#generate-text-dataset","text":"Generate Text Dataset The notebook demonstrates the use of the webdataset library for generating and storing synthetic text data in a sharded TAR format, which is suitable for large-scale machine learning tasks. It uses the TarWriter class to create shards containing text samples generated by GPT-2, and includes functions for checking the existence of and uploading these shards to cloud storage. The notebook also shows how to parallelize the shard generation and upload process using Ray.","title":"Generate Text Dataset"},{"location":"examples/webdataset/#mi-prompts","text":"Mi Prompts The notebook outlines the process of generating a Mini ImageNet dataset using a text generation model ( TextGenerationModel ) and a caption generator ( CaptionGenerator ). It utilizes the webdataset library to handle the dataset creation and shuffling, specifically using classes like TarWriter for writing shards and read_webdataset and write_webdataset functions for reading and shuffling the dataset shards. The notebook demonstrates parallelization with Ray to scale up the generation process across multiple GPUs and the use of webdataset for managing large-scale image-caption datasets.","title":"Mi Prompts"},{"location":"examples/webdataset/#train-resnet50-multiray-wds","text":"Train Resnet50 Multiray Wds The notebook demonstrates how to use the WebDataset library for distributed training with PyTorch's DistributedDataParallel . It focuses on creating data loaders for streaming data directly from cloud storage for training deep learning models, specifically using the WebDataset , WebLoader , and IterableDataset classes from the webdataset library. The notebook also shows how to integrate this setup with Ray for easy distributed training across multiple GPUs.","title":"Train Resnet50 Multiray Wds"},{"location":"examples/webdataset/#train-resnet50-wds","text":"Train Resnet50 Wds The notebook demonstrates how to use the webdataset library for training a ResNet50 model on a large-scale image dataset, specifically a fake version of ImageNet, by streaming data directly from cloud storage. It showcases the construction of a data loader using WebDataset , WebLoader , and various transformations and shuffling methods provided by the library to prepare the data for training. The notebook also includes a typical PyTorch training loop, illustrating how the webdataset library integrates with PyTorch's training pipeline.","title":"Train Resnet50 Wds"},{"location":"examples/webdataset/#wds-notes","text":"Wds Notes The notebook provides an overview and tutorial on how to use the webdataset library, which is a PyTorch IterableDataset for handling large-scale datasets stored in web-friendly formats like tar files. It demonstrates how to read and process data from such datasets, including shuffling, decoding, and preprocessing for deep learning applications. The notebook showcases the use of classes like WebDataset , DataPipeline , and TarWriter from the webdataset library, illustrating how to create efficient I/O pipelines for training models with large datasets that can be read from local disk or cloud storage.","title":"Wds Notes"},{"location":"examples/webdataset/#mi-images","text":"Mi Images The notebook demonstrates the use of the webdataset library to create a mini ImageNet-like dataset by generating images from textual prompts using a stable diffusion model. It showcases the WebDataset class for reading and writing dataset shards and the TarWriter class for outputting transformed data. The notebook also illustrates parallel processing with Ray to distribute the image rendering workload across multiple GPUs, utilizing the ActorPool class for managing a pool of actors.","title":"Mi Images"},{"location":"examples/webdataset/#train-ocr-errors-hf","text":"Train Ocr Errors Hf The notebook demonstrates the use of the WebDataset library, specifically the ShardListDataset and ShardedSampler classes, for efficient remote data access and loading in the context of fine-tuning a Huggingface large language model. It highlights how training data can be incrementally downloaded and cached locally, and how a custom sampler can be employed to optimize the data retrieval process from shards. The notebook also showcases the integration of WebDataset with Huggingface's training loop, including the use of adapters for model fine-tuning.","title":"Train Ocr Errors Hf"},{"location":"examples/webdataset/column-store/","text":"Using WebDataset as a Column Store Sometimes it is desirable to break up a dataset not just by rows but also by columns. This is quite easy in WebDataset, although there is no explicit API for it (one will likely be added). The idea is to just use the __url__ field in a sample to load additional columns as necessary. # We usually abbreviate webdataset as wds import webdataset as wds batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\" # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset(training_urls, resampled=True, shardshuffle=True) This function computes the URL for an additional column from a base URL. This is then used by the add_column function to add data from that additional URL to the data already loaded from the base URL. def find_column_url(url): # In this function, given the main URL for a shard, find the corresponding # extra column URL. # For the demo, we just return the same URL, which means that we simply # add the same values to the samples twice. return url # .replace(\"-train\", \"-train-more\") def add_column(src, find_column_url=find_column_url): \"\"\"Given an iterator over a dataset, add an extra column from a separate dataset.\"\"\" last_url = None column_src = None for sample in src: # We use the __url__ field to keep track of which shard we are working on. # We then open the corresponding URL for the extra column data if necessary. if last_url != sample[\"__url__\"]: column_url = find_column_url(sample[\"__url__\"]) print(\"*** opening column_url\", column_url) column_src = iter(wds.WebDataset(column_url, shardshuffle=False)) last_url = sample[\"__url__\"] # Read the next sample from the extra column data. extra = next(column_src) # Check that the keys match. assert extra[\"__key__\"] == sample[\"__key__\"] # Update the sample with the extra data. for k, v in extra.items(): if k[0] != \"_\": sample[k] = v yield sample trainset = trainset.compose(add_column) # NB: any shuffling, decoding, etc. needs to happen after the `add_column` call Let's see all of it in action. Actually, nothing particularly interesting happens here because we are just loading the same data for the base URL and the additional column. Really, the only feedback you get from this code is the message about opening the column_url. for k, v in next(iter(trainset)).items(): print(k, repr(v)[:60]) Some comments: The code above assumes an exact correspondence between the samples in the different columnn shards; this is really what you ought to aim for. But you can add code to skip data. For small amounts of data (like class labels), you probably just want to store the data in a dbm-style database and use .associate(data) . You could also use wids to retrieve additional samples in add_column . If you want to do the same thing in wids , the code becomes even simpler: class CombinedDataset: def __init__(self, ds1, ds2): self.ds1 = wids.ShardListDataset(ds1) self.ds2 = wids.ShardListDataset(ds2) assert len(self.ds1) == len(self.ds2) def getitem(self, index): return self.ds1[index].update(self.ds2[index]) def __len__(self): return len(self.ds1)","title":"Using WebDataset as a Column Store"},{"location":"examples/webdataset/column-store/#using-webdataset-as-a-column-store","text":"Sometimes it is desirable to break up a dataset not just by rows but also by columns. This is quite easy in WebDataset, although there is no explicit API for it (one will likely be added). The idea is to just use the __url__ field in a sample to load additional columns as necessary. # We usually abbreviate webdataset as wds import webdataset as wds batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\" # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset(training_urls, resampled=True, shardshuffle=True) This function computes the URL for an additional column from a base URL. This is then used by the add_column function to add data from that additional URL to the data already loaded from the base URL. def find_column_url(url): # In this function, given the main URL for a shard, find the corresponding # extra column URL. # For the demo, we just return the same URL, which means that we simply # add the same values to the samples twice. return url # .replace(\"-train\", \"-train-more\") def add_column(src, find_column_url=find_column_url): \"\"\"Given an iterator over a dataset, add an extra column from a separate dataset.\"\"\" last_url = None column_src = None for sample in src: # We use the __url__ field to keep track of which shard we are working on. # We then open the corresponding URL for the extra column data if necessary. if last_url != sample[\"__url__\"]: column_url = find_column_url(sample[\"__url__\"]) print(\"*** opening column_url\", column_url) column_src = iter(wds.WebDataset(column_url, shardshuffle=False)) last_url = sample[\"__url__\"] # Read the next sample from the extra column data. extra = next(column_src) # Check that the keys match. assert extra[\"__key__\"] == sample[\"__key__\"] # Update the sample with the extra data. for k, v in extra.items(): if k[0] != \"_\": sample[k] = v yield sample trainset = trainset.compose(add_column) # NB: any shuffling, decoding, etc. needs to happen after the `add_column` call Let's see all of it in action. Actually, nothing particularly interesting happens here because we are just loading the same data for the base URL and the additional column. Really, the only feedback you get from this code is the message about opening the column_url. for k, v in next(iter(trainset)).items(): print(k, repr(v)[:60]) Some comments: The code above assumes an exact correspondence between the samples in the different columnn shards; this is really what you ought to aim for. But you can add code to skip data. For small amounts of data (like class labels), you probably just want to store the data in a dbm-style database and use .associate(data) . You could also use wids to retrieve additional samples in add_column . If you want to do the same thing in wids , the code becomes even simpler: class CombinedDataset: def __init__(self, ds1, ds2): self.ds1 = wids.ShardListDataset(ds1) self.ds2 = wids.ShardListDataset(ds2) assert len(self.ds1) == len(self.ds2) def getitem(self, index): return self.ds1[index].update(self.ds2[index]) def __len__(self): return len(self.ds1)","title":"Using WebDataset as a Column Store"},{"location":"examples/webdataset/column-store.summary/","text":"The notebook demonstrates how to use the WebDataset library to handle datasets that are split not only by rows but also by columns, by adding an additional column from a separate dataset to the main dataset using a custom add_column function. It shows how to use the __url__ field to load additional data and ensure the keys match between the main and additional data. The notebook also introduces a simpler approach using the wids library with a CombinedDataset class to combine two datasets, ensuring they have the same length and merging their contents by index.","title":"Column store.summary"},{"location":"examples/webdataset/generate-text-dataset/","text":"Dataset Generation This is a simple example of dataset generation using WebDataset TarWriter . Shard are uploaded to a server or to the cloud as they are generated. Parallel dataset generation with Ray is illustrated at the very end. This particular notebook generates short text samples using GPT-2. These can be used to generate OCR training data. # package installs for colab import sys if \"google.colab\" in sys.modules: !pip install --quiet webdataset !pip install --quiet adapter-transformers !pip install --quiet sentencepiece !pip install --quiet datasets import uuid import webdataset as wds import os from transformers import GPT2LMHeadModel, GPT2Tokenizer from transformers import pipeline import textwrap # Parameters nsamples = 10 ntokens = 100 nshards = 3 # text generation with Huggingface and GPT2 tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\") tokenizer.pad_token = tokenizer.eos_token model = GPT2LMHeadModel.from_pretrained(\"gpt2\") generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) def generate(n, prompt=\"\"): \"\"\"Generate n words of text, starting with prompt.\"\"\" global tokenizer, model, generator output = generator( prompt, max_length=n + len(tokenizer.encode(prompt)), do_sample=True, temperature=0.99, top_k=50, top_p=0.99, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, )[0] return output[\"generated_text\"] text = generate(100).strip() print() print(textwrap.fill(text, 64)) # function generating an entire shard using TarWriter def generate_shard(oname, nsamples=10000, ntokens=500, prefix=\"\"): \"\"\"Generate a shard of samples with text. Each sample has a \"__key__\" field and a \"txt.gz\" field. That is, the individual text files are compressed automatically on write. They will be automatically decompressed when read. \"\"\" with wds.TarWriter(oname) as output: for i in range(nsamples): text = generate(100).strip() key = uuid.uuid4().hex text = generate(ntokens) sample = {\"__key__\": key, \"txt.gz\": text} output.write(sample) if i % 10 == 0: print(f\"{i:6d} {prefix}:\", repr(text)[:60]) generate_shard(\"temp.tar\", nsamples=10, ntokens=10) !ls -l temp.tar !tar tf temp.tar | head -5 # We need a couple of simple functions to upload to the cloud. def cloud_exists(oname): \"\"\"Check whether a file exists in the cloud.\"\"\" # return os.system(f\"gsutil stat gs://mybucket/500tokens/{oname}\") == 0 return True def cloud_upload(oname): \"\"\"Upload a file to the cloud.\"\"\" # assert os.system(f\"gsutil cp {oname} gs://mybucket/500tokens/{oname}\") == 0 pass # We can now generate a shard and upload it to the cloud. # We skip the generation if the file already exists in the cloud. def generate_and_upload(i): \"\"\"Generate a shard and upload it to the cloud.\"\"\" oname = f\"text-{i:06d}.tar\" if cloud_exists(oname): print(f\"{oname} already exists, skipping\") return False generate_shard(oname, nsamples=nsamples, ntokens=ntokens, prefix=f\"{i:6d} {oname}\") cloud_upload(oname) os.remove(oname) return True # For sequential generation, use this for i in range(nshards): generate_and_upload(i) %%script true # For parallel generation, use this import ray @ray.remote(num_cpus=1, num_gpus=1) def ray_generate_and_upload(i): \"\"\"A Ray remote function that generates a shard and uploads it to the cloud.\"\"\" return generate_and_upload(i) def generate_shards(nshards=10): \"\"\"Generate a number of shards and upload them to the cloud. Runs in parallel on a Ray cluster. \"\"\" ray.init(address='auto') # Connect to the Ray cluster tasks = [ray_generate_and_upload.remote(i) for i in range(nshards)] ray.shutdown() return shard_names","title":"Dataset Generation"},{"location":"examples/webdataset/generate-text-dataset/#dataset-generation","text":"This is a simple example of dataset generation using WebDataset TarWriter . Shard are uploaded to a server or to the cloud as they are generated. Parallel dataset generation with Ray is illustrated at the very end. This particular notebook generates short text samples using GPT-2. These can be used to generate OCR training data. # package installs for colab import sys if \"google.colab\" in sys.modules: !pip install --quiet webdataset !pip install --quiet adapter-transformers !pip install --quiet sentencepiece !pip install --quiet datasets import uuid import webdataset as wds import os from transformers import GPT2LMHeadModel, GPT2Tokenizer from transformers import pipeline import textwrap # Parameters nsamples = 10 ntokens = 100 nshards = 3 # text generation with Huggingface and GPT2 tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\") tokenizer.pad_token = tokenizer.eos_token model = GPT2LMHeadModel.from_pretrained(\"gpt2\") generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) def generate(n, prompt=\"\"): \"\"\"Generate n words of text, starting with prompt.\"\"\" global tokenizer, model, generator output = generator( prompt, max_length=n + len(tokenizer.encode(prompt)), do_sample=True, temperature=0.99, top_k=50, top_p=0.99, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, )[0] return output[\"generated_text\"] text = generate(100).strip() print() print(textwrap.fill(text, 64)) # function generating an entire shard using TarWriter def generate_shard(oname, nsamples=10000, ntokens=500, prefix=\"\"): \"\"\"Generate a shard of samples with text. Each sample has a \"__key__\" field and a \"txt.gz\" field. That is, the individual text files are compressed automatically on write. They will be automatically decompressed when read. \"\"\" with wds.TarWriter(oname) as output: for i in range(nsamples): text = generate(100).strip() key = uuid.uuid4().hex text = generate(ntokens) sample = {\"__key__\": key, \"txt.gz\": text} output.write(sample) if i % 10 == 0: print(f\"{i:6d} {prefix}:\", repr(text)[:60]) generate_shard(\"temp.tar\", nsamples=10, ntokens=10) !ls -l temp.tar !tar tf temp.tar | head -5 # We need a couple of simple functions to upload to the cloud. def cloud_exists(oname): \"\"\"Check whether a file exists in the cloud.\"\"\" # return os.system(f\"gsutil stat gs://mybucket/500tokens/{oname}\") == 0 return True def cloud_upload(oname): \"\"\"Upload a file to the cloud.\"\"\" # assert os.system(f\"gsutil cp {oname} gs://mybucket/500tokens/{oname}\") == 0 pass # We can now generate a shard and upload it to the cloud. # We skip the generation if the file already exists in the cloud. def generate_and_upload(i): \"\"\"Generate a shard and upload it to the cloud.\"\"\" oname = f\"text-{i:06d}.tar\" if cloud_exists(oname): print(f\"{oname} already exists, skipping\") return False generate_shard(oname, nsamples=nsamples, ntokens=ntokens, prefix=f\"{i:6d} {oname}\") cloud_upload(oname) os.remove(oname) return True # For sequential generation, use this for i in range(nshards): generate_and_upload(i) %%script true # For parallel generation, use this import ray @ray.remote(num_cpus=1, num_gpus=1) def ray_generate_and_upload(i): \"\"\"A Ray remote function that generates a shard and uploads it to the cloud.\"\"\" return generate_and_upload(i) def generate_shards(nshards=10): \"\"\"Generate a number of shards and upload them to the cloud. Runs in parallel on a Ray cluster. \"\"\" ray.init(address='auto') # Connect to the Ray cluster tasks = [ray_generate_and_upload.remote(i) for i in range(nshards)] ray.shutdown() return shard_names","title":"Dataset Generation"},{"location":"examples/webdataset/generate-text-dataset.summary/","text":"The notebook demonstrates the use of the webdataset library for generating and storing synthetic text data in a sharded TAR format, which is suitable for large-scale machine learning tasks. It uses the TarWriter class to create shards containing text samples generated by GPT-2, and includes functions for checking the existence of and uploading these shards to cloud storage. The notebook also shows how to parallelize the shard generation and upload process using Ray.","title":"Generate text dataset.summary"},{"location":"examples/webdataset/mi-images/","text":"Mini Imagenet Generation This is one of a pair of notebooks used for generating an ImageNet-like dataset of training data using stable diffusion models. The difficulty of such artificial datasets can be easily tuned, and they are useful for debugging and testing deep learning applications. The first notebook uses Mistral-7B for taking class labels and generating descriptive prompts for image generation. The prompts are written out as shards to disk and shuffled. The process is parallelized using Ray. The second notebook uses Stable Diffustion to take descriptive prompts/image captions and renders them as image. This is a straightfowrard shard-to-shard transformation. Note that we are using explicit parallelization over shard files in the initial generation and the image generation, while we are using ray.data for the actual shuffling. That is because using explicit parallelization over shards makes it easier to restart jobs that have failed halfway through for some reason. %matplotlib inline import matplotlib.pyplot as plt from pprint import pprint import webdataset as wds from diffusers import AutoPipelineForText2Image import torch import warnings import logging import logging import tqdm from IPython.display import display, clear_output from PIL import Image as PILImage from itertools import islice import glob import os import io from contextlib import contextmanager import sys class SuppressWarning: def __enter__(self): logging.disable(logging.WARNING) def __exit__(self, type, value, traceback): logging.disable(logging.NOTSET) tqdm.tqdm.disable = True def get_num_gpus(): cluster_resources = ray.cluster_resources() return cluster_resources[\"GPU\"] @contextmanager def suppress_outputs(redirect): old_stdout = sys.stdout old_stderr = sys.stderr sys.stdout = redirect sys.stderr = redirect try: yield finally: sys.stdout = old_stdout sys.stderr = old_stderr # parameters odir = \"./mini-imagenet-10\" nactors = -1 check_sufficient = True actor_startup_wait = 10 Transformation Class We encapsulate the rendering into a RenderPrompts class. This class is instantiated once per GPU, loads the model, and then is ready to transform shards. \"\"\" class ShardTransformer: def __init__(self): self.pipe = AutoPipelineForText2Image.from_pretrained( \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\" ).to(\"cuda\") def transform_shard(self, input_shard, output_shard, display_in_notebook=False): ds = wds.WebDataset(input_shard).decode() output = wds.TarWriter(output_shard) for sample in ds: sample = dict(sample) text = sample[\"json\"][\"response\"] with SuppressWarning(): image = self.pipe(prompt=text, num_inference_steps=4, guidance_scale=0.1).images[0] sample[\"jpg\"] = image output.write(sample) if display_in_notebook: clear_output(wait=True) display(image) pprint(text) output.close() \"\"\" def maybe_clear_output(): try: clear_output(wait=True) except: pass class RenderPrompts: def __init__(self, display_in_notebook=False): self.display_in_notebook = display_in_notebook def gpu_is_sufficient(self): return torch.cuda.get_device_properties(0).total_memory > 10**10 def load_model(self): self.pipe = AutoPipelineForText2Image.from_pretrained( \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\" ).to(\"cuda\") def transform_sample(self, sample): text = sample[\"json\"][\"response\"] with SuppressWarning(): image = self.pipe(prompt=text, num_inference_steps=4, guidance_scale=0.1).images[0] sample[\"jpg\"] = image return sample def transform_sample_with_redirect(self, sample): stdout = io.StringIO() with suppress_outputs(stdout): sample = self.transform_sample(sample) sample[\"stdout\"] = stdout.getvalue() return sample def transform_shard(self, input_shard, output_shard, maxcount=999999999): ds = wds.WebDataset(input_shard).decode() output = wds.TarWriter(output_shard+\".temp\") for sample in islice(ds, maxcount): transformed_sample = self.transform_sample_with_redirect(dict(sample)) del transformed_sample[\"stdout\"] maybe_clear_output() output.write(transformed_sample) if self.display_in_notebook: clear_output(wait=True) display(transformed_sample['jpg']) pprint(transformed_sample[\"json\"][\"response\"]) output.close() os.rename(output_shard+\".temp\", output_shard) transformer = RenderPrompts(display_in_notebook=True) transformer.load_model() shards = glob.glob(f\"{odir}/shuffled/*.tar\") transformer.transform_shard(shards[0], \"temp.tar\", maxcount=10) del transformer Parallelization with Ray For parallel rendering, we use a Ray cluster. This will also work on a single machine with just one GPU. import ray if not ray.is_initialized(): ray.init(log_to_driver=False) @ray.remote(num_gpus=1) class RayRenderPrompts(RenderPrompts): def __init__(self): super().__init__() # Start up and create the actor pool. # This tries to adapt to the number of GPUs available. # It also checks that each actor has sufficient memory. # If not, set up your cluster differently by excluding GPUs that are too small. # (Ray's facilities for heterogenous clusters are somewhat limited) ngpus = get_num_gpus() if nactors == -1 else nactors print(f\"using {ngpus} actors\") actors = [RayRenderPrompts.remote() for i in range(int(ngpus))] print(\"loading the models\") for actor in actors: assert ray.get(actor.gpu_is_sufficient.remote()), \"GPU memory insufficient\" ray.get(actor.load_model.remote()) print(\"creating the pool\") pool = ray.util.ActorPool(actors) import glob import os def apply_actor(actor, action): src, dst = action print(f\"START {src} -> {dst}\") result = actor.transform_shard.remote(src, dst) print(f\"DONE {src} -> {dst}\") return result !mkdir -p $odir/images shards = [os.path.basename(p) for p in sorted(glob.glob(f\"{odir}/shuffled/*.tar\"))] actions = [(f\"{odir}/shuffled/{shard}\", f\"{odir}/images/{shard}\") for shard in shards] result = list(pool.map(apply_actor, actions))","title":"Mini Imagenet Generation"},{"location":"examples/webdataset/mi-images/#mini-imagenet-generation","text":"This is one of a pair of notebooks used for generating an ImageNet-like dataset of training data using stable diffusion models. The difficulty of such artificial datasets can be easily tuned, and they are useful for debugging and testing deep learning applications. The first notebook uses Mistral-7B for taking class labels and generating descriptive prompts for image generation. The prompts are written out as shards to disk and shuffled. The process is parallelized using Ray. The second notebook uses Stable Diffustion to take descriptive prompts/image captions and renders them as image. This is a straightfowrard shard-to-shard transformation. Note that we are using explicit parallelization over shard files in the initial generation and the image generation, while we are using ray.data for the actual shuffling. That is because using explicit parallelization over shards makes it easier to restart jobs that have failed halfway through for some reason. %matplotlib inline import matplotlib.pyplot as plt from pprint import pprint import webdataset as wds from diffusers import AutoPipelineForText2Image import torch import warnings import logging import logging import tqdm from IPython.display import display, clear_output from PIL import Image as PILImage from itertools import islice import glob import os import io from contextlib import contextmanager import sys class SuppressWarning: def __enter__(self): logging.disable(logging.WARNING) def __exit__(self, type, value, traceback): logging.disable(logging.NOTSET) tqdm.tqdm.disable = True def get_num_gpus(): cluster_resources = ray.cluster_resources() return cluster_resources[\"GPU\"] @contextmanager def suppress_outputs(redirect): old_stdout = sys.stdout old_stderr = sys.stderr sys.stdout = redirect sys.stderr = redirect try: yield finally: sys.stdout = old_stdout sys.stderr = old_stderr # parameters odir = \"./mini-imagenet-10\" nactors = -1 check_sufficient = True actor_startup_wait = 10","title":"Mini Imagenet Generation"},{"location":"examples/webdataset/mi-images/#transformation-class","text":"We encapsulate the rendering into a RenderPrompts class. This class is instantiated once per GPU, loads the model, and then is ready to transform shards. \"\"\" class ShardTransformer: def __init__(self): self.pipe = AutoPipelineForText2Image.from_pretrained( \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\" ).to(\"cuda\") def transform_shard(self, input_shard, output_shard, display_in_notebook=False): ds = wds.WebDataset(input_shard).decode() output = wds.TarWriter(output_shard) for sample in ds: sample = dict(sample) text = sample[\"json\"][\"response\"] with SuppressWarning(): image = self.pipe(prompt=text, num_inference_steps=4, guidance_scale=0.1).images[0] sample[\"jpg\"] = image output.write(sample) if display_in_notebook: clear_output(wait=True) display(image) pprint(text) output.close() \"\"\" def maybe_clear_output(): try: clear_output(wait=True) except: pass class RenderPrompts: def __init__(self, display_in_notebook=False): self.display_in_notebook = display_in_notebook def gpu_is_sufficient(self): return torch.cuda.get_device_properties(0).total_memory > 10**10 def load_model(self): self.pipe = AutoPipelineForText2Image.from_pretrained( \"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\" ).to(\"cuda\") def transform_sample(self, sample): text = sample[\"json\"][\"response\"] with SuppressWarning(): image = self.pipe(prompt=text, num_inference_steps=4, guidance_scale=0.1).images[0] sample[\"jpg\"] = image return sample def transform_sample_with_redirect(self, sample): stdout = io.StringIO() with suppress_outputs(stdout): sample = self.transform_sample(sample) sample[\"stdout\"] = stdout.getvalue() return sample def transform_shard(self, input_shard, output_shard, maxcount=999999999): ds = wds.WebDataset(input_shard).decode() output = wds.TarWriter(output_shard+\".temp\") for sample in islice(ds, maxcount): transformed_sample = self.transform_sample_with_redirect(dict(sample)) del transformed_sample[\"stdout\"] maybe_clear_output() output.write(transformed_sample) if self.display_in_notebook: clear_output(wait=True) display(transformed_sample['jpg']) pprint(transformed_sample[\"json\"][\"response\"]) output.close() os.rename(output_shard+\".temp\", output_shard) transformer = RenderPrompts(display_in_notebook=True) transformer.load_model() shards = glob.glob(f\"{odir}/shuffled/*.tar\") transformer.transform_shard(shards[0], \"temp.tar\", maxcount=10) del transformer","title":"Transformation Class"},{"location":"examples/webdataset/mi-images/#parallelization-with-ray","text":"For parallel rendering, we use a Ray cluster. This will also work on a single machine with just one GPU. import ray if not ray.is_initialized(): ray.init(log_to_driver=False) @ray.remote(num_gpus=1) class RayRenderPrompts(RenderPrompts): def __init__(self): super().__init__() # Start up and create the actor pool. # This tries to adapt to the number of GPUs available. # It also checks that each actor has sufficient memory. # If not, set up your cluster differently by excluding GPUs that are too small. # (Ray's facilities for heterogenous clusters are somewhat limited) ngpus = get_num_gpus() if nactors == -1 else nactors print(f\"using {ngpus} actors\") actors = [RayRenderPrompts.remote() for i in range(int(ngpus))] print(\"loading the models\") for actor in actors: assert ray.get(actor.gpu_is_sufficient.remote()), \"GPU memory insufficient\" ray.get(actor.load_model.remote()) print(\"creating the pool\") pool = ray.util.ActorPool(actors) import glob import os def apply_actor(actor, action): src, dst = action print(f\"START {src} -> {dst}\") result = actor.transform_shard.remote(src, dst) print(f\"DONE {src} -> {dst}\") return result !mkdir -p $odir/images shards = [os.path.basename(p) for p in sorted(glob.glob(f\"{odir}/shuffled/*.tar\"))] actions = [(f\"{odir}/shuffled/{shard}\", f\"{odir}/images/{shard}\") for shard in shards] result = list(pool.map(apply_actor, actions))","title":"Parallelization with Ray"},{"location":"examples/webdataset/mi-images.summary/","text":"The notebook demonstrates the use of the webdataset library to create a mini ImageNet-like dataset by generating images from textual prompts using a stable diffusion model. It showcases the WebDataset class for reading and writing dataset shards and the TarWriter class for outputting transformed data. The notebook also illustrates parallel processing with Ray to distribute the image rendering workload across multiple GPUs, utilizing the ActorPool class for managing a pool of actors.","title":"Mi images.summary"},{"location":"examples/webdataset/mi-prompts/","text":"Mini Imagenet Generation This is one of a pair of notebooks used for generating an ImageNet-like dataset of training data using stable diffusion models. The difficulty of such artificial datasets can be easily tuned, and they are useful for debugging and testing deep learning applications. The first notebook uses Mistral-7B for taking class labels and generating descriptive prompts for image generation. The prompts are written out as shards to disk and shuffled. The process is parallelized using Ray. The second notebook uses Stable Diffustion to take descriptive prompts/image captions and renders them as image. This is a straightfowrard shard-to-shard transformation. Note that we are using explicit parallelization over shard files in the initial generation and the image generation, while we are using ray.data for the actual shuffling. That is because using explicit parallelization over shards makes it easier to restart jobs that have failed halfway through for some reason. import itertools, random, uuid from pprint import pprint import os import torch import webdataset as wds from transformers import AutoModelForCausalLM, AutoTokenizer from webdataset import filters import textwrap import time from transformers import AutoTokenizer, AutoModelForCausalLM import torch from typing import List def take(n, iterable): \"\"\"Return first n items of the iterable as a list\"\"\" return list(itertools.islice(iterable, n)) def get_gpu_memories(): memory = [] if torch.cuda.is_available(): for i in range(torch.cuda.device_count()): memory.append(torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(i)) return memory def get_num_gpus(): cluster_resources = ray.cluster_resources() return cluster_resources[\"GPU\"] def ray_get(future, timeout=0.1): ready, not_ready = ray.wait([future], timeout=timeout) if not_ready: raise TimeoutError() return ray.get(future) def is_ready(actor, timeout=0.1): ready, not_ready = ray.wait([actor], timeout=timeout) if not_ready: return False return True def select_or_delete(actors, predicate): result = [] for actor in actors: if predicate(actor): result.append(actor) else: del actor return result # parameters # number of classes, must be 10, 100, or 1000 nclasses = 10 # number of images per shard nimages = 100 # number of prompts generated at once per class ngenerated = 20 # number of training shards nshards = 1281 # number of validation shards nvalshards = 50 # output directory odir = f\"./mini-imagenet-{nclasses}\" # output file prefix oprefix = f\"mi{nclasses}\" # number of actors to use, -1 for =number of GPUs nactors = -1 # check that each actor has sufficient memory check_sufficient = True # seconds to wait for actors to start up actor_startup_wait = 10 !echo \"odir=$odir\" !mkdir -p $odir if nclasses == 10: imagenet_classes = \"dog cat car plane bird fish frog horse sheep truck\".split() elif nclasses == 100: imagenet_classes = sorted(list(set(\"\"\" 3d_printer aircraft_carrier airplane apple backpack banana baseball_bat baseball_glove bat bear bed bench bird book bottle bowl broccoli cake camel car carrot cat cell_phone chair clock cloud couch cup dining_table dog donut elephant fire fish fork fox frisbee frog giraffe hair_drier handbag horse hot_dog hydrant kangaroo keyboard kite knife lamp laptop lion meteor microwave monitor monkey mouse mushroom octopus orange oven palm_tree panda parking_meter pear pizza plane potted_plant refrigerator remote rocket sandwich scissors sheep sink skateboard skis snowboard spoon sports_ball stop_sign street_sign suitcase surfboard sweet_pepper table teddy_bear telephone tennis_racket tie tiger toaster toilet toothbrush tree truck tv umbrella vase wine_glass zebra \"\"\".split()))) elif nclasses == 1000: imagenet_classes = open(\"imagenet1000.txt\").read().split() else: raise ValueError(f\"invalid number of classes: {nclasses}, must be 10, 100, or 1000\") assert len(imagenet_classes) == nclasses Generation Classes We encapsulate the model and the generation in a low level and high level class. We can then instantiate those classes once per GPU and call them to generate the shards. class TextGenerationModel: def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.1\", temperature: float = 2.0, top_p: float = 0.9, top_k: int = 10, max_length: int = 96, num_return_sequences: int = 10): \"\"\" Initialize the text generation model. Args: model_name: The name of the pretrained model. temperature: The temperature for the generation process. top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. top_k: The number of highest probability vocabulary tokens to keep for top-k filtering. max_length: The maximum length of the sequence to be generated. num_return_sequences: The number of independently computed returned sequences for each element in the batch. \"\"\" # Load the tokenizer and model self.tokenizer = AutoTokenizer.from_pretrained( model_name, padding_side=\"left\", ) self.tokenizer.pad_token = self.tokenizer.eos_token self.model = AutoModelForCausalLM.from_pretrained(model_name) # Ensure the model is on GPU self.model.to(\"cuda\").half() # Set generation parameters self.temperature = temperature self.top_p = top_p self.top_k = top_k self.max_length = max_length self.num_return_sequences = num_return_sequences def generate_responses(self, texts: List[str]) -> List[str]: \"\"\" Generate responses for the given texts. Args: texts: A list of texts to generate responses for. Returns: A list of generated responses. \"\"\" # Prepare the inputs inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length).to(\"cuda\") # Generate responses with torch.no_grad(): outputs = self.model.generate( input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, do_sample=True, temperature=self.temperature, top_p=self.top_p, top_k=self.top_k, max_length=self.max_length, num_return_sequences=self.num_return_sequences, ) # Decode the responses responses = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs] return responses from typing import Dict, List, Iterator class CaptionGenerator: def __init__(self): self.template = \"[INST] Generate a random, detailed visual caption/description of a photo showing: {object}. [/INST]\" def load_model(self): self.model = TextGenerationModel() def gpu_is_sufficient(self): gpu_memories = get_gpu_memories() assert len(gpu_memories) == 1, \"more than one GPU allocated to actor???\" return gpu_memories[0] / 1e9 > 32.0 def process_batch(self, batch: List[Dict], trim: bool = True) -> List[Dict]: \"\"\"Process a batch of samples, generating responses for each.\"\"\" n = len(batch) texts = [batch[i][\"text\"] for i in range(n)] responses = self.model.generate_responses(texts) if trim: responses = [response.split(\"[/INST]\")[-1].strip() for response in responses] responses = [responses[i : i + self.model.num_return_sequences] for i in range(0, len(responses), self.model.num_return_sequences)] for i in range(n): batch[i][\"responses\"] = responses[i] return batch def process_list_by_batches(self, samples: List[Dict], batch_size: int = 1) -> Iterator[Dict]: \"\"\"Process a list of samples by batches.\"\"\" samples_iter = iter(samples) while True: batch = take(batch_size, samples_iter) if not batch: break responses = self.process_batch(batch) yield from responses def make_samples(self, n: int) -> Iterator[Dict]: \"\"\"Generate a list of samples.\"\"\" for i in range(n): cls = random.randrange(len(imagenet_classes)) object = imagenet_classes[cls] yield dict(cls=cls, object=object, text=self.template.format(object=object)) def make_captions(self, samples: List[Dict]) -> Iterator[Dict]: \"\"\"Generate captions for a list of samples.\"\"\" for sample in self.process_list_by_batches(samples): for response in sample[\"responses\"]: yield dict( cls=sample[\"cls\"], object=sample[\"object\"], text=sample[\"text\"], response=response, ) def make_shard(self, output: str, n: int, k: int = 5): \"\"\" Generate a shard of samples with generated captions. Args: output: The output file to write the shard to. n: The number of samples to generate in the shard. k: The number of return sequences for each sample. \"\"\" if os.path.exists(output): return self.model.num_return_sequences = k writer = wds.TarWriter(output+\".temp\") captions = self.make_captions(self.make_samples(n // k + k)) for caption in itertools.islice(captions, n): sample = dict( __key__=uuid.uuid4().hex, json=caption, ) writer.write(sample) os.rename(output+\".temp\", output) Parallelization with Ra For parallel generation, we use a Ray cluster. This will also do the right thing with a single machine/single GPU setup. It automatically scales up. import ray if not ray.is_initialized(): ray.init() @ray.remote(num_gpus=1) class RayCaptionGenerator(CaptionGenerator): def __init__(self): super().__init__() # Start up and create the actor pool. # This tries to adapt to the number of GPUs available. # It also checks that each actor has sufficient memory. # If not, set up your cluster differently by excluding GPUs that are too small. # (Ray's facilities for heterogenous clusters are somewhat limited) ngpus = get_num_gpus() if nactors == -1 else nactors print(f\"using {ngpus} actors\") actors = [RayCaptionGenerator.remote() for i in range(int(ngpus))] print(\"loading the models\") for actor in actors: assert ray.get(actor.gpu_is_sufficient.remote()), \"GPU memory insufficient\" ray.get(actor.load_model.remote()) print(\"creating the pool\") pool = ray.util.ActorPool(actors) # It would be nice if there were a .map_with_actors method in pool, # but there isn't, so we use this workaround. def apply_actor(actor, dest): return actor.make_shard.remote(dest, nimages, ngenerated) !mkdir -p $odir/prompts # Perform the actual shard generation. dests = [f\"{odir}/prompts/prompts-{i:06d}.tar\" for i in range(nshards + nvalshards)] result = list(pool.map(apply_actor, dests)) del actors del pool Shuffle For shuffling the dataset, we use the ray.data read_webdataset and write_webdataset functions. import ray from ray.data import read_webdataset import glob !mkdir -p $odir/shuffled !rm -f $odir/shuffled/* shards = glob.glob(f\"{odir}/prompts/prompts-*.tar\") dataset = read_webdataset(shards) shuffled_dataset = dataset.random_shuffle() shuffled_dataset.repartition(len(shards)).write_webdataset(f\"{odir}/shuffled/\") # The output of write_webdataset is a directory of shards, but not following # the usual naming conventions. We rename the shards to follow typical # webdataset conventions. import glob import os shuffled = sorted(glob.glob(f\"{odir}/shuffled/*.tar\")) for i in range(nshards): os.rename(shuffled[i], f\"{odir}/shuffled/{oprefix}-{i:06d}.tar\") for i in range(nvalshards): os.rename(shuffled[nshards+i], f\"{odir}/shuffled/{oprefix}-val-{i:06d}.tar\")","title":"Mini Imagenet Generation"},{"location":"examples/webdataset/mi-prompts/#mini-imagenet-generation","text":"This is one of a pair of notebooks used for generating an ImageNet-like dataset of training data using stable diffusion models. The difficulty of such artificial datasets can be easily tuned, and they are useful for debugging and testing deep learning applications. The first notebook uses Mistral-7B for taking class labels and generating descriptive prompts for image generation. The prompts are written out as shards to disk and shuffled. The process is parallelized using Ray. The second notebook uses Stable Diffustion to take descriptive prompts/image captions and renders them as image. This is a straightfowrard shard-to-shard transformation. Note that we are using explicit parallelization over shard files in the initial generation and the image generation, while we are using ray.data for the actual shuffling. That is because using explicit parallelization over shards makes it easier to restart jobs that have failed halfway through for some reason. import itertools, random, uuid from pprint import pprint import os import torch import webdataset as wds from transformers import AutoModelForCausalLM, AutoTokenizer from webdataset import filters import textwrap import time from transformers import AutoTokenizer, AutoModelForCausalLM import torch from typing import List def take(n, iterable): \"\"\"Return first n items of the iterable as a list\"\"\" return list(itertools.islice(iterable, n)) def get_gpu_memories(): memory = [] if torch.cuda.is_available(): for i in range(torch.cuda.device_count()): memory.append(torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(i)) return memory def get_num_gpus(): cluster_resources = ray.cluster_resources() return cluster_resources[\"GPU\"] def ray_get(future, timeout=0.1): ready, not_ready = ray.wait([future], timeout=timeout) if not_ready: raise TimeoutError() return ray.get(future) def is_ready(actor, timeout=0.1): ready, not_ready = ray.wait([actor], timeout=timeout) if not_ready: return False return True def select_or_delete(actors, predicate): result = [] for actor in actors: if predicate(actor): result.append(actor) else: del actor return result # parameters # number of classes, must be 10, 100, or 1000 nclasses = 10 # number of images per shard nimages = 100 # number of prompts generated at once per class ngenerated = 20 # number of training shards nshards = 1281 # number of validation shards nvalshards = 50 # output directory odir = f\"./mini-imagenet-{nclasses}\" # output file prefix oprefix = f\"mi{nclasses}\" # number of actors to use, -1 for =number of GPUs nactors = -1 # check that each actor has sufficient memory check_sufficient = True # seconds to wait for actors to start up actor_startup_wait = 10 !echo \"odir=$odir\" !mkdir -p $odir if nclasses == 10: imagenet_classes = \"dog cat car plane bird fish frog horse sheep truck\".split() elif nclasses == 100: imagenet_classes = sorted(list(set(\"\"\" 3d_printer aircraft_carrier airplane apple backpack banana baseball_bat baseball_glove bat bear bed bench bird book bottle bowl broccoli cake camel car carrot cat cell_phone chair clock cloud couch cup dining_table dog donut elephant fire fish fork fox frisbee frog giraffe hair_drier handbag horse hot_dog hydrant kangaroo keyboard kite knife lamp laptop lion meteor microwave monitor monkey mouse mushroom octopus orange oven palm_tree panda parking_meter pear pizza plane potted_plant refrigerator remote rocket sandwich scissors sheep sink skateboard skis snowboard spoon sports_ball stop_sign street_sign suitcase surfboard sweet_pepper table teddy_bear telephone tennis_racket tie tiger toaster toilet toothbrush tree truck tv umbrella vase wine_glass zebra \"\"\".split()))) elif nclasses == 1000: imagenet_classes = open(\"imagenet1000.txt\").read().split() else: raise ValueError(f\"invalid number of classes: {nclasses}, must be 10, 100, or 1000\") assert len(imagenet_classes) == nclasses","title":"Mini Imagenet Generation"},{"location":"examples/webdataset/mi-prompts/#generation-classes","text":"We encapsulate the model and the generation in a low level and high level class. We can then instantiate those classes once per GPU and call them to generate the shards. class TextGenerationModel: def __init__(self, model_name: str = \"mistralai/Mistral-7B-Instruct-v0.1\", temperature: float = 2.0, top_p: float = 0.9, top_k: int = 10, max_length: int = 96, num_return_sequences: int = 10): \"\"\" Initialize the text generation model. Args: model_name: The name of the pretrained model. temperature: The temperature for the generation process. top_p: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. top_k: The number of highest probability vocabulary tokens to keep for top-k filtering. max_length: The maximum length of the sequence to be generated. num_return_sequences: The number of independently computed returned sequences for each element in the batch. \"\"\" # Load the tokenizer and model self.tokenizer = AutoTokenizer.from_pretrained( model_name, padding_side=\"left\", ) self.tokenizer.pad_token = self.tokenizer.eos_token self.model = AutoModelForCausalLM.from_pretrained(model_name) # Ensure the model is on GPU self.model.to(\"cuda\").half() # Set generation parameters self.temperature = temperature self.top_p = top_p self.top_k = top_k self.max_length = max_length self.num_return_sequences = num_return_sequences def generate_responses(self, texts: List[str]) -> List[str]: \"\"\" Generate responses for the given texts. Args: texts: A list of texts to generate responses for. Returns: A list of generated responses. \"\"\" # Prepare the inputs inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length).to(\"cuda\") # Generate responses with torch.no_grad(): outputs = self.model.generate( input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, do_sample=True, temperature=self.temperature, top_p=self.top_p, top_k=self.top_k, max_length=self.max_length, num_return_sequences=self.num_return_sequences, ) # Decode the responses responses = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs] return responses from typing import Dict, List, Iterator class CaptionGenerator: def __init__(self): self.template = \"[INST] Generate a random, detailed visual caption/description of a photo showing: {object}. [/INST]\" def load_model(self): self.model = TextGenerationModel() def gpu_is_sufficient(self): gpu_memories = get_gpu_memories() assert len(gpu_memories) == 1, \"more than one GPU allocated to actor???\" return gpu_memories[0] / 1e9 > 32.0 def process_batch(self, batch: List[Dict], trim: bool = True) -> List[Dict]: \"\"\"Process a batch of samples, generating responses for each.\"\"\" n = len(batch) texts = [batch[i][\"text\"] for i in range(n)] responses = self.model.generate_responses(texts) if trim: responses = [response.split(\"[/INST]\")[-1].strip() for response in responses] responses = [responses[i : i + self.model.num_return_sequences] for i in range(0, len(responses), self.model.num_return_sequences)] for i in range(n): batch[i][\"responses\"] = responses[i] return batch def process_list_by_batches(self, samples: List[Dict], batch_size: int = 1) -> Iterator[Dict]: \"\"\"Process a list of samples by batches.\"\"\" samples_iter = iter(samples) while True: batch = take(batch_size, samples_iter) if not batch: break responses = self.process_batch(batch) yield from responses def make_samples(self, n: int) -> Iterator[Dict]: \"\"\"Generate a list of samples.\"\"\" for i in range(n): cls = random.randrange(len(imagenet_classes)) object = imagenet_classes[cls] yield dict(cls=cls, object=object, text=self.template.format(object=object)) def make_captions(self, samples: List[Dict]) -> Iterator[Dict]: \"\"\"Generate captions for a list of samples.\"\"\" for sample in self.process_list_by_batches(samples): for response in sample[\"responses\"]: yield dict( cls=sample[\"cls\"], object=sample[\"object\"], text=sample[\"text\"], response=response, ) def make_shard(self, output: str, n: int, k: int = 5): \"\"\" Generate a shard of samples with generated captions. Args: output: The output file to write the shard to. n: The number of samples to generate in the shard. k: The number of return sequences for each sample. \"\"\" if os.path.exists(output): return self.model.num_return_sequences = k writer = wds.TarWriter(output+\".temp\") captions = self.make_captions(self.make_samples(n // k + k)) for caption in itertools.islice(captions, n): sample = dict( __key__=uuid.uuid4().hex, json=caption, ) writer.write(sample) os.rename(output+\".temp\", output)","title":"Generation Classes"},{"location":"examples/webdataset/mi-prompts/#parallelization-with-ra","text":"For parallel generation, we use a Ray cluster. This will also do the right thing with a single machine/single GPU setup. It automatically scales up. import ray if not ray.is_initialized(): ray.init() @ray.remote(num_gpus=1) class RayCaptionGenerator(CaptionGenerator): def __init__(self): super().__init__() # Start up and create the actor pool. # This tries to adapt to the number of GPUs available. # It also checks that each actor has sufficient memory. # If not, set up your cluster differently by excluding GPUs that are too small. # (Ray's facilities for heterogenous clusters are somewhat limited) ngpus = get_num_gpus() if nactors == -1 else nactors print(f\"using {ngpus} actors\") actors = [RayCaptionGenerator.remote() for i in range(int(ngpus))] print(\"loading the models\") for actor in actors: assert ray.get(actor.gpu_is_sufficient.remote()), \"GPU memory insufficient\" ray.get(actor.load_model.remote()) print(\"creating the pool\") pool = ray.util.ActorPool(actors) # It would be nice if there were a .map_with_actors method in pool, # but there isn't, so we use this workaround. def apply_actor(actor, dest): return actor.make_shard.remote(dest, nimages, ngenerated) !mkdir -p $odir/prompts # Perform the actual shard generation. dests = [f\"{odir}/prompts/prompts-{i:06d}.tar\" for i in range(nshards + nvalshards)] result = list(pool.map(apply_actor, dests)) del actors del pool","title":"Parallelization with Ra"},{"location":"examples/webdataset/mi-prompts/#shuffle","text":"For shuffling the dataset, we use the ray.data read_webdataset and write_webdataset functions. import ray from ray.data import read_webdataset import glob !mkdir -p $odir/shuffled !rm -f $odir/shuffled/* shards = glob.glob(f\"{odir}/prompts/prompts-*.tar\") dataset = read_webdataset(shards) shuffled_dataset = dataset.random_shuffle() shuffled_dataset.repartition(len(shards)).write_webdataset(f\"{odir}/shuffled/\") # The output of write_webdataset is a directory of shards, but not following # the usual naming conventions. We rename the shards to follow typical # webdataset conventions. import glob import os shuffled = sorted(glob.glob(f\"{odir}/shuffled/*.tar\")) for i in range(nshards): os.rename(shuffled[i], f\"{odir}/shuffled/{oprefix}-{i:06d}.tar\") for i in range(nvalshards): os.rename(shuffled[nshards+i], f\"{odir}/shuffled/{oprefix}-val-{i:06d}.tar\")","title":"Shuffle"},{"location":"examples/webdataset/mi-prompts.summary/","text":"The notebook outlines the process of generating a Mini ImageNet dataset using a text generation model ( TextGenerationModel ) and a caption generator ( CaptionGenerator ). It utilizes the webdataset library to handle the dataset creation and shuffling, specifically using classes like TarWriter for writing shards and read_webdataset and write_webdataset functions for reading and shuffling the dataset shards. The notebook demonstrates parallelization with Ray to scale up the generation process across multiple GPUs and the use of webdataset for managing large-scale image-caption datasets.","title":"Mi prompts.summary"},{"location":"examples/webdataset/tesseract-wds/","text":"# imports and helper functions import os import sys import webdataset as wds import braceexpand import tempfile import glob from itertools import islice import random def summarize(sample): for k, v in sample.items(): print(k, repr(v)[:100]) def read_binary(fname): with open(fname, \"rb\") as stream: return stream.read() Parallel Processing of Shards: Large Scale OCR This notebook illustrates how to take a large collection of shards consisting of PDFs and process them using pdftoppm and tessearact into a new dataset consisting of page images and corresponding OCR output. The general approach is to process each shard sequentially and to process multiple shards in parallel. The basic structure of such a job looks like: with WebDataset(srcname) as src: with TarWriter(dstname) as dst: for sample in src: ... do something with sample ... dst.write(sample) upload(dstname) The Arxiv Dataset of PDFs # The dataset is tar files containing PDFs, each using the Arxiv naming convention. !gsutil cat gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar | tar tf - | sed 5q # Arxiv naming convenitions are incompatible with WebDataset, but we can add # a file renaming function to the WebDataset to fix this. def arxiv_rename(name): return name.replace(\".pdf\", \"\").replace(\".\", \"_\") + \".pdf\" # For this example, we just use two shards, but usually, you would have hundreds # or thousands of shards. dataset = \"gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar\" # Let's open the dataset and read the first sample. shardurls = list(braceexpand.braceexpand(dataset)) ds = wds.WebDataset(shardurls, rename_files=arxiv_rename) sample = next(iter(ds)) summarize(sample) Running Tesseract on a Single PDF def process_sample(sample, maxpages=9999, shuffle=True): \"\"\"Process a sample from the Arxiv dataset. This function converts the PDF file to a sequence of JPEG images and then invokes Tesseract to recognize the text in the images. It returns a sequence of samples, one per page, each containing the JPEG image and the hOCR output from Tesseract. \"\"\" # We work in a temporary directory; most operations are command line tools with tempfile.TemporaryDirectory() as dirname: # Write the PDF file to disk and convert it to a sequence of JPEGs using pdftoppm pdfpath = dirname + \"/sample.pdf\" with open(pdfpath, \"wb\") as stream: stream.write(sample[\"pdf\"]) assert os.system(f\"(cd {dirname} && pdftoppm -forcenum -jpeg -r 300 -l 9999 sample.pdf page)\") == 0 # Next, we are going to iterate over the pages, convert them to text using tesseract, pages = sorted(glob.glob(dirname + \"/page-*.jpg\")) if shuffle: random.shuffle(pages) for page in islice(pages, maxpages): page_without_suffix = page[:-4] base = os.path.basename(page_without_suffix) # Invoke Tesseract to convert the page image to hOCR. os.system(f\"tesseract {page} {page_without_suffix} hocr\") # Construct the output sample. nsample = { \"__key__\": sample[\"__key__\"] + f\"/{base}\", \"jpg\": read_binary(page_without_suffix + \".jpg\"), \"hocr\": read_binary(page_without_suffix + \".hocr\"), } # This function returns an iterator over the recognized pages. yield nsample output = next(process_sample(sample)) summarize(output) Processing a Shard of PDF Files def process_shard(src, dst, maxpdfs=999999, maxpages=9999): \"\"\"Process a shard of the Arxiv dataset. This function reads a shard of the Arxiv dataset, processes each sample using the process_sample function, and writes the page images and corresponding hOCR output to a new shard, one sample per page. The maxpdfs and maxpages parameters can be used to limit the number of samples and pages processed. This is useful for testing, as well as limit the number of pages selected from very long PDF documents. \"\"\" with wds.TarWriter(dst) as sink: for sample in islice(wds.WebDataset(src, rename_files=arxiv_rename), maxpdfs): print(sample[\"__key__\"], sample.keys()) for nsample in process_sample(sample, maxpages=maxpages): print(\" \", nsample[\"__key__\"]) sink.write(nsample) !rm -f output.tar process_shard(shardurls[0], \"output.tar\", maxpdfs=2, maxpages=2) !tar tvf output.tar Parallelizing Processing with Ray This illustrates how to use Ray to process many shards in parallel. You don't need to use Ray for this, you can also invoke process_shard in parallel using a job queueing system or using some other distributed computing framework. Generally, it is easiest to process each shard sequentially, and to process multiple shards in parallel. However, you could use additional parallelization to perform processing of the samples in parallel. maxpdfs = 2 # for testing, we just use two PDFs per shard maxpages = 2 # for testing, we just use two pages per PDF upload_cmd = \"echo gsutil cp {src} {dst}\" # for testing, we don't actually upload the completed shards import ray if not ray.is_initialized(): ray.init() @ray.remote(num_cpus=4) def process_shard_parallel(src, dstbucket, maxpdfs=999999, maxpages=9999): \"\"\"Process a shard of the Arxiv dataset and upload the output shard to a bucket. This function reads a shard of the Arxiv dataset, processes each sample using the process_sample function, and writes the page images and corresponding hOCR output to a new shard, one sample per page. The output shard is then uploaded to the specified bucket using `upload_cmd`. \"\"\" dst = dstbucket + \"/\" + os.path.basename(src) with tempfile.NamedTemporaryFile() as tmp: process_shard(src, tmp.name, maxpdfs=maxpdfs, maxpages=maxpages) assert os.system(upload_cmd.format(src=tmp.name, dst=dst)) == 0 !rm -f output.tar ray.get([process_shard_parallel.remote(src, \"gs://somebucket\", maxpdfs=maxpdfs, maxpages=maxpages) for src in shardurls])","title":"Tesseract wds"},{"location":"examples/webdataset/tesseract-wds/#parallel-processing-of-shards-large-scale-ocr","text":"This notebook illustrates how to take a large collection of shards consisting of PDFs and process them using pdftoppm and tessearact into a new dataset consisting of page images and corresponding OCR output. The general approach is to process each shard sequentially and to process multiple shards in parallel. The basic structure of such a job looks like: with WebDataset(srcname) as src: with TarWriter(dstname) as dst: for sample in src: ... do something with sample ... dst.write(sample) upload(dstname)","title":"Parallel Processing of Shards: Large Scale OCR"},{"location":"examples/webdataset/tesseract-wds/#the-arxiv-dataset-of-pdfs","text":"# The dataset is tar files containing PDFs, each using the Arxiv naming convention. !gsutil cat gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar | tar tf - | sed 5q # Arxiv naming convenitions are incompatible with WebDataset, but we can add # a file renaming function to the WebDataset to fix this. def arxiv_rename(name): return name.replace(\".pdf\", \"\").replace(\".\", \"_\") + \".pdf\" # For this example, we just use two shards, but usually, you would have hundreds # or thousands of shards. dataset = \"gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar\" # Let's open the dataset and read the first sample. shardurls = list(braceexpand.braceexpand(dataset)) ds = wds.WebDataset(shardurls, rename_files=arxiv_rename) sample = next(iter(ds)) summarize(sample)","title":"The Arxiv Dataset of PDFs"},{"location":"examples/webdataset/tesseract-wds/#running-tesseract-on-a-single-pdf","text":"def process_sample(sample, maxpages=9999, shuffle=True): \"\"\"Process a sample from the Arxiv dataset. This function converts the PDF file to a sequence of JPEG images and then invokes Tesseract to recognize the text in the images. It returns a sequence of samples, one per page, each containing the JPEG image and the hOCR output from Tesseract. \"\"\" # We work in a temporary directory; most operations are command line tools with tempfile.TemporaryDirectory() as dirname: # Write the PDF file to disk and convert it to a sequence of JPEGs using pdftoppm pdfpath = dirname + \"/sample.pdf\" with open(pdfpath, \"wb\") as stream: stream.write(sample[\"pdf\"]) assert os.system(f\"(cd {dirname} && pdftoppm -forcenum -jpeg -r 300 -l 9999 sample.pdf page)\") == 0 # Next, we are going to iterate over the pages, convert them to text using tesseract, pages = sorted(glob.glob(dirname + \"/page-*.jpg\")) if shuffle: random.shuffle(pages) for page in islice(pages, maxpages): page_without_suffix = page[:-4] base = os.path.basename(page_without_suffix) # Invoke Tesseract to convert the page image to hOCR. os.system(f\"tesseract {page} {page_without_suffix} hocr\") # Construct the output sample. nsample = { \"__key__\": sample[\"__key__\"] + f\"/{base}\", \"jpg\": read_binary(page_without_suffix + \".jpg\"), \"hocr\": read_binary(page_without_suffix + \".hocr\"), } # This function returns an iterator over the recognized pages. yield nsample output = next(process_sample(sample)) summarize(output)","title":"Running Tesseract on a Single PDF"},{"location":"examples/webdataset/tesseract-wds/#processing-a-shard-of-pdf-files","text":"def process_shard(src, dst, maxpdfs=999999, maxpages=9999): \"\"\"Process a shard of the Arxiv dataset. This function reads a shard of the Arxiv dataset, processes each sample using the process_sample function, and writes the page images and corresponding hOCR output to a new shard, one sample per page. The maxpdfs and maxpages parameters can be used to limit the number of samples and pages processed. This is useful for testing, as well as limit the number of pages selected from very long PDF documents. \"\"\" with wds.TarWriter(dst) as sink: for sample in islice(wds.WebDataset(src, rename_files=arxiv_rename), maxpdfs): print(sample[\"__key__\"], sample.keys()) for nsample in process_sample(sample, maxpages=maxpages): print(\" \", nsample[\"__key__\"]) sink.write(nsample) !rm -f output.tar process_shard(shardurls[0], \"output.tar\", maxpdfs=2, maxpages=2) !tar tvf output.tar","title":"Processing a Shard of PDF Files"},{"location":"examples/webdataset/tesseract-wds/#parallelizing-processing-with-ray","text":"This illustrates how to use Ray to process many shards in parallel. You don't need to use Ray for this, you can also invoke process_shard in parallel using a job queueing system or using some other distributed computing framework. Generally, it is easiest to process each shard sequentially, and to process multiple shards in parallel. However, you could use additional parallelization to perform processing of the samples in parallel. maxpdfs = 2 # for testing, we just use two PDFs per shard maxpages = 2 # for testing, we just use two pages per PDF upload_cmd = \"echo gsutil cp {src} {dst}\" # for testing, we don't actually upload the completed shards import ray if not ray.is_initialized(): ray.init() @ray.remote(num_cpus=4) def process_shard_parallel(src, dstbucket, maxpdfs=999999, maxpages=9999): \"\"\"Process a shard of the Arxiv dataset and upload the output shard to a bucket. This function reads a shard of the Arxiv dataset, processes each sample using the process_sample function, and writes the page images and corresponding hOCR output to a new shard, one sample per page. The output shard is then uploaded to the specified bucket using `upload_cmd`. \"\"\" dst = dstbucket + \"/\" + os.path.basename(src) with tempfile.NamedTemporaryFile() as tmp: process_shard(src, tmp.name, maxpdfs=maxpdfs, maxpages=maxpages) assert os.system(upload_cmd.format(src=tmp.name, dst=dst)) == 0 !rm -f output.tar ray.get([process_shard_parallel.remote(src, \"gs://somebucket\", maxpdfs=maxpdfs, maxpages=maxpages) for src in shardurls])","title":"Parallelizing Processing with Ray"},{"location":"examples/webdataset/tesseract-wds.summary/","text":"The notebook demonstrates how to process a large dataset of PDF files stored in shards using the webdataset library. It illustrates the conversion of PDFs to JPEG images and the subsequent OCR processing with Tesseract, showcasing the use of WebDataset for reading and TarWriter for writing datasets. The notebook also exemplifies parallel processing of shards with Ray, highlighting the library's capability to handle large-scale OCR tasks efficiently.","title":"Tesseract wds.summary"},{"location":"examples/webdataset/train-ocr-errors-hf/","text":"Fine Tuning LLM with Huggingface and WebDataset This notebook illustrates the use of WebDataset together with Huggingface for fine-tuning large language models. Some features of note: training data is loaded directly from Huggingface data is downloaded and stored locally incrementally as needed a custom sampler is used in order to make remote data access more efficient # parameters base_model = \"google/flan-t5-base\" dataset_url = ( \"https://huggingface.co/tmbdev/d-tokens/resolve/main/d-tokens.json?download=true\" ) cache_dir = \"./_cache\" batch_size = 1 max_steps = 10000 epochs = 1 learning_rate = 3e-4 # imports import string import random import numpy as np import regex import unicodedata import logging import torch.utils.data from transformers import AutoTokenizer, AutoModelForSequenceClassification from transformers import AutoModelForSeq2SeqLM from transformers.adapters import LoRAConfig from transformers import TrainingArguments, AdapterTrainer, TrainerCallback # workaround for running this in the source tree, you usually don't need this try: import wids except: sys.path += [\"..\"] import wids def normalize_string(s): \"\"\"Take a string and normalize it. Normalization removes common typographic variants of punctuation characters that would otherwise have to be learned explicitly by the model. It also simplifies whitespace and removes long strings of punctuations used for graphical effect. \"\"\" # start with Unicode normalization s = unicodedata.normalize(\"NFKC\", s) s = regex.sub(r\"[*@`]\", \"\", s) s = regex.sub(r\"[\\u0027\\u2019\\u2018\\u201A\\u201B]\", \"'\", s) # replace all single quotes with ' s = regex.sub(r\"[\\u0022\\u201C\\u201D\\u201E\\u201F]\", '\"', s) # replace all double quotes with \" s = regex.sub(r\"[\\u2013\\u2014\\u2012\\u2015-]\", \"-\", s) # normalize dashes s = regex.sub(r\"(\\p{P})\\p{P}+\", r\"\\1\", s) # remove duplicate punctuation s = regex.sub(r\"[^\\p{L}\\p{N}\\p{Z}().,?!:;'\\\"\\n-]+\", \" \", s) s = regex.sub(r\"[ \\t]+\", \" \", s) s = s.strip() return s # Data augmentation. Actually, in this case, we generate a synthetic training sample from # a clean input string. replacements = list( set(string.ascii_letters + string.digits + \" \" + \"\" + string.punctuation) - set([\"*\"]) ) def degrade(s, prange=(0.05, 0.1), seed=None, special=\"*\"): \"\"\"Generate training samples by degrading a string. Our model is a sequence-to-sequence model that identifies the location of OCR errors in a text string. It is trained on a synthetic dataset that contains pairs of strings, one of which is a degraded string, and the other is the degraded string with errors marked by asterisks. The model is trained to predict the location of the asterisks. \"\"\" seed = random.randint(0, 1000000) if seed is None else seed rng = random.Random(seed) s = normalize_string(s) if len(s) < 2: return s, s for _ in range(100): if rng.random() < 0.5: # use regex to delete the first k words, where k is random between 1 and 2 # we do this because otherwise the model will flag lower case letters at the beginning # of a string as errors k = rng.randint(1, 4) expr = r\"^([^\\p{Z}]+?\\p{Z}+){%d}\" % k s = regex.sub(expr, \"\", s, count=1) if len(s) > 1: break result = \"\" target = \"\" p = rng.uniform(*prange) for c in s: if c == special: continue if c != \"\\n\" and rng.random() < p: r = rng.choice(replacements) result += r target += special else: result += c target += c result = normalize_string(result) return result, target degrade(\"Hello, world's biggest ball-of-yarn!\") # We use Flan T5 as the base model. Other models might work better. tokenizer = AutoTokenizer.from_pretrained(base_model) # This is a helper function that takes a sample, unpacks it, applies the degradation, # and then returns a dictionary with the input_ids and the labels as required by Huggingface. def make_sample(sample, *, prange=(0.05, 0.1), seed=None, prefix=\"ocr-errors: \"): \"\"\"Given a sample consisting of a clean text string, generate a training sample. Args: sample: a sample from the webdataset prange: range of error probability seed: random seed or None for random seed prefix: prefix (prompt) to add to the input stringf \"\"\" clean = sample[\".txt.gz\"] clean = normalize_string(clean) text, target = degrade(clean, prange=prange, seed=seed) text_ids = torch.tensor( tokenizer.encode(prefix + text, max_length=512, truncation=True, padding=\"max_length\") ) target_ids = torch.tensor(tokenizer.encode(target, max_length=512, truncation=True, padding=\"max_length\")) return dict(input_ids=text_ids, labels=target_ids) # This is really all that is WebDataset specific: # - we specify a URL for the JSON index file # - we specify a local cache directory # - we instantiate a ShardListDataset with keep=True # - we add the make_sample transform to the dataset # - we create a custom sampler that respects shard boundaries dataset = wids.ShardListDataset( dataset_url, cache_dir=cache_dir, cache_size=int(1e10), keep=True ) dataset.add_transform(make_sample) dataset[999] sampler = wids.ShardedSampler(dataset) # This plot illustrates the behavior of the shard sampler: it generates a sequence # of samples from each shard in turn, and then moves on to the next shard. import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline plt.subplot(121) plt.plot(list(sampler)[:10000]) plt.subplot(122) plt.plot(list(sampler)[:500]); # Standard Hugginface LoRA setup. # start with the pretrained base model model = AutoModelForSeq2SeqLM.from_pretrained(base_model) # set the parameters for LoRA config = LoRAConfig( r=8, alpha=16, # use it on all of the layers intermediate_lora=True, output_lora=True, ) # make a new adapter for the xerr dataset model.add_adapter(\"xerr\", config=config) # enable the adapter for training model.train_adapter(\"xerr\") model.set_active_adapters([\"xerr\"]) # Standard Huggingface adapter training, except for the custom sampler. training_args = TrainingArguments( learning_rate=learning_rate, num_train_epochs=epochs, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, logging_steps=2000, save_steps=5000, output_dir=\"./training_output\", overwrite_output_dir=True, remove_unused_columns=False, max_steps=max_steps, ) # create the trainer trainer = AdapterTrainer( model=model, args=training_args, tokenizer=tokenizer, train_dataset=dataset, # eval_dataset=OCRDataset(\"test\", maxsize=100), ) # to set the sampler, we override the get_train_sampler method # Huggingface doesn't provide a better way to do this trainer._get_train_sampler = lambda: sampler # Run the bulk of the training. trainer.train() # Show some examples (this isn't really \"validation\"). num_validation = 10 validation_dataset = dataset logging.getLogger(\"transformers\").setLevel(logging.ERROR) for i in range(num_validation): # load the input and label (note: we get a different degradation each time) sample = validation_dataset[i] # convert the input and label to tensors input_ids = sample[\"input_ids\"].unsqueeze(0).to(0) label_ids = sample[\"labels\"].unsqueeze(0).to(0) # use the model to generate the output output = model.generate(input_ids, max_length=1024) # convert the tokens to text input_text = ( tokenizer.decode(input_ids[0], skip_special_tokens=True) .replace(\"ocr-errors:\", \"\") .strip() ) output_text = tokenizer.decode(output[0], skip_special_tokens=True).strip() label_text = tokenizer.decode(label_ids[0], skip_special_tokens=True).strip() print(f\"[{i}]\") print(\"Input: \", input_text) print(\"Output:\", output_text) print(\"Label: \", label_text) print(\"---\")","title":"Fine Tuning LLM with Huggingface and WebDataset"},{"location":"examples/webdataset/train-ocr-errors-hf/#fine-tuning-llm-with-huggingface-and-webdataset","text":"This notebook illustrates the use of WebDataset together with Huggingface for fine-tuning large language models. Some features of note: training data is loaded directly from Huggingface data is downloaded and stored locally incrementally as needed a custom sampler is used in order to make remote data access more efficient # parameters base_model = \"google/flan-t5-base\" dataset_url = ( \"https://huggingface.co/tmbdev/d-tokens/resolve/main/d-tokens.json?download=true\" ) cache_dir = \"./_cache\" batch_size = 1 max_steps = 10000 epochs = 1 learning_rate = 3e-4 # imports import string import random import numpy as np import regex import unicodedata import logging import torch.utils.data from transformers import AutoTokenizer, AutoModelForSequenceClassification from transformers import AutoModelForSeq2SeqLM from transformers.adapters import LoRAConfig from transformers import TrainingArguments, AdapterTrainer, TrainerCallback # workaround for running this in the source tree, you usually don't need this try: import wids except: sys.path += [\"..\"] import wids def normalize_string(s): \"\"\"Take a string and normalize it. Normalization removes common typographic variants of punctuation characters that would otherwise have to be learned explicitly by the model. It also simplifies whitespace and removes long strings of punctuations used for graphical effect. \"\"\" # start with Unicode normalization s = unicodedata.normalize(\"NFKC\", s) s = regex.sub(r\"[*@`]\", \"\", s) s = regex.sub(r\"[\\u0027\\u2019\\u2018\\u201A\\u201B]\", \"'\", s) # replace all single quotes with ' s = regex.sub(r\"[\\u0022\\u201C\\u201D\\u201E\\u201F]\", '\"', s) # replace all double quotes with \" s = regex.sub(r\"[\\u2013\\u2014\\u2012\\u2015-]\", \"-\", s) # normalize dashes s = regex.sub(r\"(\\p{P})\\p{P}+\", r\"\\1\", s) # remove duplicate punctuation s = regex.sub(r\"[^\\p{L}\\p{N}\\p{Z}().,?!:;'\\\"\\n-]+\", \" \", s) s = regex.sub(r\"[ \\t]+\", \" \", s) s = s.strip() return s # Data augmentation. Actually, in this case, we generate a synthetic training sample from # a clean input string. replacements = list( set(string.ascii_letters + string.digits + \" \" + \"\" + string.punctuation) - set([\"*\"]) ) def degrade(s, prange=(0.05, 0.1), seed=None, special=\"*\"): \"\"\"Generate training samples by degrading a string. Our model is a sequence-to-sequence model that identifies the location of OCR errors in a text string. It is trained on a synthetic dataset that contains pairs of strings, one of which is a degraded string, and the other is the degraded string with errors marked by asterisks. The model is trained to predict the location of the asterisks. \"\"\" seed = random.randint(0, 1000000) if seed is None else seed rng = random.Random(seed) s = normalize_string(s) if len(s) < 2: return s, s for _ in range(100): if rng.random() < 0.5: # use regex to delete the first k words, where k is random between 1 and 2 # we do this because otherwise the model will flag lower case letters at the beginning # of a string as errors k = rng.randint(1, 4) expr = r\"^([^\\p{Z}]+?\\p{Z}+){%d}\" % k s = regex.sub(expr, \"\", s, count=1) if len(s) > 1: break result = \"\" target = \"\" p = rng.uniform(*prange) for c in s: if c == special: continue if c != \"\\n\" and rng.random() < p: r = rng.choice(replacements) result += r target += special else: result += c target += c result = normalize_string(result) return result, target degrade(\"Hello, world's biggest ball-of-yarn!\") # We use Flan T5 as the base model. Other models might work better. tokenizer = AutoTokenizer.from_pretrained(base_model) # This is a helper function that takes a sample, unpacks it, applies the degradation, # and then returns a dictionary with the input_ids and the labels as required by Huggingface. def make_sample(sample, *, prange=(0.05, 0.1), seed=None, prefix=\"ocr-errors: \"): \"\"\"Given a sample consisting of a clean text string, generate a training sample. Args: sample: a sample from the webdataset prange: range of error probability seed: random seed or None for random seed prefix: prefix (prompt) to add to the input stringf \"\"\" clean = sample[\".txt.gz\"] clean = normalize_string(clean) text, target = degrade(clean, prange=prange, seed=seed) text_ids = torch.tensor( tokenizer.encode(prefix + text, max_length=512, truncation=True, padding=\"max_length\") ) target_ids = torch.tensor(tokenizer.encode(target, max_length=512, truncation=True, padding=\"max_length\")) return dict(input_ids=text_ids, labels=target_ids) # This is really all that is WebDataset specific: # - we specify a URL for the JSON index file # - we specify a local cache directory # - we instantiate a ShardListDataset with keep=True # - we add the make_sample transform to the dataset # - we create a custom sampler that respects shard boundaries dataset = wids.ShardListDataset( dataset_url, cache_dir=cache_dir, cache_size=int(1e10), keep=True ) dataset.add_transform(make_sample) dataset[999] sampler = wids.ShardedSampler(dataset) # This plot illustrates the behavior of the shard sampler: it generates a sequence # of samples from each shard in turn, and then moves on to the next shard. import matplotlib as mpl import matplotlib.pyplot as plt %matplotlib inline plt.subplot(121) plt.plot(list(sampler)[:10000]) plt.subplot(122) plt.plot(list(sampler)[:500]); # Standard Hugginface LoRA setup. # start with the pretrained base model model = AutoModelForSeq2SeqLM.from_pretrained(base_model) # set the parameters for LoRA config = LoRAConfig( r=8, alpha=16, # use it on all of the layers intermediate_lora=True, output_lora=True, ) # make a new adapter for the xerr dataset model.add_adapter(\"xerr\", config=config) # enable the adapter for training model.train_adapter(\"xerr\") model.set_active_adapters([\"xerr\"]) # Standard Huggingface adapter training, except for the custom sampler. training_args = TrainingArguments( learning_rate=learning_rate, num_train_epochs=epochs, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, logging_steps=2000, save_steps=5000, output_dir=\"./training_output\", overwrite_output_dir=True, remove_unused_columns=False, max_steps=max_steps, ) # create the trainer trainer = AdapterTrainer( model=model, args=training_args, tokenizer=tokenizer, train_dataset=dataset, # eval_dataset=OCRDataset(\"test\", maxsize=100), ) # to set the sampler, we override the get_train_sampler method # Huggingface doesn't provide a better way to do this trainer._get_train_sampler = lambda: sampler # Run the bulk of the training. trainer.train() # Show some examples (this isn't really \"validation\"). num_validation = 10 validation_dataset = dataset logging.getLogger(\"transformers\").setLevel(logging.ERROR) for i in range(num_validation): # load the input and label (note: we get a different degradation each time) sample = validation_dataset[i] # convert the input and label to tensors input_ids = sample[\"input_ids\"].unsqueeze(0).to(0) label_ids = sample[\"labels\"].unsqueeze(0).to(0) # use the model to generate the output output = model.generate(input_ids, max_length=1024) # convert the tokens to text input_text = ( tokenizer.decode(input_ids[0], skip_special_tokens=True) .replace(\"ocr-errors:\", \"\") .strip() ) output_text = tokenizer.decode(output[0], skip_special_tokens=True).strip() label_text = tokenizer.decode(label_ids[0], skip_special_tokens=True).strip() print(f\"[{i}]\") print(\"Input: \", input_text) print(\"Output:\", output_text) print(\"Label: \", label_text) print(\"---\")","title":"Fine Tuning LLM with Huggingface and WebDataset"},{"location":"examples/webdataset/train-ocr-errors-hf.summary/","text":"The notebook demonstrates the use of the WebDataset library, specifically the ShardListDataset and ShardedSampler classes, for efficient remote data access and loading in the context of fine-tuning a Huggingface large language model. It highlights how training data can be incrementally downloaded and cached locally, and how a custom sampler can be employed to optimize the data retrieval process from shards. The notebook also showcases the integration of WebDataset with Huggingface's training loop, including the use of adapters for model fine-tuning.","title":"Train ocr errors hf.summary"},{"location":"examples/webdataset/train-resnet50-multiray-wds/","text":"WebDataset + Distributed PyTorch Training This notebook illustrates how to use the Web Indexed Dataset ( wids ) library for distributed PyTorch training using DistributedDataParallel . Using webdataset results in training code that is almost identical to plain PyTorch except for the dataset creation. Since WebDataset is an iterable dataset, you need to account for that when creating the DataLoader . Furthermore, for distributed training, easy restarts, etc., it is convenient to use a resampled dataset; this is in contrast to sampling without replacement for each epoch as used more commonly for small, local training. (If you want to use sampling without replacement with webdataset format datasets, see the companion wids -based training notebooks.) Training with WebDataset can be carried out completely without local storage; this is the usual setup in the cloud and on high speed compute clusters. When running locally on a desktop, you may want to cache the data, and for that, you set a cache_dir directory. import os import sys import torch import torch.nn as nn import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torchvision.models import resnet50 from torchvision import datasets, transforms import ray import webdataset as wds import dataclasses import time from collections import deque from typing import Optional def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # Parameters epochs = 10 maxsteps = int(1e12) batch_size = 32 Data Loading for Distributed Training # Datasets are just collections of shards in the cloud. We usually specify # them using {lo..hi} brace notation (there is also a YAML spec for more complex # datasets). bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" trainset_url = bucket + \"/imagenet-train-{000000..001281}.tar\" valset_url = bucket + \"/imagenet-val-{000000..000049}.tar\" batch_size = 32 # If running in the cloud or with a fast network storage system, we don't # need any local storage. if \"google.colab\" in sys.modules: cache_dir = None print(\"running on colab, streaming data directly from storage\") else: cache_dir = \"./_cache\" print(f\"not running in colab, caching data locally in {cache_dir}\") # The dataloader pipeline is a fairly typical `IterableDataset` pipeline # for PyTorch def make_dataloader_train(): \"\"\"Create a DataLoader for training on the ImageNet dataset using WebDataset.\"\"\" transform = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), ] ) def make_sample(sample): return transform(sample[\"jpg\"]), sample[\"cls\"] # This is the basic WebDataset definition: it starts with a URL and add shuffling, # decoding, and augmentation. Note `resampled=True`; this is essential for # distributed training to work correctly. trainset = wds.WebDataset(trainset_url, resampled=True, shardshuffle=True, cache_dir=cache_dir, nodesplitter=wds.split_by_node) trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample) # For IterableDataset objects, the batching needs to happen in the dataset. trainset = trainset.batched(64) trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4) # We unbatch, shuffle, and rebatch to mix samples from different workers. trainloader = trainloader.unbatched().shuffle(1000).batched(batch_size) # A resampled dataset is infinite size, but we can recreate a fixed epoch length. trainloader = trainloader.with_epoch(1282 * 100 // 64) return trainloader # Let's try it out def make_dataloader(split=\"train\"): \"\"\"Make a dataloader for training or validation.\"\"\" if split == \"train\": return make_dataloader_train() elif split == \"val\": return make_dataloader_val() # not implemented for this notebook else: raise ValueError(f\"unknown split {split}\") # Try it out. os.environ[\"GOPEN_VERBOSE\"] = \"1\" sample = next(iter(make_dataloader())) print(sample[0].shape, sample[1].shape) os.environ[\"GOPEN_VERBOSE\"] = \"0\" Standard PyTorch Training This is completely standard PyTorch training; nothing changes by using WebDataset. # We gather all the configuration info into a single typed dataclass. @dataclasses.dataclass class Config: epochs: int = 1 max_steps: int = int(1e18) lr: float = 0.001 momentum: float = 0.9 rank: Optional[int] = None world_size: int = 2 backend: str = \"nccl\" master_addr: str = \"localhost\" master_port: str = \"12355\" report_s: float = 15.0 report_growth: float = 1.1 def train(config): # Define the model, loss function, and optimizer model = resnet50(pretrained=False).cuda() if config.rank is not None: model = DistributedDataParallel(model) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=config.lr) # Data loading code trainloader = make_dataloader(split=\"train\") losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0 # Training loop for epoch in range(config.epochs): for i, data, verbose in enumerate_report(trainloader, config.report_s): inputs, labels = data[0].cuda(), data[1].cuda() # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) # update statistics loss = loss_fn(outputs, labels) accuracy = ( (outputs.argmax(1) == labels).float().mean() ) # calculate accuracy losses.append(loss.item()) accuracies.append(accuracy.item()) if verbose and len(losses) > 0: avgloss = sum(losses) / len(losses) avgaccuracy = sum(accuracies) / len(accuracies) print( f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr, ) loss.backward() optimizer.step() steps += len(labels) if steps > config.max_steps: print( \"finished training (max_steps)\", steps, config.max_steps, file=sys.stderr, ) return print(\"finished Training\", steps) # A quick smoke test of the training function. config = Config() config.epochs = 1 config.max_steps = 1000 train(config) Setting up Distributed Training with Ray Ray is a convenient distributed computing framework. We are using it here to start up the training jobs on multiple GPUs. You can use torch.distributed.launch or other such tools as well with the above code. Ray has the advantage that it is runtime environment independent; you set up your Ray cluster in whatever way works for your environment, and afterwards, this code will run in it without change. @ray.remote(num_gpus=1) def train_on_ray(rank, config): \"\"\"Set up distributed torch env and train the model on this node.\"\"\" # Set up distributed PyTorch. if rank is not None: os.environ[\"MASTER_ADDR\"] = config.master_addr os.environ[\"MASTER_PORT\"] = config.master_port dist.init_process_group( backend=config.backend, rank=rank, world_size=config.world_size ) config.rank = rank # Ray will automatically set CUDA_VISIBLE_DEVICES for each task. train(config) if not ray.is_initialized(): ray.init() ray.available_resources()[\"GPU\"] def distributed_training(config): \"\"\"Perform distributed training with the given config.\"\"\" num_gpus = ray.available_resources()[\"GPU\"] config.world_size = min(config.world_size, num_gpus) results = ray.get( [train_on_ray.remote(i, config) for i in range(config.world_size)] ) print(results) config = Config() config.epochs = epochs config.max_steps = max_steps config.batch_size = batch_size print(config) distributed_training(config)","title":"WebDataset + Distributed PyTorch Training"},{"location":"examples/webdataset/train-resnet50-multiray-wds/#webdataset-distributed-pytorch-training","text":"This notebook illustrates how to use the Web Indexed Dataset ( wids ) library for distributed PyTorch training using DistributedDataParallel . Using webdataset results in training code that is almost identical to plain PyTorch except for the dataset creation. Since WebDataset is an iterable dataset, you need to account for that when creating the DataLoader . Furthermore, for distributed training, easy restarts, etc., it is convenient to use a resampled dataset; this is in contrast to sampling without replacement for each epoch as used more commonly for small, local training. (If you want to use sampling without replacement with webdataset format datasets, see the companion wids -based training notebooks.) Training with WebDataset can be carried out completely without local storage; this is the usual setup in the cloud and on high speed compute clusters. When running locally on a desktop, you may want to cache the data, and for that, you set a cache_dir directory. import os import sys import torch import torch.nn as nn import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torchvision.models import resnet50 from torchvision import datasets, transforms import ray import webdataset as wds import dataclasses import time from collections import deque from typing import Optional def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # Parameters epochs = 10 maxsteps = int(1e12) batch_size = 32","title":"WebDataset + Distributed PyTorch Training"},{"location":"examples/webdataset/train-resnet50-multiray-wds/#data-loading-for-distributed-training","text":"# Datasets are just collections of shards in the cloud. We usually specify # them using {lo..hi} brace notation (there is also a YAML spec for more complex # datasets). bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" trainset_url = bucket + \"/imagenet-train-{000000..001281}.tar\" valset_url = bucket + \"/imagenet-val-{000000..000049}.tar\" batch_size = 32 # If running in the cloud or with a fast network storage system, we don't # need any local storage. if \"google.colab\" in sys.modules: cache_dir = None print(\"running on colab, streaming data directly from storage\") else: cache_dir = \"./_cache\" print(f\"not running in colab, caching data locally in {cache_dir}\") # The dataloader pipeline is a fairly typical `IterableDataset` pipeline # for PyTorch def make_dataloader_train(): \"\"\"Create a DataLoader for training on the ImageNet dataset using WebDataset.\"\"\" transform = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), ] ) def make_sample(sample): return transform(sample[\"jpg\"]), sample[\"cls\"] # This is the basic WebDataset definition: it starts with a URL and add shuffling, # decoding, and augmentation. Note `resampled=True`; this is essential for # distributed training to work correctly. trainset = wds.WebDataset(trainset_url, resampled=True, shardshuffle=True, cache_dir=cache_dir, nodesplitter=wds.split_by_node) trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample) # For IterableDataset objects, the batching needs to happen in the dataset. trainset = trainset.batched(64) trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4) # We unbatch, shuffle, and rebatch to mix samples from different workers. trainloader = trainloader.unbatched().shuffle(1000).batched(batch_size) # A resampled dataset is infinite size, but we can recreate a fixed epoch length. trainloader = trainloader.with_epoch(1282 * 100 // 64) return trainloader # Let's try it out def make_dataloader(split=\"train\"): \"\"\"Make a dataloader for training or validation.\"\"\" if split == \"train\": return make_dataloader_train() elif split == \"val\": return make_dataloader_val() # not implemented for this notebook else: raise ValueError(f\"unknown split {split}\") # Try it out. os.environ[\"GOPEN_VERBOSE\"] = \"1\" sample = next(iter(make_dataloader())) print(sample[0].shape, sample[1].shape) os.environ[\"GOPEN_VERBOSE\"] = \"0\"","title":"Data Loading for Distributed Training"},{"location":"examples/webdataset/train-resnet50-multiray-wds/#standard-pytorch-training","text":"This is completely standard PyTorch training; nothing changes by using WebDataset. # We gather all the configuration info into a single typed dataclass. @dataclasses.dataclass class Config: epochs: int = 1 max_steps: int = int(1e18) lr: float = 0.001 momentum: float = 0.9 rank: Optional[int] = None world_size: int = 2 backend: str = \"nccl\" master_addr: str = \"localhost\" master_port: str = \"12355\" report_s: float = 15.0 report_growth: float = 1.1 def train(config): # Define the model, loss function, and optimizer model = resnet50(pretrained=False).cuda() if config.rank is not None: model = DistributedDataParallel(model) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=config.lr) # Data loading code trainloader = make_dataloader(split=\"train\") losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0 # Training loop for epoch in range(config.epochs): for i, data, verbose in enumerate_report(trainloader, config.report_s): inputs, labels = data[0].cuda(), data[1].cuda() # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) # update statistics loss = loss_fn(outputs, labels) accuracy = ( (outputs.argmax(1) == labels).float().mean() ) # calculate accuracy losses.append(loss.item()) accuracies.append(accuracy.item()) if verbose and len(losses) > 0: avgloss = sum(losses) / len(losses) avgaccuracy = sum(accuracies) / len(accuracies) print( f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr, ) loss.backward() optimizer.step() steps += len(labels) if steps > config.max_steps: print( \"finished training (max_steps)\", steps, config.max_steps, file=sys.stderr, ) return print(\"finished Training\", steps) # A quick smoke test of the training function. config = Config() config.epochs = 1 config.max_steps = 1000 train(config)","title":"Standard PyTorch Training"},{"location":"examples/webdataset/train-resnet50-multiray-wds/#setting-up-distributed-training-with-ray","text":"Ray is a convenient distributed computing framework. We are using it here to start up the training jobs on multiple GPUs. You can use torch.distributed.launch or other such tools as well with the above code. Ray has the advantage that it is runtime environment independent; you set up your Ray cluster in whatever way works for your environment, and afterwards, this code will run in it without change. @ray.remote(num_gpus=1) def train_on_ray(rank, config): \"\"\"Set up distributed torch env and train the model on this node.\"\"\" # Set up distributed PyTorch. if rank is not None: os.environ[\"MASTER_ADDR\"] = config.master_addr os.environ[\"MASTER_PORT\"] = config.master_port dist.init_process_group( backend=config.backend, rank=rank, world_size=config.world_size ) config.rank = rank # Ray will automatically set CUDA_VISIBLE_DEVICES for each task. train(config) if not ray.is_initialized(): ray.init() ray.available_resources()[\"GPU\"] def distributed_training(config): \"\"\"Perform distributed training with the given config.\"\"\" num_gpus = ray.available_resources()[\"GPU\"] config.world_size = min(config.world_size, num_gpus) results = ray.get( [train_on_ray.remote(i, config) for i in range(config.world_size)] ) print(results) config = Config() config.epochs = epochs config.max_steps = max_steps config.batch_size = batch_size print(config) distributed_training(config)","title":"Setting up Distributed Training with Ray"},{"location":"examples/webdataset/train-resnet50-multiray-wds.summary/","text":"The notebook demonstrates how to use the WebDataset library for distributed training with PyTorch's DistributedDataParallel . It focuses on creating data loaders for streaming data directly from cloud storage for training deep learning models, specifically using the WebDataset , WebLoader , and IterableDataset classes from the webdataset library. The notebook also shows how to integrate this setup with Ray for easy distributed training across multiple GPUs.","title":"Train resnet50 multiray wds.summary"},{"location":"examples/webdataset/train-resnet50-wds/","text":"Resnet 50 Training on (Fake)Imagenet with WebDataset This notebook illustrates how to use WebDataset with PyTorch training. # imports %matplotlib inline from functools import partial from pprint import pprint import random from collections import deque import numpy as np from matplotlib import pyplot as plt import sys import os import torch import torchvision import torchvision.transforms as transforms from torchvision.models import resnet50 from torch.utils.data import DataLoader from torch import nn, optim # helpers import time def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # We usually abbreviate webdataset as wds import webdataset as wds # parameters epochs = 1 max_steps = int(1e12) batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\" Data Loader Construction # WebDataset is designed to work without any local storage. Use caching # only if you are on a desktop with slow networking. if 'google.colab' in sys.modules: cache_dir = None print(\"running on colab, streaming data directly from storage\") else: !mkdir -p ./_cache cache_dir = \"./_cache\" print(f\"not running in colab, caching data locally in {cache_dir}\") # The standard TorchVision transformations. transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) def make_sample(sample, val=False): \"\"\"Take a decoded sample dictionary, augment it, and return an (image, label) tuple.\"\"\" assert not val, \"only implemented training dataset for this notebook\" image = sample[\"jpg\"] label = sample[\"cls\"] return transform_train(image), label # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset( training_urls, resampled=True, cache_dir=cache_dir, shardshuffle=True ) trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample) # Since this is an IterableDataset, PyTorch requires that we batch in the dataset. # WebLoader is PyTorch DataLoader with some convenience methods. trainset = trainset.batched(64) trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4) # Unbatch, shuffle between workers, then rebatch. trainloader = trainloader.unbatched().shuffle(1000).batched(64) # Since we are using resampling, the dataset is infinite; set an artificial epoch size. trainloader = trainloader.with_epoch(1282 * 100 // 64) # Smoke test it. os.environ[\"GOPEN_VERBOSE\"] = \"1\" images, classes = next(iter(trainloader)) print(images.shape, classes.shape) os.environ[\"GOPEN_VERBOSE\"] = \"0\" PyTorch Training This is a typical PyTorch training pipeline. # The usual PyTorch model definition. We use an uninitialized ResNet50 model. model = resnet50(pretrained=False) # Define the loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) # Move the model to the GPU if available device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") model = model.to(device) losses, accuracies = deque(maxlen=100), deque(maxlen=100) steps = 0 # Train the model for epoch in range(epochs): for i, data, verbose in enumerate_report(trainloader, 5): # get the inputs; data is a list of [inputs, labels] inputs, labels = data[0].to(device), data[1].to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() pred = outputs.cpu().detach().argmax(dim=1, keepdim=True) correct = pred.eq(labels.cpu().view_as(pred)).sum().item() accuracy = correct / float(len(labels)) losses.append(loss.item()) accuracies.append(accuracy) steps += len(inputs) if verbose and len(losses) > 5: print( \"[%d, %5d] loss: %.5f correct: %.5f\" % (epoch + 1, i + 1, np.mean(losses), np.mean(accuracies)) ) running_loss = 0.0 if steps > max_steps: break if steps > max_steps: break print(\"Finished Training\")","title":"Resnet 50 Training on (Fake)Imagenet with WebDataset"},{"location":"examples/webdataset/train-resnet50-wds/#resnet-50-training-on-fakeimagenet-with-webdataset","text":"This notebook illustrates how to use WebDataset with PyTorch training. # imports %matplotlib inline from functools import partial from pprint import pprint import random from collections import deque import numpy as np from matplotlib import pyplot as plt import sys import os import torch import torchvision import torchvision.transforms as transforms from torchvision.models import resnet50 from torch.utils.data import DataLoader from torch import nn, optim # helpers import time def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # We usually abbreviate webdataset as wds import webdataset as wds # parameters epochs = 1 max_steps = int(1e12) batchsize = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet\" training_urls = bucket + \"/imagenet-train-{000000..001281}.tar\"","title":"Resnet 50 Training on (Fake)Imagenet with WebDataset"},{"location":"examples/webdataset/train-resnet50-wds/#data-loader-construction","text":"# WebDataset is designed to work without any local storage. Use caching # only if you are on a desktop with slow networking. if 'google.colab' in sys.modules: cache_dir = None print(\"running on colab, streaming data directly from storage\") else: !mkdir -p ./_cache cache_dir = \"./_cache\" print(f\"not running in colab, caching data locally in {cache_dir}\") # The standard TorchVision transformations. transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) def make_sample(sample, val=False): \"\"\"Take a decoded sample dictionary, augment it, and return an (image, label) tuple.\"\"\" assert not val, \"only implemented training dataset for this notebook\" image = sample[\"jpg\"] label = sample[\"cls\"] return transform_train(image), label # Create the datasets with shard and sample shuffling and decoding. trainset = wds.WebDataset( training_urls, resampled=True, cache_dir=cache_dir, shardshuffle=True ) trainset = trainset.shuffle(1000).decode(\"pil\").map(make_sample) # Since this is an IterableDataset, PyTorch requires that we batch in the dataset. # WebLoader is PyTorch DataLoader with some convenience methods. trainset = trainset.batched(64) trainloader = wds.WebLoader(trainset, batch_size=None, num_workers=4) # Unbatch, shuffle between workers, then rebatch. trainloader = trainloader.unbatched().shuffle(1000).batched(64) # Since we are using resampling, the dataset is infinite; set an artificial epoch size. trainloader = trainloader.with_epoch(1282 * 100 // 64) # Smoke test it. os.environ[\"GOPEN_VERBOSE\"] = \"1\" images, classes = next(iter(trainloader)) print(images.shape, classes.shape) os.environ[\"GOPEN_VERBOSE\"] = \"0\"","title":"Data Loader Construction"},{"location":"examples/webdataset/train-resnet50-wds/#pytorch-training","text":"This is a typical PyTorch training pipeline. # The usual PyTorch model definition. We use an uninitialized ResNet50 model. model = resnet50(pretrained=False) # Define the loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) # Move the model to the GPU if available device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") model = model.to(device) losses, accuracies = deque(maxlen=100), deque(maxlen=100) steps = 0 # Train the model for epoch in range(epochs): for i, data, verbose in enumerate_report(trainloader, 5): # get the inputs; data is a list of [inputs, labels] inputs, labels = data[0].to(device), data[1].to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() pred = outputs.cpu().detach().argmax(dim=1, keepdim=True) correct = pred.eq(labels.cpu().view_as(pred)).sum().item() accuracy = correct / float(len(labels)) losses.append(loss.item()) accuracies.append(accuracy) steps += len(inputs) if verbose and len(losses) > 5: print( \"[%d, %5d] loss: %.5f correct: %.5f\" % (epoch + 1, i + 1, np.mean(losses), np.mean(accuracies)) ) running_loss = 0.0 if steps > max_steps: break if steps > max_steps: break print(\"Finished Training\")","title":"PyTorch Training"},{"location":"examples/webdataset/train-resnet50-wds.summary/","text":"The notebook demonstrates how to use the webdataset library for training a ResNet50 model on a large-scale image dataset, specifically a fake version of ImageNet, by streaming data directly from cloud storage. It showcases the construction of a data loader using WebDataset , WebLoader , and various transformations and shuffling methods provided by the library to prepare the data for training. The notebook also includes a typical PyTorch training loop, illustrating how the webdataset library integrates with PyTorch's training pipeline.","title":"Train resnet50 wds.summary"},{"location":"examples/webdataset/wds-notes/","text":"%matplotlib inline import matplotlib.pyplot as plt import torch.utils.data import torch.nn from random import randrange import os os.environ[\"WDS_VERBOSE_CACHE\"] = \"1\" os.environ[\"GOPEN_VERBOSE\"] = \"0\" The WebDataset Format WebDataset format files are tar files, with two conventions: within each tar file, files that belong together and make up a training sample share the same basename when stripped of all filename extensions the shards of a tar file are numbered like something-000000.tar to something-012345.tar , usually specified using brace notation something-{000000..012345}.tar WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. WebDataset can also read concatenated MsgPack and CBORs sources. The WebDataset representation allows writing purely sequential I/O pipelines for large scale deep learning. This is important for achieving high I/O rates from local storage (3x-10x for local drives compared to random access) and for using object stores and cloud storage for training. The WebDataset format represents images, movies, audio, etc. in their native file formats, making the creation of WebDataset format data as easy as just creating a tar archive. Because of the way data is aligned, WebDataset works well with block deduplication as well and aligns data on predictable boundaries. Standard tools can be used for accessing and processing WebDataset-format files. bucket = \"https://storage.googleapis.com/webdataset/testdata/\" dataset = \"publaynet-train-{000000..000009}.tar\" url = bucket + dataset !curl -s {url} | tar tf - | sed 10q Note that in these .tar files, we have pairs of .json and .png files; each such pair makes up a training sample. WebDataset Libraries There are several libraries supporting the WebDataset format: webdataset for Python3 (includes the wids library), this repository Webdataset.jl a Julia implementation tarp , a Golang implementation and command line tool Ray Data sources and sinks The webdataset library can be used with PyTorch, Tensorflow, and Jax. The webdataset Library The webdataset library is an implementation of PyTorch IterableDataset (or a mock implementation thereof if you aren't using PyTorch). It implements as form of stream processing. Some of its features are: large scale parallel data access through sharding high performance disk I/O due to purely sequential reads latency insensitive due to big fat pipes no local storage required instant startup for training jobs only requires reading from file descriptors/network streams, no special APIs its API encourages high performance I/O pipelines scalable from tiny desktop datasets to petascale datasets provides local caching if desired requires no dataset metadata; any collection of shards can be read and used instantly The main limitations people run into are related to the fact that IterableDataset is less commonly used in PyTorch and some existing code may not support it as well, and that achieving an exactly balanced number of training samples across many compute nodes for a fixed epoch size is tricky; for multinode training, webdataset is usually used with shard resampling. There are two interfaces, the concise \"fluid\" interface and a longer \"pipeline\" interface. We'll show examples using the fluid interface, which is usually what you want. import webdataset as wds pil_dataset = wds.WebDataset(url).shuffle(1000).decode(\"pil\").to_tuple(\"png\", \"json\") The resulting datasets are standard PyTorch IterableDataset instances. isinstance(pil_dataset, torch.utils.data.IterableDataset) for image, json in pil_dataset: break plt.imshow(image) We can add onto the existing pipeline for augmentation and data preparation. import torchvision.transforms as transforms from PIL import Image preproc = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), lambda x: 1-x, ]) def preprocess(sample): image, json = sample try: label = json[\"annotations\"][0][\"category_id\"] except: label = 0 return preproc(image), label dataset = pil_dataset.map(preprocess) for image, label in dataset: break plt.imshow(image.numpy().transpose(1, 2, 0)) WebDataset is just an instance of a standard IterableDataset . It's a single-threaded way of iterating over a dataset. Since image decompression and data augmentation can be compute intensive, PyTorch usually uses the DataLoader class to parallelize data loading and preprocessing. WebDataset is fully compatible with the standard DataLoader . The wds.WebDataset fluid interface is just a convenient shorthand for writing down pipelines. The underlying pipeline is an instance of the wds.DataPipeline class, and you can construct data pipelines explicitly, similar to the way you use nn.Sequential inside models. dataset = wds.DataPipeline( wds.SimpleShardList(url), # at this point we have an iterator over all the shards wds.shuffle(100), # add wds.split_by_node here if you are using multiple nodes wds.split_by_worker, # at this point, we have an iterator over the shards assigned to each worker wds.tarfile_to_samples(), # this shuffles the samples in memory wds.shuffle(1000), # this decodes the images and json wds.decode(\"pil\"), wds.to_tuple(\"png\", \"json\"), wds.map(preprocess), wds.shuffle(1000), wds.batched(16) ) batch = next(iter(dataset)) batch[0].shape, batch[1].shape Here are a number of notebooks showing how to use WebDataset for image classification and LLM training: train-resnet50-wds -- simple, single GPU training from Imagenet train-resnet50-multiray-wds -- multinode training using webdataset generate-text-dataset -- initial dataset generation tesseract-wds -- shard-to-shard transformations, here for OCR running over large dataset Installation and Documentation $ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets Examples: (NB: some of these are for older versions of WebDataset, but the differences should be small) loading videos splitting raw videos into clips for training converting the Falling Things dataset Dependencies The WebDataset library only requires PyTorch, NumPy, and a small library called braceexpand . WebDataset loads a few additional libraries dynamically only when they are actually needed and only in the decoder: PIL/Pillow for image decoding torchvision , torchvideo , torchaudio for image/video/audio decoding msgpack for MessagePack decoding the curl command line tool for accessing HTTP servers the Google/Amazon/Azure command line tools for accessing cloud storage buckets Loading of one of these libraries is triggered by configuring a decoder that attempts to decode content in the given format and encountering a file in that format during decoding. (Eventually, the torch... dependencies will be refactored into those libraries.) Data Decoding Data decoding is a special kind of transformations of samples. You could simply write a decoding function like this: def my_sample_decoder(sample): result = dict(__key__=sample[\"__key__\"]) for key, value in sample.items(): if key == \"png\" or key.endswith(\".png\"): result[key] = mageio.imread(io.BytesIO(value)) elif ...: ... return result dataset = wds.Processor(wds.map, my_sample_decoder)(dataset) This gets tedious, though, and it also unnecessarily hardcodes the sample's keys into the processing pipeline. To help with this, there is a helper class that simplifies this kind of code. The primary use of Decoder is for decoding compressed image, video, and audio formats, as well as unzipping .gz files. Here is an example of automatically decoding .png images with imread and using the default torch_video and torch_audio decoders for video and audio: def my_png_decoder(key, value): if not key.endswith(\".png\"): return None assert isinstance(value, bytes) return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(my_png_decoder, wds.torch_video, wds.torch_audio)(dataset) You can use whatever criteria you like for deciding how to decode values in samples. When used with standard WebDataset format files, the keys are the full extensions of the file names inside a .tar file. For consistency, it's recommended that you primarily rely on the extensions (e.g., .png , .mp4 ) to decide which decoders to use. There is a special helper function that simplifies this: def my_decoder(value): return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(wds.handle_extension(\".png\", my_decoder))(dataset) \"Smaller\" Datasets and Desktop Computing WebDataset is an ideal solution for training on petascale datasets kept on high performance distributed data stores like AIStore, AWS/S3, and Google Cloud. Compared to data center GPU servers, desktop machines have much slower network connections, but training jobs on desktop machines often also use much smaller datasets. WebDataset also is very useful for such smaller datasets, and it can easily be used for developing and testing on small datasets and then scaling up to large datasets by simply using more shards. Here are different usage scenarios: desktop deep learning, smaller datasets copy all shards to local disk manually use automatic shard caching prototyping, development, testing of jobs for large scale training copy a small subset of shards to local disk use automatic shard caching with a small subrange of shards cloud training against cloud buckets use WebDataset directly with remote URLs on premises training with high performance store (e.g., AIStore) and fast networks use WebDataset directly with remote URLs on premises training with slower object stores and/or slower networks use automatic shard caching Location Independence, Caching, Etc. WebDataset makes it easy to use a single specification for your datasets and run your code without change in different environments. Loadable Dataset Specifications If you write your input pipelines such that they are defined by a dataset specification in some language, you can most easily retarget your training pipelines to different datasets. You can do this either by dynamically loading the Python code that constructs the pipeline or by using a YAML/JSON dataset specification. A YAML dataset specification looks like this: dataset: - shards: gs://nvdata-ocropus-tess/ia1-{000000..000033}.tar scaleprob: 0.3 - shards: gs://nvdata-ocropus-tess/cdipsub-{000000..000022}.tar scale: [1.0, 3.0] - shards: gs://nvdata-ocropus-tess/gsub-{000000..000167}.tar scale: [0.4, 1.0] - shards: gs://nvdata-ocropus-tess/bin-gsub-{000000..000167}.tar extensions: nrm.jpg scale: [0.3, 1.0] - shards: gs://nvdata-ocropus/rendered.tar scaleprob: 1.0 Note that datasets can be composed from different shard collections, mixed in different proportions. The dataset specification reader will be integrated in the next minor version update. AIStore Proxy If you want to use an AISTore server as a cache, you can tell any WebDataset pipeline to replace direct accesses to your URLs to proxied accesses via the AIStore server. To do that, you need to set a couple of environment variables. export AIS_ENDPOINT=http://nix:51080 export USE_AIS_FOR=\"gs\" Now, any accesses to Google Cloud Storage ( gs:// urls) will be routed through the AIS server. URL Rewriting You can rewrite URLs using regular expressions via an environment variable; the syntax is WDS_REWRITE=regex=regex;regex=regex . For example, to replace gs:// accesses with local file accesses, use export WDS_REWRITE=\"gs://=/shared/data/\" To access Google cloud data via ssh, you might use something like: export WDS_REWRITE=\"gs://=pipe:ssh proxyhost gsutil cat \" Use the Caching Mechanism If you use the built-in caching mechanism, you can simply download shards to a local directory and specify that directory as the cache directory. The shards in that directory will override the shards that are being downloaded. Shards in the cache are mapped based on the pathname and file name of your shard names. Direct Copying of Shards Let's take the OpenImages dataset as an example; it's half a terabyte large. For development and testing, you may not want to download the entire dataset, but you may also not want to use the dataset remotely. With WebDataset, you can just download a small number of shards and use them during development. !curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar > /tmp/openimages-train-000000.tar dataset = wds.DataPipeline( wds.SimpleShardList(\"/tmp/openimages-train-000000.tar\"), wds.tarfile_to_samples(), ) repr(next(iter(dataset)))[:200] Note that the WebDataset class works the same way on local files as it does on remote files. Furthermore, unlike other kinds of dataset formats and archive formats, downloaded datasets are immediately useful and don't need to be unpacked. Creating a WebDataset Using tar Since WebDatasets are just regular tar files, you can usually create them by just using the tar command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with $ tar --sort=name -cf dataset.tar dataset/ If your dataset has some other directory layout, you may need a different file name in the archive from the name on disk. You can use the --transform argument to GNU tar to transform file names. You can also use the -T argument to read the files from a text file and embed other options in that text file. The tarp create Command The tarp command is a little utility for manipulating tar archives. Its create subcommand makes it particularly simple to construct tar archives from files. The tarp create command takes a recipe for building a tar archive that contains lines of the form: archive-name-1 source-name-1 archive-name-2 source-name-2 ... The source name can either be a file, \"text:something\", or \"pipe:something\". Programmatically in Python You can also create a WebDataset with library functions in this library: webdataset.TarWriter takes dictionaries containing key value pairs and writes them to disk webdataset.ShardWriter takes dictionaries containing key value pairs and writes them to disk as a series of shards Here is a quick way of converting an existing dataset into a WebDataset; this will store all tensors as Python pickles: sink = wds.TarWriter(\"dest.tar\") dataset = open_my_dataset() for index, (input, output) in dataset: sink.write({ \"__key__\": \"sample%06d\" % index, \"input.pyd\": input, \"output.pyd\": output, }) sink.close() Storing data as Python pickles allows most common Python datatypes to be stored, it is lossless, and the format is fast to decode. However, it is uncompressed and cannot be read by non-Python programs. It's often better to choose other storage formats, e.g., taking advantage of common image compression formats. If you know that the input is an image and the output is an integer class, you can also write something like this: sink = wds.TarWriter(\"dest.tar\") dataset = open_my_dataset() for index, (input, output) in dataset: assert input.ndim == 3 and input.shape[2] == 3 assert input.dtype = np.float32 and np.amin(input) >= 0 and np.amax(input) <= 1 assert type(output) == int sink.write({ \"__key__\": \"sample%06d\" % index, \"input.jpg\": input, \"output.cls\": output, }) sink.close() The assert statements in that loop are not necessary, but they document and illustrate the expectations for this particular dataset. Generally, the \".jpg\" encoder can actually encode a wide variety of array types as images. The \".cls\" encoder always requires an integer for encoding. Here is how you can use TarWriter for writing a dataset without using an encoder: sink = wds.TarWriter(\"dest.tar\", encoder=False) for basename in basenames: with open(f\"{basename}.png\", \"rb\") as stream): image = stream.read() cls = lookup_cls(basename) sample = { \"__key__\": basename, \"input.png\": image, \"target.cls\": cls } sink.write(sample) sink.close() Since no encoder is used, if you want to be able to read this data with the default decoder, image must contain a byte string corresponding to a PNG image (as indicated by the \".png\" extension on its dictionary key), and cls must contain an integer encoded in ASCII (as indicated by the \".cls\" extension on its dictionary key). Writing Filters and Offline Augmentation Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels. from torchvision import transforms from itertools import islice def extract_class(data): # mock implementation return 0 def preproc(image): image = transforms.ToTensor()(image) # more preprocessing here return image def augment_wds(input, output, maxcount=999999999): src = wds.DataPipeline( wds.SimpleShardList(input), wds.tarfile_to_samples(), wds.decode(\"pil\"), wds.to_tuple(\"__key__\", \"jpg;png\", \"json\"), wds.map_tuple(None, preproc, None), ) with wds.TarWriter(output) as dst: for key, image, data in islice(src, 0, maxcount): print(key) image = image.numpy().transpose(1, 2, 0) image -= np.amin(image) image /= np.amax(image) sample = { \"__key__\": key, \"png\": image, \"cls\": extract_class(data) } dst.write(sample) Now run the augmentation pipeline: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" augment_wds(url, \"_temp.tar\", maxcount=5) To verify that things worked correctly, let's look at the output file: %%bash tar tf _temp.tar If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system. For example, using Dask, you could process all 554 shards in parallel using code like this: shards = braceexpand.braceexpand(\"{000000..000554}\") inputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards] outputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards] results = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)] dask.compute(*results) Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker. For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes Job template, or using a workflow engine like Argo. Whether you prefer WebDataset or Dataset is a matter of style. Syntax for URL Sources The SimpleShardList and ResampledShards take either a string or a list of URLs as an argument. If it is given a string, the string is expanded using the braceexpand library. So, the following are equivalent: ShardList(\"dataset-{000..001}.tar\") ShardList([\"dataset-000.tar\", \"dataset-001.tar\"]) The url strings in a shard list are handled by default by the webdataset.url_opener filter. It recognizes three simple kinds of strings: \"-\", \"/path/to/file\", and \"pipe:command\": the string \"-\", referring to stdin a UNIX path, opened as a regular file a URL-like string with the schema \"pipe:\"; such URLs are opened with subprocess.Popen . For example: pipe:curl -s -L http://server/file accesses a file via HTTP pipe:gsutil cat gs://bucket/file accesses a file on GCS pipe:az cp --container bucket --name file --file /dev/stdout accesses a file on Azure pipe:ssh host cat file accesses a file via ssh It might seem at first glance to be \"more efficient\" to use built-in Python libraries for accessing object stores rather than subprocesses, but efficient object store access from Python really requires spawning a separate process anyway, so this approach to accessing object stores is not only convenient, it also is as efficient as we can make it in Python. Length Properties WebDataset instances are subclasses of IterableDataset . These instances are not supposed to have a __len__ method, and some code actually tests for that. If you want to have a length property on your dataset, use the with_length(n) method with whatever length you would like to set. If you want to change the size of the epoch, i.e., if you want to force the iterator to quit after a given number of samples or batches, use the with_epoch method. You can combine both methods; use with_length last. Tar Header Overhead Tar imposes a 512 byte overhead for each file stored in the archive. For most applications, this is not an issue because images and other content tends to be much larger. If you have datasets that contain large amounts of small files (e.g., text-only training, etc.), this overhead may become significant. In that case, you have several options: store some or all of your sample in JSON, MsgPack, or CBOR format gzip-compress your tar file (use .tgz instead of .tar); WebDatset will automatically decompress pre-batch the data (not recommended) Both of the first options are very simple. To store your entire sample in MsgPack format, do something like this: # Writing ... construct sample ... sample = dict(mp=sample) writer.write(sample) # Reading dataset = ... initial construction ... dataset = dataset.map(sample: sample[\"mp\"]) ... use sample as usual ... Related Libraries and Software The AIStore server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs. The tarproc utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and xargs -like functionality. The tensorcom library provides fast three-tiered I/O; it can be inserted between AIStore and WebDataset to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available. You can find the full PyTorch ImageNet sample code converted to WebDataset at tmbdev/pytorch-imagenet-wds","title":"Wds notes"},{"location":"examples/webdataset/wds-notes/#the-webdataset-format","text":"WebDataset format files are tar files, with two conventions: within each tar file, files that belong together and make up a training sample share the same basename when stripped of all filename extensions the shards of a tar file are numbered like something-000000.tar to something-012345.tar , usually specified using brace notation something-{000000..012345}.tar WebDataset can read files from local disk or from any pipe, which allows it to access files using common cloud object stores. WebDataset can also read concatenated MsgPack and CBORs sources. The WebDataset representation allows writing purely sequential I/O pipelines for large scale deep learning. This is important for achieving high I/O rates from local storage (3x-10x for local drives compared to random access) and for using object stores and cloud storage for training. The WebDataset format represents images, movies, audio, etc. in their native file formats, making the creation of WebDataset format data as easy as just creating a tar archive. Because of the way data is aligned, WebDataset works well with block deduplication as well and aligns data on predictable boundaries. Standard tools can be used for accessing and processing WebDataset-format files. bucket = \"https://storage.googleapis.com/webdataset/testdata/\" dataset = \"publaynet-train-{000000..000009}.tar\" url = bucket + dataset !curl -s {url} | tar tf - | sed 10q Note that in these .tar files, we have pairs of .json and .png files; each such pair makes up a training sample.","title":"The WebDataset Format"},{"location":"examples/webdataset/wds-notes/#webdataset-libraries","text":"There are several libraries supporting the WebDataset format: webdataset for Python3 (includes the wids library), this repository Webdataset.jl a Julia implementation tarp , a Golang implementation and command line tool Ray Data sources and sinks The webdataset library can be used with PyTorch, Tensorflow, and Jax.","title":"WebDataset Libraries"},{"location":"examples/webdataset/wds-notes/#the-webdataset-library","text":"The webdataset library is an implementation of PyTorch IterableDataset (or a mock implementation thereof if you aren't using PyTorch). It implements as form of stream processing. Some of its features are: large scale parallel data access through sharding high performance disk I/O due to purely sequential reads latency insensitive due to big fat pipes no local storage required instant startup for training jobs only requires reading from file descriptors/network streams, no special APIs its API encourages high performance I/O pipelines scalable from tiny desktop datasets to petascale datasets provides local caching if desired requires no dataset metadata; any collection of shards can be read and used instantly The main limitations people run into are related to the fact that IterableDataset is less commonly used in PyTorch and some existing code may not support it as well, and that achieving an exactly balanced number of training samples across many compute nodes for a fixed epoch size is tricky; for multinode training, webdataset is usually used with shard resampling. There are two interfaces, the concise \"fluid\" interface and a longer \"pipeline\" interface. We'll show examples using the fluid interface, which is usually what you want. import webdataset as wds pil_dataset = wds.WebDataset(url).shuffle(1000).decode(\"pil\").to_tuple(\"png\", \"json\") The resulting datasets are standard PyTorch IterableDataset instances. isinstance(pil_dataset, torch.utils.data.IterableDataset) for image, json in pil_dataset: break plt.imshow(image) We can add onto the existing pipeline for augmentation and data preparation. import torchvision.transforms as transforms from PIL import Image preproc = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), lambda x: 1-x, ]) def preprocess(sample): image, json = sample try: label = json[\"annotations\"][0][\"category_id\"] except: label = 0 return preproc(image), label dataset = pil_dataset.map(preprocess) for image, label in dataset: break plt.imshow(image.numpy().transpose(1, 2, 0)) WebDataset is just an instance of a standard IterableDataset . It's a single-threaded way of iterating over a dataset. Since image decompression and data augmentation can be compute intensive, PyTorch usually uses the DataLoader class to parallelize data loading and preprocessing. WebDataset is fully compatible with the standard DataLoader . The wds.WebDataset fluid interface is just a convenient shorthand for writing down pipelines. The underlying pipeline is an instance of the wds.DataPipeline class, and you can construct data pipelines explicitly, similar to the way you use nn.Sequential inside models. dataset = wds.DataPipeline( wds.SimpleShardList(url), # at this point we have an iterator over all the shards wds.shuffle(100), # add wds.split_by_node here if you are using multiple nodes wds.split_by_worker, # at this point, we have an iterator over the shards assigned to each worker wds.tarfile_to_samples(), # this shuffles the samples in memory wds.shuffle(1000), # this decodes the images and json wds.decode(\"pil\"), wds.to_tuple(\"png\", \"json\"), wds.map(preprocess), wds.shuffle(1000), wds.batched(16) ) batch = next(iter(dataset)) batch[0].shape, batch[1].shape Here are a number of notebooks showing how to use WebDataset for image classification and LLM training: train-resnet50-wds -- simple, single GPU training from Imagenet train-resnet50-multiray-wds -- multinode training using webdataset generate-text-dataset -- initial dataset generation tesseract-wds -- shard-to-shard transformations, here for OCR running over large dataset","title":"The webdataset Library"},{"location":"examples/webdataset/wds-notes/#installation-and-documentation","text":"$ pip install webdataset For the Github version: $ pip install git+https://github.com/tmbdev/webdataset.git Here are some videos talking about WebDataset and large scale deep learning: Introduction to Large Scale Deep Learning Loading Training Data with WebDataset Creating Datasets in WebDataset Format Tools for Working with Large Datasets Examples: (NB: some of these are for older versions of WebDataset, but the differences should be small) loading videos splitting raw videos into clips for training converting the Falling Things dataset","title":"Installation and Documentation"},{"location":"examples/webdataset/wds-notes/#dependencies","text":"The WebDataset library only requires PyTorch, NumPy, and a small library called braceexpand . WebDataset loads a few additional libraries dynamically only when they are actually needed and only in the decoder: PIL/Pillow for image decoding torchvision , torchvideo , torchaudio for image/video/audio decoding msgpack for MessagePack decoding the curl command line tool for accessing HTTP servers the Google/Amazon/Azure command line tools for accessing cloud storage buckets Loading of one of these libraries is triggered by configuring a decoder that attempts to decode content in the given format and encountering a file in that format during decoding. (Eventually, the torch... dependencies will be refactored into those libraries.)","title":"Dependencies"},{"location":"examples/webdataset/wds-notes/#data-decoding","text":"Data decoding is a special kind of transformations of samples. You could simply write a decoding function like this: def my_sample_decoder(sample): result = dict(__key__=sample[\"__key__\"]) for key, value in sample.items(): if key == \"png\" or key.endswith(\".png\"): result[key] = mageio.imread(io.BytesIO(value)) elif ...: ... return result dataset = wds.Processor(wds.map, my_sample_decoder)(dataset) This gets tedious, though, and it also unnecessarily hardcodes the sample's keys into the processing pipeline. To help with this, there is a helper class that simplifies this kind of code. The primary use of Decoder is for decoding compressed image, video, and audio formats, as well as unzipping .gz files. Here is an example of automatically decoding .png images with imread and using the default torch_video and torch_audio decoders for video and audio: def my_png_decoder(key, value): if not key.endswith(\".png\"): return None assert isinstance(value, bytes) return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(my_png_decoder, wds.torch_video, wds.torch_audio)(dataset) You can use whatever criteria you like for deciding how to decode values in samples. When used with standard WebDataset format files, the keys are the full extensions of the file names inside a .tar file. For consistency, it's recommended that you primarily rely on the extensions (e.g., .png , .mp4 ) to decide which decoders to use. There is a special helper function that simplifies this: def my_decoder(value): return imageio.imread(io.BytesIO(value)) dataset = wds.Decoder(wds.handle_extension(\".png\", my_decoder))(dataset)","title":"Data Decoding"},{"location":"examples/webdataset/wds-notes/#smaller-datasets-and-desktop-computing","text":"WebDataset is an ideal solution for training on petascale datasets kept on high performance distributed data stores like AIStore, AWS/S3, and Google Cloud. Compared to data center GPU servers, desktop machines have much slower network connections, but training jobs on desktop machines often also use much smaller datasets. WebDataset also is very useful for such smaller datasets, and it can easily be used for developing and testing on small datasets and then scaling up to large datasets by simply using more shards. Here are different usage scenarios: desktop deep learning, smaller datasets copy all shards to local disk manually use automatic shard caching prototyping, development, testing of jobs for large scale training copy a small subset of shards to local disk use automatic shard caching with a small subrange of shards cloud training against cloud buckets use WebDataset directly with remote URLs on premises training with high performance store (e.g., AIStore) and fast networks use WebDataset directly with remote URLs on premises training with slower object stores and/or slower networks use automatic shard caching","title":"\"Smaller\" Datasets and Desktop Computing"},{"location":"examples/webdataset/wds-notes/#location-independence-caching-etc","text":"WebDataset makes it easy to use a single specification for your datasets and run your code without change in different environments.","title":"Location Independence, Caching, Etc."},{"location":"examples/webdataset/wds-notes/#loadable-dataset-specifications","text":"If you write your input pipelines such that they are defined by a dataset specification in some language, you can most easily retarget your training pipelines to different datasets. You can do this either by dynamically loading the Python code that constructs the pipeline or by using a YAML/JSON dataset specification. A YAML dataset specification looks like this: dataset: - shards: gs://nvdata-ocropus-tess/ia1-{000000..000033}.tar scaleprob: 0.3 - shards: gs://nvdata-ocropus-tess/cdipsub-{000000..000022}.tar scale: [1.0, 3.0] - shards: gs://nvdata-ocropus-tess/gsub-{000000..000167}.tar scale: [0.4, 1.0] - shards: gs://nvdata-ocropus-tess/bin-gsub-{000000..000167}.tar extensions: nrm.jpg scale: [0.3, 1.0] - shards: gs://nvdata-ocropus/rendered.tar scaleprob: 1.0 Note that datasets can be composed from different shard collections, mixed in different proportions. The dataset specification reader will be integrated in the next minor version update.","title":"Loadable Dataset Specifications"},{"location":"examples/webdataset/wds-notes/#aistore-proxy","text":"If you want to use an AISTore server as a cache, you can tell any WebDataset pipeline to replace direct accesses to your URLs to proxied accesses via the AIStore server. To do that, you need to set a couple of environment variables. export AIS_ENDPOINT=http://nix:51080 export USE_AIS_FOR=\"gs\" Now, any accesses to Google Cloud Storage ( gs:// urls) will be routed through the AIS server.","title":"AIStore Proxy"},{"location":"examples/webdataset/wds-notes/#url-rewriting","text":"You can rewrite URLs using regular expressions via an environment variable; the syntax is WDS_REWRITE=regex=regex;regex=regex . For example, to replace gs:// accesses with local file accesses, use export WDS_REWRITE=\"gs://=/shared/data/\" To access Google cloud data via ssh, you might use something like: export WDS_REWRITE=\"gs://=pipe:ssh proxyhost gsutil cat \"","title":"URL Rewriting"},{"location":"examples/webdataset/wds-notes/#use-the-caching-mechanism","text":"If you use the built-in caching mechanism, you can simply download shards to a local directory and specify that directory as the cache directory. The shards in that directory will override the shards that are being downloaded. Shards in the cache are mapped based on the pathname and file name of your shard names.","title":"Use the Caching Mechanism"},{"location":"examples/webdataset/wds-notes/#direct-copying-of-shards","text":"Let's take the OpenImages dataset as an example; it's half a terabyte large. For development and testing, you may not want to download the entire dataset, but you may also not want to use the dataset remotely. With WebDataset, you can just download a small number of shards and use them during development. !curl -L -s http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar > /tmp/openimages-train-000000.tar dataset = wds.DataPipeline( wds.SimpleShardList(\"/tmp/openimages-train-000000.tar\"), wds.tarfile_to_samples(), ) repr(next(iter(dataset)))[:200] Note that the WebDataset class works the same way on local files as it does on remote files. Furthermore, unlike other kinds of dataset formats and archive formats, downloaded datasets are immediately useful and don't need to be unpacked.","title":"Direct Copying of Shards"},{"location":"examples/webdataset/wds-notes/#creating-a-webdataset","text":"","title":"Creating a WebDataset"},{"location":"examples/webdataset/wds-notes/#using-tar","text":"Since WebDatasets are just regular tar files, you can usually create them by just using the tar command. All you have to do is to arrange for any files that should be in the same sample to share the same basename. Many datasets already come that way. For those, you can simply create a WebDataset with $ tar --sort=name -cf dataset.tar dataset/ If your dataset has some other directory layout, you may need a different file name in the archive from the name on disk. You can use the --transform argument to GNU tar to transform file names. You can also use the -T argument to read the files from a text file and embed other options in that text file.","title":"Using tar"},{"location":"examples/webdataset/wds-notes/#the-tarp-create-command","text":"The tarp command is a little utility for manipulating tar archives. Its create subcommand makes it particularly simple to construct tar archives from files. The tarp create command takes a recipe for building a tar archive that contains lines of the form: archive-name-1 source-name-1 archive-name-2 source-name-2 ... The source name can either be a file, \"text:something\", or \"pipe:something\".","title":"The tarp create Command"},{"location":"examples/webdataset/wds-notes/#programmatically-in-python","text":"You can also create a WebDataset with library functions in this library: webdataset.TarWriter takes dictionaries containing key value pairs and writes them to disk webdataset.ShardWriter takes dictionaries containing key value pairs and writes them to disk as a series of shards Here is a quick way of converting an existing dataset into a WebDataset; this will store all tensors as Python pickles: sink = wds.TarWriter(\"dest.tar\") dataset = open_my_dataset() for index, (input, output) in dataset: sink.write({ \"__key__\": \"sample%06d\" % index, \"input.pyd\": input, \"output.pyd\": output, }) sink.close() Storing data as Python pickles allows most common Python datatypes to be stored, it is lossless, and the format is fast to decode. However, it is uncompressed and cannot be read by non-Python programs. It's often better to choose other storage formats, e.g., taking advantage of common image compression formats. If you know that the input is an image and the output is an integer class, you can also write something like this: sink = wds.TarWriter(\"dest.tar\") dataset = open_my_dataset() for index, (input, output) in dataset: assert input.ndim == 3 and input.shape[2] == 3 assert input.dtype = np.float32 and np.amin(input) >= 0 and np.amax(input) <= 1 assert type(output) == int sink.write({ \"__key__\": \"sample%06d\" % index, \"input.jpg\": input, \"output.cls\": output, }) sink.close() The assert statements in that loop are not necessary, but they document and illustrate the expectations for this particular dataset. Generally, the \".jpg\" encoder can actually encode a wide variety of array types as images. The \".cls\" encoder always requires an integer for encoding. Here is how you can use TarWriter for writing a dataset without using an encoder: sink = wds.TarWriter(\"dest.tar\", encoder=False) for basename in basenames: with open(f\"{basename}.png\", \"rb\") as stream): image = stream.read() cls = lookup_cls(basename) sample = { \"__key__\": basename, \"input.png\": image, \"target.cls\": cls } sink.write(sample) sink.close() Since no encoder is used, if you want to be able to read this data with the default decoder, image must contain a byte string corresponding to a PNG image (as indicated by the \".png\" extension on its dictionary key), and cls must contain an integer encoded in ASCII (as indicated by the \".cls\" extension on its dictionary key).","title":"Programmatically in Python"},{"location":"examples/webdataset/wds-notes/#writing-filters-and-offline-augmentation","text":"Webdataset can be used for filters and offline augmentation of datasets. Here is a complete example that pre-augments a shard and extracts class labels. from torchvision import transforms from itertools import islice def extract_class(data): # mock implementation return 0 def preproc(image): image = transforms.ToTensor()(image) # more preprocessing here return image def augment_wds(input, output, maxcount=999999999): src = wds.DataPipeline( wds.SimpleShardList(input), wds.tarfile_to_samples(), wds.decode(\"pil\"), wds.to_tuple(\"__key__\", \"jpg;png\", \"json\"), wds.map_tuple(None, preproc, None), ) with wds.TarWriter(output) as dst: for key, image, data in islice(src, 0, maxcount): print(key) image = image.numpy().transpose(1, 2, 0) image -= np.amin(image) image /= np.amax(image) sample = { \"__key__\": key, \"png\": image, \"cls\": extract_class(data) } dst.write(sample) Now run the augmentation pipeline: url = \"http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\" url = f\"pipe:curl -L -s {url} || true\" augment_wds(url, \"_temp.tar\", maxcount=5) To verify that things worked correctly, let's look at the output file: %%bash tar tf _temp.tar If you want to preprocess the entire OpenImages dataset with a process like this, you can use your favorite job queueing or worflow system. For example, using Dask, you could process all 554 shards in parallel using code like this: shards = braceexpand.braceexpand(\"{000000..000554}\") inputs = [f\"gs://bucket/openimages-{shard}.tar\" for shard in shards] outputs = [f\"gs://bucket2/openimages-augmented-{shard}.tar\" for shard in shards] results = [dask.delayed(augment_wds)(args) for args in zip(inputs, outputs)] dask.compute(*results) Note that the data is streaming from and to Google Cloud Storage buckets, so very little local storage is required on each worker. For very large scale processing, it's easiest to submit separate jobs to a Kubernetes cluster using the Kubernetes Job template, or using a workflow engine like Argo. Whether you prefer WebDataset or Dataset is a matter of style.","title":"Writing Filters and Offline Augmentation"},{"location":"examples/webdataset/wds-notes/#syntax-for-url-sources","text":"The SimpleShardList and ResampledShards take either a string or a list of URLs as an argument. If it is given a string, the string is expanded using the braceexpand library. So, the following are equivalent: ShardList(\"dataset-{000..001}.tar\") ShardList([\"dataset-000.tar\", \"dataset-001.tar\"]) The url strings in a shard list are handled by default by the webdataset.url_opener filter. It recognizes three simple kinds of strings: \"-\", \"/path/to/file\", and \"pipe:command\": the string \"-\", referring to stdin a UNIX path, opened as a regular file a URL-like string with the schema \"pipe:\"; such URLs are opened with subprocess.Popen . For example: pipe:curl -s -L http://server/file accesses a file via HTTP pipe:gsutil cat gs://bucket/file accesses a file on GCS pipe:az cp --container bucket --name file --file /dev/stdout accesses a file on Azure pipe:ssh host cat file accesses a file via ssh It might seem at first glance to be \"more efficient\" to use built-in Python libraries for accessing object stores rather than subprocesses, but efficient object store access from Python really requires spawning a separate process anyway, so this approach to accessing object stores is not only convenient, it also is as efficient as we can make it in Python.","title":"Syntax for URL Sources"},{"location":"examples/webdataset/wds-notes/#length-properties","text":"WebDataset instances are subclasses of IterableDataset . These instances are not supposed to have a __len__ method, and some code actually tests for that. If you want to have a length property on your dataset, use the with_length(n) method with whatever length you would like to set. If you want to change the size of the epoch, i.e., if you want to force the iterator to quit after a given number of samples or batches, use the with_epoch method. You can combine both methods; use with_length last.","title":"Length Properties"},{"location":"examples/webdataset/wds-notes/#tar-header-overhead","text":"Tar imposes a 512 byte overhead for each file stored in the archive. For most applications, this is not an issue because images and other content tends to be much larger. If you have datasets that contain large amounts of small files (e.g., text-only training, etc.), this overhead may become significant. In that case, you have several options: store some or all of your sample in JSON, MsgPack, or CBOR format gzip-compress your tar file (use .tgz instead of .tar); WebDatset will automatically decompress pre-batch the data (not recommended) Both of the first options are very simple. To store your entire sample in MsgPack format, do something like this: # Writing ... construct sample ... sample = dict(mp=sample) writer.write(sample) # Reading dataset = ... initial construction ... dataset = dataset.map(sample: sample[\"mp\"]) ... use sample as usual ...","title":"Tar Header Overhead"},{"location":"examples/webdataset/wds-notes/#related-libraries-and-software","text":"The AIStore server provides an efficient backend for WebDataset; it functions like a combination of web server, content distribution network, P2P network, and distributed file system. Together, AIStore and WebDataset can serve input data from rotational drives distributed across many servers at the speed of local SSDs to many GPUs, at a fraction of the cost. We can easily achieve hundreds of MBytes/s of I/O per GPU even in large, distributed training jobs. The tarproc utilities provide command line manipulation and processing of webdatasets and other tar files, including splitting, concatenation, and xargs -like functionality. The tensorcom library provides fast three-tiered I/O; it can be inserted between AIStore and WebDataset to permit distributed data augmentation and I/O. It is particularly useful when data augmentation requires more CPU than the GPU server has available. You can find the full PyTorch ImageNet sample code converted to WebDataset at tmbdev/pytorch-imagenet-wds","title":"Related Libraries and Software"},{"location":"examples/webdataset/wds-notes.summary/","text":"The notebook provides an overview and tutorial on how to use the webdataset library, which is a PyTorch IterableDataset for handling large-scale datasets stored in web-friendly formats like tar files. It demonstrates how to read and process data from such datasets, including shuffling, decoding, and preprocessing for deep learning applications. The notebook showcases the use of classes like WebDataset , DataPipeline , and TarWriter from the webdataset library, illustrating how to create efficient I/O pipelines for training models with large datasets that can be read from local disk or cloud storage.","title":"Wds notes.summary"},{"location":"examples/wids/","text":"Train Resnet50 Multiray Wids Train Resnet50 Multiray Wids The notebook demonstrates how to perform distributed PyTorch training using the wids library, specifically focusing on the use of ShardListDataset for dataset construction and DistributedChunkedSampler for efficient sampling in a distributed environment. It highlights the ease of integrating wids with PyTorch code for training models on datasets stored in the cloud, with an example using a fake version of ImageNet. The notebook also shows how to set up distributed training using Ray and includes a configuration dataclass to manage training parameters. Train Resnet50 Wids Train Resnet50 Wids The notebook demonstrates the use of the wids library for handling large-scale image datasets in a distributed training context. It illustrates how to load and preprocess images from a sharded dataset stored remotely using wids.ShardListDataset , apply transformations, and efficiently sample the data using wids.DistributedChunkedSampler for training a deep learning model. The notebook also shows how to integrate these datasets with PyTorch's DataLoader for training a model with reporting on training progress.","title":"Index"},{"location":"examples/wids/#train-resnet50-multiray-wids","text":"Train Resnet50 Multiray Wids The notebook demonstrates how to perform distributed PyTorch training using the wids library, specifically focusing on the use of ShardListDataset for dataset construction and DistributedChunkedSampler for efficient sampling in a distributed environment. It highlights the ease of integrating wids with PyTorch code for training models on datasets stored in the cloud, with an example using a fake version of ImageNet. The notebook also shows how to set up distributed training using Ray and includes a configuration dataclass to manage training parameters.","title":"Train Resnet50 Multiray Wids"},{"location":"examples/wids/#train-resnet50-wids","text":"Train Resnet50 Wids The notebook demonstrates the use of the wids library for handling large-scale image datasets in a distributed training context. It illustrates how to load and preprocess images from a sharded dataset stored remotely using wids.ShardListDataset , apply transformations, and efficiently sample the data using wids.DistributedChunkedSampler for training a deep learning model. The notebook also shows how to integrate these datasets with PyTorch's DataLoader for training a model with reporting on training progress.","title":"Train Resnet50 Wids"},{"location":"examples/wids/train-resnet50-multiray-wids/","text":"WebIndexedDataset + Distributed PyTorch Training This notebook illustrates how to use the Web Indexed Dataset ( wids ) library for distributed PyTorch training using DistributedDataParallel . Using wids results in training code that is almost identical to plain PyTorch, with the only changes being the use of ShardListDataset for the dataset construction, and the use of the DistributedChunkedSampler for generating random samples from the dataset. ShardListDataset requires some local storage. By default, that local storage just grows as shards are downloaded, but if you have limited space, you can run create_cleanup_background_process to clean up the cache; shards will be re-downloaded as necessary. import os import sys from typing import ( List, Tuple, Dict, Optional, Any, Union, Callable, Iterable, Iterator, NamedTuple, Set, Sequence, ) import torch import torch.nn as nn import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torchvision.models import resnet50 from torchvision import datasets, transforms from torch.utils.data import DataLoader import ray import wids import dataclasses import time from collections import deque from pprint import pprint def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth Data Loading for Distributed Training The datasets we use for training are stored in the cloud. We use fake-imagenet , which is 1/10th the size of Imagenet and artificially generated, but it has the same number of shards and trains quickly. Note that unlike the webdataset library, wids always needs a local cache directory (it will use /tmp if you don't give it anything explicitly). # Parameters epochs = 1 max_steps = int(1e12) batch_size = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet/\" trainset_url = bucket+\"imagenet-train.json\" valset_url = bucket+\"imagenet-val.json\" cache_dir = \"./_cache\" # This is a typical PyTorch dataset, except that we read from the cloud. def make_dataset_train(): transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) def make_sample(sample): image = sample[\".jpg\"] label = sample[\".cls\"] return transform_train(image), label trainset = wids.ShardListDataset(trainset_url, cache_dir=\"./_cache\", keep=True) trainset = trainset.add_transform(make_sample) return trainset This is really the only thing that is ever so slightly special about the wids library: you should use the special DistributedChunkedSampler for sampling. The regular DistributedSampler will technically work, but because of its poor locality of reference, will be significantly slower. # To keep locality of reference in the dataloader, we use a special sampler # for distributed training, DistributedChunkedSampler. def make_dataloader_train(): dataset = make_dataset_train() sampler = wids.DistributedChunkedSampler(dataset, chunksize=1000, shuffle=True) dataloader = DataLoader( dataset, batch_size=batch_size, sampler=sampler, num_workers=4 ) return dataloader def make_dataloader(split=\"train\"): \"\"\"Make a dataloader for training or validation.\"\"\" if split == \"train\": return make_dataloader_train() elif split == \"val\": return make_dataloader_val() else: raise ValueError(f\"unknown split {split}\") # Try it out. sample = next(iter(make_dataloader())) print(sample[0].shape, sample[1].shape) PyTorch Distributed Training Code Really, all that's needed for distributed training is the DistributedDataParallel wrapper around the model. # For convenience, we collect all the configuration parameters into # a dataclass. @dataclasses.dataclass class Config: rank: Optional[int] = None epochs: int = 1 max_steps: int = int(1e18) lr: float = 0.001 momentum: float = 0.9 world_size: int = 8 backend: str = \"nccl\" master_addr: str = \"localhost\" master_port: str = \"12355\" report_s: float = 15.0 report_growth: float = 1.1 Config() # A typical PyTorch training function. def train(config): # Define the model, loss function, and optimizer model = resnet50(pretrained=False).cuda() if config.rank is not None: model = DistributedDataParallel(model) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=config.lr) # Data loading code trainloader = make_dataloader(split=\"train\") losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0 # Training loop for epoch in range(config.epochs): for i, data, verbose in enumerate_report(trainloader, config.report_s): inputs, labels = data[0].cuda(), data[1].cuda() # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() # just bookkeping and progress report steps += len(labels) accuracy = ( (outputs.argmax(1) == labels).float().mean() ) # calculate accuracy losses.append(loss.item()) accuracies.append(accuracy.item()) if verbose and len(losses) > 0: avgloss = sum(losses) / len(losses) avgaccuracy = sum(accuracies) / len(accuracies) print( f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr, ) if steps > config.max_steps: print( \"finished training (max_steps)\", steps, config.max_steps, file=sys.stderr, ) return print(\"finished Training\", steps) # A quick smoke test. os.environ[\"GOPEN_VERBOSE\"] = \"1\" config = Config() config.epochs = 1 config.max_steps = 1000 train(config) os.environ[\"GOPEN_VERBOSE\"] = \"0\" Distributed Training in Ray The code above can be used with any distributed computing framwork, including torch.distributed.launch . Below is simply an example of how to launch the training jobs with the Ray framework. Ray is nice for distributed training because it makes the Python code independent of the runtime environment (Kubernetes, Slurm, ad-hoc networking, etc.). Meaning, the code below will work regardless of how you start up your Ray cluster. # The distributed training function to be used with Ray. # Since this is started via Ray remote, we set up the distributed # training environment here. @ray.remote(num_gpus=1) def train_in_ray(rank, config): if rank is not None: # Set up distributed PyTorch. config.rank = rank os.environ[\"MASTER_ADDR\"] = config.master_addr os.environ[\"MASTER_PORT\"] = config.master_port dist.init_process_group( backend=config.backend, rank=rank, world_size=config.world_size ) # Ray will automatically set CUDA_VISIBLE_DEVICES for each task. train(config) if not ray.is_initialized(): ray.init() print(\"#gpus available in the cluster\", ray.available_resources()[\"GPU\"]) def distributed_training(config): num_gpus = ray.available_resources()[\"GPU\"] config.world_size = int(min(config.world_size, num_gpus)) pprint(config) results = ray.get( [train_in_ray.remote(i, config) for i in range(config.world_size)] ) print(results) config = Config() config.epochs = epochs config.max_steps = max_steps config.batch_size = batch_size print(config) distributed_training(config)","title":"WebIndexedDataset + Distributed PyTorch Training"},{"location":"examples/wids/train-resnet50-multiray-wids/#webindexeddataset-distributed-pytorch-training","text":"This notebook illustrates how to use the Web Indexed Dataset ( wids ) library for distributed PyTorch training using DistributedDataParallel . Using wids results in training code that is almost identical to plain PyTorch, with the only changes being the use of ShardListDataset for the dataset construction, and the use of the DistributedChunkedSampler for generating random samples from the dataset. ShardListDataset requires some local storage. By default, that local storage just grows as shards are downloaded, but if you have limited space, you can run create_cleanup_background_process to clean up the cache; shards will be re-downloaded as necessary. import os import sys from typing import ( List, Tuple, Dict, Optional, Any, Union, Callable, Iterable, Iterator, NamedTuple, Set, Sequence, ) import torch import torch.nn as nn import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel from torchvision.models import resnet50 from torchvision import datasets, transforms from torch.utils.data import DataLoader import ray import wids import dataclasses import time from collections import deque from pprint import pprint def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth","title":"WebIndexedDataset + Distributed PyTorch Training"},{"location":"examples/wids/train-resnet50-multiray-wids/#data-loading-for-distributed-training","text":"The datasets we use for training are stored in the cloud. We use fake-imagenet , which is 1/10th the size of Imagenet and artificially generated, but it has the same number of shards and trains quickly. Note that unlike the webdataset library, wids always needs a local cache directory (it will use /tmp if you don't give it anything explicitly). # Parameters epochs = 1 max_steps = int(1e12) batch_size = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet/\" trainset_url = bucket+\"imagenet-train.json\" valset_url = bucket+\"imagenet-val.json\" cache_dir = \"./_cache\" # This is a typical PyTorch dataset, except that we read from the cloud. def make_dataset_train(): transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) def make_sample(sample): image = sample[\".jpg\"] label = sample[\".cls\"] return transform_train(image), label trainset = wids.ShardListDataset(trainset_url, cache_dir=\"./_cache\", keep=True) trainset = trainset.add_transform(make_sample) return trainset This is really the only thing that is ever so slightly special about the wids library: you should use the special DistributedChunkedSampler for sampling. The regular DistributedSampler will technically work, but because of its poor locality of reference, will be significantly slower. # To keep locality of reference in the dataloader, we use a special sampler # for distributed training, DistributedChunkedSampler. def make_dataloader_train(): dataset = make_dataset_train() sampler = wids.DistributedChunkedSampler(dataset, chunksize=1000, shuffle=True) dataloader = DataLoader( dataset, batch_size=batch_size, sampler=sampler, num_workers=4 ) return dataloader def make_dataloader(split=\"train\"): \"\"\"Make a dataloader for training or validation.\"\"\" if split == \"train\": return make_dataloader_train() elif split == \"val\": return make_dataloader_val() else: raise ValueError(f\"unknown split {split}\") # Try it out. sample = next(iter(make_dataloader())) print(sample[0].shape, sample[1].shape)","title":"Data Loading for Distributed Training"},{"location":"examples/wids/train-resnet50-multiray-wids/#pytorch-distributed-training-code","text":"Really, all that's needed for distributed training is the DistributedDataParallel wrapper around the model. # For convenience, we collect all the configuration parameters into # a dataclass. @dataclasses.dataclass class Config: rank: Optional[int] = None epochs: int = 1 max_steps: int = int(1e18) lr: float = 0.001 momentum: float = 0.9 world_size: int = 8 backend: str = \"nccl\" master_addr: str = \"localhost\" master_port: str = \"12355\" report_s: float = 15.0 report_growth: float = 1.1 Config() # A typical PyTorch training function. def train(config): # Define the model, loss function, and optimizer model = resnet50(pretrained=False).cuda() if config.rank is not None: model = DistributedDataParallel(model) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=config.lr) # Data loading code trainloader = make_dataloader(split=\"train\") losses, accuracies, steps = deque(maxlen=100), deque(maxlen=100), 0 # Training loop for epoch in range(config.epochs): for i, data, verbose in enumerate_report(trainloader, config.report_s): inputs, labels = data[0].cuda(), data[1].cuda() # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() # just bookkeping and progress report steps += len(labels) accuracy = ( (outputs.argmax(1) == labels).float().mean() ) # calculate accuracy losses.append(loss.item()) accuracies.append(accuracy.item()) if verbose and len(losses) > 0: avgloss = sum(losses) / len(losses) avgaccuracy = sum(accuracies) / len(accuracies) print( f\"rank {config.rank} epoch {epoch:5d}/{i:9d} loss {avgloss:8.3f} acc {avgaccuracy:8.3f} {steps:9d}\", file=sys.stderr, ) if steps > config.max_steps: print( \"finished training (max_steps)\", steps, config.max_steps, file=sys.stderr, ) return print(\"finished Training\", steps) # A quick smoke test. os.environ[\"GOPEN_VERBOSE\"] = \"1\" config = Config() config.epochs = 1 config.max_steps = 1000 train(config) os.environ[\"GOPEN_VERBOSE\"] = \"0\"","title":"PyTorch Distributed Training Code"},{"location":"examples/wids/train-resnet50-multiray-wids/#distributed-training-in-ray","text":"The code above can be used with any distributed computing framwork, including torch.distributed.launch . Below is simply an example of how to launch the training jobs with the Ray framework. Ray is nice for distributed training because it makes the Python code independent of the runtime environment (Kubernetes, Slurm, ad-hoc networking, etc.). Meaning, the code below will work regardless of how you start up your Ray cluster. # The distributed training function to be used with Ray. # Since this is started via Ray remote, we set up the distributed # training environment here. @ray.remote(num_gpus=1) def train_in_ray(rank, config): if rank is not None: # Set up distributed PyTorch. config.rank = rank os.environ[\"MASTER_ADDR\"] = config.master_addr os.environ[\"MASTER_PORT\"] = config.master_port dist.init_process_group( backend=config.backend, rank=rank, world_size=config.world_size ) # Ray will automatically set CUDA_VISIBLE_DEVICES for each task. train(config) if not ray.is_initialized(): ray.init() print(\"#gpus available in the cluster\", ray.available_resources()[\"GPU\"]) def distributed_training(config): num_gpus = ray.available_resources()[\"GPU\"] config.world_size = int(min(config.world_size, num_gpus)) pprint(config) results = ray.get( [train_in_ray.remote(i, config) for i in range(config.world_size)] ) print(results) config = Config() config.epochs = epochs config.max_steps = max_steps config.batch_size = batch_size print(config) distributed_training(config)","title":"Distributed Training in Ray"},{"location":"examples/wids/train-resnet50-multiray-wids.summary/","text":"The notebook demonstrates how to perform distributed PyTorch training using the wids library, specifically focusing on the use of ShardListDataset for dataset construction and DistributedChunkedSampler for efficient sampling in a distributed environment. It highlights the ease of integrating wids with PyTorch code for training models on datasets stored in the cloud, with an example using a fake version of ImageNet. The notebook also shows how to set up distributed training using Ray and includes a configuration dataclass to manage training parameters.","title":"Train resnet50 multiray wids.summary"},{"location":"examples/wids/train-resnet50-wids/","text":"%matplotlib inline from functools import partial from pprint import pprint import random from collections import deque import numpy as np from matplotlib import pyplot as plt import torch import torchvision import torchvision.transforms as transforms from torchvision.models import resnet50 from torch.utils.data import DataLoader from torch import nn, optim import wids # parameters epochs = 3 max_steps = 100000 batch_size = 32 bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet/\" num_workers = 4 cache_dir = \"./_cache\" # helpers import time def enumerate_report(seq, delta, growth=1.0): last = 0 count = 0 for count, item in enumerate(seq): now = time.time() if now - last > delta: last = now yield count, item, True else: yield count, item, False delta *= growth # The standard TorchVision transformations. transform_train = transforms.Compose( [ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) transform_val = transforms.Compose( [ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ] ) # The dataset returns dictionaries. This is a small function we transform it # with to get the augmented image and the label. def make_sample(sample, val=False): image = sample[\".jpg\"] label = sample[\".cls\"] if val: return transform_val(image), label else: return transform_train(image), label # These are standard PyTorch datasets. Download is incremental into the cache. trainset = wids.ShardListDataset( bucket+\"imagenet-train.json\", cache_dir=cache_dir, keep=True ) valset = wids.ShardListDataset( bucket+\"imagenet-val.json\", cache_dir=cache_dir, keep=True ) trainset[0] # Next, we add the transformation to the dataset. Transformations # are executed in sequence. In fact, by default, there is a transformation # that reads and decodes images. trainset.add_transform(make_sample) valset.add_transform(partial(make_sample, val=True)) print(trainset[0][0].shape, trainset[0][1]) # We also need a sampler for the training set. There are three # special samplers in the `wids` package that work particularly # well with sharded datasets: # - `wids.ShardedSampler` shuffles shards and then samples in shards; # it guarantees that only one shard is used at a time # - `wids.ChunkedSampler` samples by fixed sized chunks, shuffles # the chunks, and the the samples within each chunk # - `wids.DistributedChunkedSampler` is like `ChunkedSampler` but # works with distributed training (it first divides the entire # dataset into per-node chunks, then the per-node chunks into # smaller chunks, then shuffles the smaller chunks) # trainsampler = wids.ShardedSampler(trainset) # trainsampler = wids.ChunkedSampler(trainset, chunksize=1000, shuffle=True) trainsampler = wids.DistributedChunkedSampler(trainset, chunksize=1000, shuffle=True) plt.plot(list(trainsampler)[:2500]) # Note that the sampler shuffles within each shard before moving on to # the next shard. Furthermore, on the first epoch, the sampler # uses the shards in order, but on subsequent epochs, it shuffles # them. This makes testing and debugging easier. If you don't like # this behavior, you can use shufflefirst=True trainsampler.set_epoch(0) # Create data loaders for the training and validation datasets trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=4, sampler=trainsampler) valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4) images, classes = next(iter(trainloader)) print(images.shape, classes.shape) # The usual PyTorch model definition. We use an uninitialized ResNet50 model. model = resnet50(pretrained=False) # Define the loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) # Move the model to the GPU if available device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") model = model.to(device) losses, accuracies = deque(maxlen=100), deque(maxlen=100) steps = 0 # Train the model for epoch in range(epochs): for i, data, verbose in enumerate_report(trainloader, 5): # get the inputs; data is a list of [inputs, labels] inputs, labels = data[0].to(device), data[1].to(device) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() pred = outputs.cpu().detach().argmax(dim=1, keepdim=True) correct = pred.eq(labels.cpu().view_as(pred)).sum().item() accuracy = correct / float(len(labels)) losses.append(loss.item()) accuracies.append(accuracy) steps += len(labels) if verbose and len(losses) > 5: print( \"[%d, %5d] loss: %.5f correct: %.5f\" % (epoch + 1, i + 1, np.mean(losses), np.mean(accuracies)) ) running_loss = 0.0 if steps > max_steps: break if steps > max_steps: break print(\"Finished Training\")","title":"Train resnet50 wids"},{"location":"examples/wids/train-resnet50-wids.summary/","text":"The notebook demonstrates the use of the wids library for handling large-scale image datasets in a distributed training context. It illustrates how to load and preprocess images from a sharded dataset stored remotely using wids.ShardListDataset , apply transformations, and efficiently sample the data using wids.DistributedChunkedSampler for training a deep learning model. The notebook also shows how to integrate these datasets with PyTorch's DataLoader for training a model with reporting on training progress.","title":"Train resnet50 wids.summary"}]}