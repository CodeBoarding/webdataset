<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Tesseract wds - webdataset</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Tesseract wds";
        var mkdocs_page_input_path = "tesseract-wds.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> webdataset
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">README</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../webdataset/">WebDataset</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../wids/">WIDS</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../FAQ/">FAQ</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../generate-text-dataset/">Dataset Generation</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Tesseract wds</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-wds/">Resnet 50 Training on (Fake)Imagenet with WebDataset</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-wids/">Train resnet50 wids</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-multiray-wds/">WebDataset + Distributed PyTorch Training</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-resnet50-multiray-wids/">WebIndexedDataset + Distributed PyTorch Training</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../train-ocr-errors-hf/">Fine Tuning LLM with Huggingface and WebDataset</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../mi-images/">Mini Imagenet Generation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../mi-prompts/">Mini Imagenet Generation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../wds-notes/">Wds notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../column-store/">Using WebDataset as a Column Store</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">webdataset</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Examples</li>
      <li class="breadcrumb-item active">Tesseract wds</li>
    <li class="wy-breadcrumbs-aside">
          <a href="http://github.com/webdataset/webdataset/edit/master/docs/tesseract-wds.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <pre><code class="language-python"># imports and helper functions

import os
import sys
import webdataset as wds
import braceexpand
import tempfile
import glob
from itertools import islice
import random

def summarize(sample):
    for k, v in sample.items():
        print(k, repr(v)[:100])

def read_binary(fname):
    with open(fname, &quot;rb&quot;) as stream:
        return stream.read()
</code></pre>
<h1 id="parallel-processing-of-shards-large-scale-ocr">Parallel Processing of Shards: Large Scale OCR</h1>
<p>This notebook illustrates how to take a large collection of shards consisting of PDFs and process them using <code>pdftoppm</code> and <code>tessearact</code> into a new dataset consisting of page images and corresponding OCR output.</p>
<p>The general approach is to process each shard sequentially and to process multiple shards in parallel. The basic structure of such a job looks like:</p>
<pre><code class="language-Python">with WebDataset(srcname) as src:
    with TarWriter(dstname) as dst:
        for sample in src:
            ... do something with sample ...
            dst.write(sample)
upload(dstname)
</code></pre>
<h1 id="the-arxiv-dataset-of-pdfs">The Arxiv Dataset of PDFs</h1>
<pre><code class="language-python"># The dataset is tar files containing PDFs, each using the Arxiv naming convention.

!gsutil cat gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar | tar tf - | sed 5q
</code></pre>
<pre><code>1808.00020v6.pdf
1511.05082v1.pdf
1610.08000v1.pdf
1506.03736v2.pdf
1909.03824v1.pdf
tar: stdout: write error
</code></pre>
<pre><code class="language-python"># Arxiv naming convenitions are incompatible with WebDataset, but we can add
# a file renaming function to the WebDataset to fix this.

def arxiv_rename(name):
    return name.replace(&quot;.pdf&quot;, &quot;&quot;).replace(&quot;.&quot;, &quot;_&quot;) + &quot;.pdf&quot;

# For this example, we just use two shards, but usually, you would have hundreds
# or thousands of shards.

dataset = &quot;gs://webdataset/testdata/arxiv-pdfs-{000000..000001}.tar&quot;

# Let's open the dataset and read the first sample.

shardurls = list(braceexpand.braceexpand(dataset))
ds = wds.WebDataset(shardurls, rename_files=arxiv_rename)
sample = next(iter(ds))
summarize(sample)
</code></pre>
<pre><code>GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {}


__key__ '1808_00020v6'
__url__ 'gs://webdataset/testdata/arxiv-pdfs-000000.tar'
pdf b'%PDF-1.5\n%\x8f\n18 0 obj\n&lt;&lt; /Filter /FlateDecode /Length 5428 &gt;&gt;\nstream\nx\xda\xad[]\xb3\xe3\xb
</code></pre>
<h1 id="running-tesseract-on-a-single-pdf">Running Tesseract on a Single PDF</h1>
<pre><code class="language-python">def process_sample(sample, maxpages=9999, shuffle=True):
    &quot;&quot;&quot;Process a sample from the Arxiv dataset.

    This function converts the PDF file to a sequence of JPEG images
    and then invokes Tesseract to recognize the text in the images.
    It returns a sequence of samples, one per page, each containing
    the JPEG image and the hOCR output from Tesseract.
    &quot;&quot;&quot;

    # We work in a temporary directory; most operations are command line tools

    with tempfile.TemporaryDirectory() as dirname:

        # Write the PDF file to disk and convert it to a sequence of JPEGs using pdftoppm
        pdfpath = dirname + &quot;/sample.pdf&quot;
        with open(pdfpath, &quot;wb&quot;) as stream:
            stream.write(sample[&quot;pdf&quot;])
        assert os.system(f&quot;(cd {dirname} &amp;&amp; pdftoppm -forcenum -jpeg -r 300 -l 9999 sample.pdf page)&quot;) == 0

        # Next, we are going to iterate over the pages, convert them to text using tesseract,
        pages = sorted(glob.glob(dirname + &quot;/page-*.jpg&quot;))
        if shuffle:
            random.shuffle(pages)

        for page in islice(pages, maxpages):
            page_without_suffix = page[:-4]
            base = os.path.basename(page_without_suffix)

            # Invoke Tesseract to convert the page image to hOCR.
            os.system(f&quot;tesseract {page} {page_without_suffix} hocr&quot;)

            # Construct the output sample.
            nsample = {
                &quot;__key__&quot;: sample[&quot;__key__&quot;] + f&quot;/{base}&quot;,
                &quot;jpg&quot;: read_binary(page_without_suffix + &quot;.jpg&quot;),
                &quot;hocr&quot;: read_binary(page_without_suffix + &quot;.hocr&quot;),
            }

            # This function returns an iterator over the recognized pages.
            yield nsample
</code></pre>
<pre><code class="language-python">output = next(process_sample(sample))
summarize(output)
</code></pre>
<pre><code>Tesseract Open Source OCR Engine v4.1.1 with Leptonica


__key__ '1808_00020v6/page-19'
jpg b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x01,\x01,\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x0
hocr b'&lt;?xml version="1.0" encoding="UTF-8"?&gt;\n&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional/
</code></pre>
<h1 id="processing-a-shard-of-pdf-files">Processing a Shard of PDF Files</h1>
<pre><code class="language-python">def process_shard(src, dst, maxpdfs=999999, maxpages=9999):
    &quot;&quot;&quot;Process a shard of the Arxiv dataset.

    This function reads a shard of the Arxiv dataset, processes each sample
    using the process_sample function, and writes the page images and corresponding
    hOCR output to a new shard, one sample per page.

    The maxpdfs and maxpages parameters can be used to limit the number of
    samples and pages processed. This is useful for testing, as well as limit
    the number of pages selected from very long PDF documents.
    &quot;&quot;&quot;
    with wds.TarWriter(dst) as sink:
        for sample in islice(wds.WebDataset(src, rename_files=arxiv_rename), maxpdfs):
            print(sample[&quot;__key__&quot;], sample.keys())
            for nsample in process_sample(sample, maxpages=maxpages):
                print(&quot;    &quot;, nsample[&quot;__key__&quot;])
                sink.write(nsample)
</code></pre>
<pre><code class="language-python">!rm -f output.tar
process_shard(shardurls[0], &quot;output.tar&quot;, maxpdfs=2, maxpages=2)
</code></pre>
<pre><code>GOPEN output.tar {}
GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {}


1808_00020v6 dict_keys(['__key__', '__url__', 'pdf'])


Tesseract Open Source OCR Engine v4.1.1 with Leptonica


     1808_00020v6/page-17


Tesseract Open Source OCR Engine v4.1.1 with Leptonica


     1808_00020v6/page-01
1511_05082v1 dict_keys(['__key__', '__url__', 'pdf'])


Tesseract Open Source OCR Engine v4.1.1 with Leptonica


     1511_05082v1/page-05


Tesseract Open Source OCR Engine v4.1.1 with Leptonica


     1511_05082v1/page-08
</code></pre>
<pre><code class="language-python">!tar tvf output.tar
</code></pre>
<pre><code>-r--r--r-- bigdata/bigdata 14248 2023-12-17 21:55 1808_00020v6/page-17.hocr
-r--r--r-- bigdata/bigdata 277568 2023-12-17 21:55 1808_00020v6/page-17.jpg
-r--r--r-- bigdata/bigdata 103527 2023-12-17 21:55 1808_00020v6/page-01.hocr
-r--r--r-- bigdata/bigdata 1186871 2023-12-17 21:55 1808_00020v6/page-01.jpg
-r--r--r-- bigdata/bigdata   49187 2023-12-17 21:55 1511_05082v1/page-05.hocr
-r--r--r-- bigdata/bigdata  589829 2023-12-17 21:55 1511_05082v1/page-05.jpg
-r--r--r-- bigdata/bigdata   44814 2023-12-17 21:55 1511_05082v1/page-08.hocr
-r--r--r-- bigdata/bigdata  490804 2023-12-17 21:55 1511_05082v1/page-08.jpg
</code></pre>
<h1 id="parallelizing-processing-with-ray">Parallelizing Processing with Ray</h1>
<p>This illustrates how to use Ray to process many shards in parallel.</p>
<p>You don't need to use Ray for this, you can also invoke <code>process_shard</code> in parallel using a job queueing system or using some other distributed computing framework.</p>
<p>Generally, it is easiest to process each shard sequentially, and to process multiple shards in parallel. However, you could use additional parallelization to perform processing of the samples in parallel.</p>
<pre><code class="language-python">maxpdfs = 2  # for testing, we just use two PDFs per shard
maxpages = 2  # for testing, we just use two pages per PDF
upload_cmd = &quot;echo gsutil cp {src} {dst}&quot;  # for testing, we don't actually upload the completed shards

import ray
if not ray.is_initialized():
    ray.init()

@ray.remote(num_cpus=4)
def process_shard_parallel(src, dstbucket, maxpdfs=999999, maxpages=9999):
    &quot;&quot;&quot;Process a shard of the Arxiv dataset and upload the output shard to a bucket.

    This function reads a shard of the Arxiv dataset, processes each sample
    using the process_sample function, and writes the page images and corresponding 
    hOCR output to a new shard, one sample per page. The output shard is then
    uploaded to the specified bucket using `upload_cmd`.
    &quot;&quot;&quot;
    dst = dstbucket + &quot;/&quot; + os.path.basename(src)
    with tempfile.NamedTemporaryFile() as tmp:
        process_shard(src, tmp.name, maxpdfs=maxpdfs, maxpages=maxpages)
        assert os.system(upload_cmd.format(src=tmp.name, dst=dst)) == 0

!rm -f output.tar
ray.get([process_shard_parallel.remote(src, &quot;gs://somebucket&quot;, maxpdfs=maxpdfs, maxpages=maxpages) for src in shardurls])

</code></pre>
<pre><code>2023-12-17 21:55:57,786 INFO worker.py:1664 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[36m(process_shard_parallel pid=677110)[0m GOPEN /tmp/tmpznj5reiz {}
[36m(process_shard_parallel pid=677110)[0m GOPEN gs://webdataset/testdata/arxiv-pdfs-000000.tar {}


[36m(process_shard_parallel pid=677113)[0m 1402_1973v2 dict_keys(['__key__', '__url__', 'pdf'])


[36m(process_shard_parallel pid=677113)[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica
[36m(process_shard_parallel pid=677113)[0m Warning:guessing pitch as xheight on row 4, block 2
[36m(process_shard_parallel pid=677113)[0m Warning:guessing pitch as xheight on row 27, block 2
[36m(process_shard_parallel pid=677113)[0m Warning:guessing pitch as xheight on row 1, block 10


[36m(process_shard_parallel pid=677113)[0m      1402_1973v2/page-10
[36m(process_shard_parallel pid=677110)[0m 1808_00020v6 dict_keys(['__key__', '__url__', 'pdf'])


[36m(process_shard_parallel pid=677113)[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica
[36m(process_shard_parallel pid=677113)[0m GOPEN gs://webdataset/testdata/arxiv-pdfs-000001.tar {}[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m


[36m(process_shard_parallel pid=677113)[0m      1402_1973v2/page-09
[36m(process_shard_parallel pid=677113)[0m 1612_01474v3 dict_keys(['__key__', '__url__', 'pdf'])


[36m(process_shard_parallel pid=677110)[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica
[36m(process_shard_parallel pid=677113)[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica


[36m(process_shard_parallel pid=677110)[0m      1808_00020v6/page-08
[36m(process_shard_parallel pid=677110)[0m      1808_00020v6/page-19
[36m(process_shard_parallel pid=677110)[0m 1511_05082v1 dict_keys(['__key__', '__url__', 'pdf'])


[36m(process_shard_parallel pid=677110)[0m Tesseract Open Source OCR Engine v4.1.1 with Leptonica[32m [repeated 4x across cluster][0m


[36m(process_shard_parallel pid=677113)[0m gsutil cp /tmp/tmpezg3f2_5 gs://somebucket/arxiv-pdfs-000001.tar
[36m(process_shard_parallel pid=677113)[0m      1612_01474v3/page-11[32m [repeated 3x across cluster][0m





[None, None]
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../generate-text-dataset/" class="btn btn-neutral float-left" title="Dataset Generation"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../train-resnet50-wds/" class="btn btn-neutral float-right" title="Resnet 50 Training on (Fake)Imagenet with WebDataset">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="http://github.com/webdataset/webdataset" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../generate-text-dataset/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../train-resnet50-wds/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
